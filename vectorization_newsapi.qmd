---
title: "Vectorization - NewsAPI"
execute:
  echo: false
---

```{python}
# import libraries
import pandas as pd
from itables import show

# import initial data
newsapi_clean = pd.read_csv('data/newsapi_data/newsapi_clean_snippet.csv')

# import data - overall
newsapi_overall_cv = pd.read_csv('data/newsapi_data/vectorized/newsapi_overall_cv.csv')
newsapi_overall_tf = pd.read_csv('data/newsapi_data/vectorized/newsapi_overall_tf.csv')
newsapi_overall_cv_lemmatized = pd.read_csv('data/newsapi_data/vectorized/newsapi_overall_cv_lemmatized.csv')
newsapi_overall_tf_lemmatized = pd.read_csv('data/newsapi_data/vectorized/newsapi_overall_tf_lemmatized.csv')
newsapi_overall_cv_stemmatized = pd.read_csv('data/newsapi_data/vectorized/newsapi_overall_cv_stemmatized.csv')
newsapi_overall_tf_stemmatized = pd.read_csv('data/newsapi_data/vectorized/newsapi_overall_tf_stemmatized.csv')

# import data - political biases
newsapi_left = pd.read_csv('data/newsapi_data/vectorized/left.csv')
newsapi_lean_left = pd.read_csv('data/newsapi_data/vectorized/lean_left.csv')
newsapi_center = pd.read_csv('data/newsapi_data/vectorized/center.csv')
newsapi_lean_right = pd.read_csv('data/newsapi_data/vectorized/lean_right.csv')
newsapi_right = pd.read_csv('data/newsapi_data/vectorized/right.csv')

#show(newsapi_raw, columnDefs=[{"width": "120px", "targets": "_all"}], autoWidth=False, classes="display")
```

# Introduction

Using the data which has been prepared and merged with potential labels, as seen in [Data Acquisition - NewsAPI](data_newsapi.qmd), a few more steps can be taken to turn the news articles into numerical representations which can then be used for further analyses and machine learning applications.

## Strategy - Further Preprocessing

Recall that the prepared data looks like this:

```{python}
show(newsapi_clean.head(5), style="width:100%;margin:auto")
```

The main label of interest for this data is *Political Bias*, however other potential labels include:

- *News Organization Source*
- *Author*
- *Date*
- *Search Query Parameter*

The data itself will be the *News Article*, however other potential data sources include:

- *Title*
- *Description*

This page will focus just on the entire *News Article* for data, but it could be worth comparing *Title* and *Description* in the future.

For this text data, the additional preprocessing will take place for each article:

- Remove line breaks.
- Remove punctuation.
- Remove words containing numbers.
- Remove standalone numbers.
- Remove leading and trailing spaces.
- Lowercase the remaining words.
- Remove any single-length text remaining.

## Strategy - Vectorizing

Now that the articles have been properly prepared to create a vectorized dataframe, several versions will be created. Namely, word count dataframes will be created using `CountVectorizer()` and normalized word count dataframes will be created using `TfidfVectorizer()`, both from scikit-learn. Stopwords will removed using these functions as well. Dataframes will be further subsetted along the political bias labels. **Lemmatizing** and **Stemming** will also be used to create different versions available for further analyses. One additional option could be further versions of maximum words allowed in a dataframe.

## Vectorizing - Overall

These versions will vectorize the completely preprocessed NewsAPI data in its entirety, remove stopwords and use a maximum of 200 features. A sample of the datasets for both vectorized versions will be shown. Additionally, a wordcloud and a top ten feature visualization will be shown for the `CountVectorizer()` versions.

### CountVectorizer

**Labeled Sample**

```{python}
show(newsapi_overall_cv, style="width:100%;margin:auto")
```

**Wordcloud**

![](images/newsapi/vectorized/newsapi_overall_cv.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_overall.png)

### TfidfVectorizer

**Labeled Sample**

```{python}
show(newsapi_overall_tf, style="width:100%;margin:auto")
```

## Vectorizing - Overall Lemmatized

### CountVectorizer

**Labeled Sample**

```{python}
show(newsapi_overall_cv_lemmatized, style="width:100%;margin:auto")
```

**Wordcloud**

![](images/newsapi/vectorized/newsapi_overall_cv_lemmatized.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_overall_lemmatized.png)

### TfidfVectorizer

**Labeled Sample**

```{python}
show(newsapi_overall_tf_lemmatized, style="width:100%;margin:auto")
```

## Vectorizing - Overall Stemmatized

### CountVectorizer

**Labeled Sample**

```{python}
show(newsapi_overall_cv_stemmatized, style="width:100%;margin:auto")
```

**Wordcloud**

![](images/newsapi/vectorized/newsapi_overall_cv_stemmatized.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_overall_stemmatized.png)

### TfidfVectorizer

**Labeled Sample**

```{python}
show(newsapi_overall_tf_stemmatized, style="width:100%;margin:auto")
```

## Vectorizing - Political Bias

Lemmatizing seems to aggregate the text data while retaining meaning in the words. For instance, *students* are aggregated into *student* and *loans* into *loan*, while words like *education* are not reduced to something like *educ*. Moving forward, lemmatizing is a logical preprocessing step. Thus, lemmatization and stopwords removal for creating vectorized versions of the political bias will be used. For the process of creating subsets of the political bias data, there are two methods that could be used.

- Subsetting Second: vectorize the entire dataset $\rightarrow$ append labels $\rightarrow$ subset on political bias
- Subsetting First: subset the dataset on political bias $\rightarrow$ vectorize the subset $\rightarrow$ append labels

By subsetting first, the maximum word count will be reflective of the corpus associated with the respective political bias. Therefore, it might be more useful in this comparative analysis to subset first.

Additionally, `CountVectorizer()` will be used over `TfidfVectorizer()` for a first pass. A normalized version of the features could be useful in some cases, such as when dealing with varying sizes of content (i.e. total word count of content), but feature appearance counts will be more useful for visualizing in this analysis.

### Political Bias: Left

**Labeled Sample**

```{python}
show(newsapi_left, style="width:100%;margin:auto")
```

**Wordcloud**
![](images/newsapi/vectorized/newsapi_left.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_left.png)

### Political Bias: Lean Left

**Labeled Sample**

```{python}
show(newsapi_lean_left, style="width:100%;margin:auto")
```

**Wordcloud**
![](images/newsapi/vectorized/newsapi_lean_left.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_lean_left.png)

### Political Bias: Center

**Labeled Sample**

```{python}
show(newsapi_center, style="width:100%;margin:auto")
```

**Wordcloud**
![](images/newsapi/vectorized/newsapi_center.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_center.png)

### Political Bias: Lean Right

**Labeled Sample**

```{python}
show(newsapi_lean_right, style="width:100%;margin:auto")
```

**Wordcloud**
![](images/newsapi/vectorized/newsapi_lean_right.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_lean_right.png)

### Political Bias: Right

**Labeled Sample**

```{python}
show(newsapi_right, style="width:100%;margin:auto")
```

**Wordcloud**
![](images/newsapi/vectorized/newsapi_right.png)

**Most Frequent Words**

![](images/newsapi/vectorized/most_frequent_words_right.png)

# Summary

Lemmatizing aggregates nicely while retaining the meaning within words, so this will likely be a consistently utilized preprocessing step moving forward. Although word counts over normalized word counts were shown for the political biases section, this was mainly for illustrative purposes. Normalized word counts could be useful when interpolating towards much smaller or much larger text files. Maximum number of features can be altered in future analyses, and words which are similar across all biases could be removed in future analyses to help illuminate the differences more between them.
