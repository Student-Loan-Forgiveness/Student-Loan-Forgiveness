[
  {
    "objectID": "vectorization_reddit.html",
    "href": "vectorization_reddit.html",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Using the data which has been prepared and merged with potential labels, as seen in Data Acquisition - Reddit, a few more steps can be taken to turn the Reddit posts into numerical representations which can then be used for further analyses and machine learning applications.\n\n\nRecall that the prepared data looks like this:\n\n\n\n\n    \n      \n      url\n      title\n      subreddit\n      author\n      original_author\n      author_upvotes\n      author_dates\n      author_content\n      author_content_aggregated\n      replies_to\n      replies_from\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nAs explained in the linked section above, the Reddit text data may be best fit for unsupervised learning methods, thus the labels may not be applicaple. However, potential labels of interest are:\n\nAuthor (Reddit user)\nURL (Reddit Thread)\nSubreddit (Reddit Community)\nSearch (Reddit Search Query)\n\nThe data itself will be the content posted by the Reddit users, or authors. In the inital data above, the column containing every post, comment, and reply by an author on a single Reddit thread was aggregated. This will be the first aggregation schema. A few other aggregation schemas will be considered.\nAggregation Schemas:\n\nThread - Author (INITIAL FORMAT): corpus where each file is an author’s aggregated text within a unique thread.\nSubreddit - Author: corpus where each file is an author’s aggregated text within a unique Subreddit.\nThreads: corpus where each file is the overall aggregated text within unique Threads (author’s combined)\nSubreddits: corpus where each file is the overall aggregated text within unique Subreddits (threads combined).\nAuthors: corpus where each file is the overall aggregated text by authors across every thread.\n\nUsing the initial format for the first schema, some additional preprocessing will take take place for the author’s aggregated posts on a single Reddit thread. This will include:\n\nRemove line breaks.\nRemove punctuation.\nRemove words containing numbers.\nRemove standalone numbers.\nRemove leading and trailing spaces.\nLowercase the remaining words.\nRemove any single-length text remaining.\n\nAs prescribed in the linked section in the Introduction on this page, the author AutoModerator will be removed before proceeding.\n\n\n\nFollowing queues from the Data Vectorizing - NewsAPI page, this initial pass will lemmatize the data and use CountVectorizer() from the scikit-learn library, and compare the different aggregation schemas. The TfidfVectorizer() could be invaluable in later use cases, as the length of the text content for many of the Reddit aggregation schemas vary wildly.\n\nLemmatizing is still a useful data dimensionality reduction technique. However, when it comes to Reddit posts, lemmatizing and especially stemmatizing should be used cautiously. News articles mostly use proper and accepted language, structure, and terms, whereas social media and other online community discussion boards, like Reddit, are likely to follow a more unofficial format. Social media may feature slang, text speak, mispellings, and incomplete sentences. Incomplete sentences could be due to single word responses or reactions, or simply due to improperly structured sentences. Nuances in Reddit-type posts could be lost with cleaning and simplification methods.\n\nAdditionally, stopwords will be removed and 200 maximum features will be used.\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      author\n      url\n      subreddit\n      search\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      author\n      subreddit\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      url\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      living\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      subreddit\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      living\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      author\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      living\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      author\n      url\n      subreddit\n      search\n      debt\n      don\n      just\n      like\n      loan\n      make\n      pay\n      people\n      student\n      wa\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "vectorization_reddit.html#strategy---further-preprocessing",
    "href": "vectorization_reddit.html#strategy---further-preprocessing",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Recall that the prepared data looks like this:\n\n\n\n\n    \n      \n      url\n      title\n      subreddit\n      author\n      original_author\n      author_upvotes\n      author_dates\n      author_content\n      author_content_aggregated\n      replies_to\n      replies_from\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nAs explained in the linked section above, the Reddit text data may be best fit for unsupervised learning methods, thus the labels may not be applicaple. However, potential labels of interest are:\n\nAuthor (Reddit user)\nURL (Reddit Thread)\nSubreddit (Reddit Community)\nSearch (Reddit Search Query)\n\nThe data itself will be the content posted by the Reddit users, or authors. In the inital data above, the column containing every post, comment, and reply by an author on a single Reddit thread was aggregated. This will be the first aggregation schema. A few other aggregation schemas will be considered.\nAggregation Schemas:\n\nThread - Author (INITIAL FORMAT): corpus where each file is an author’s aggregated text within a unique thread.\nSubreddit - Author: corpus where each file is an author’s aggregated text within a unique Subreddit.\nThreads: corpus where each file is the overall aggregated text within unique Threads (author’s combined)\nSubreddits: corpus where each file is the overall aggregated text within unique Subreddits (threads combined).\nAuthors: corpus where each file is the overall aggregated text by authors across every thread.\n\nUsing the initial format for the first schema, some additional preprocessing will take take place for the author’s aggregated posts on a single Reddit thread. This will include:\n\nRemove line breaks.\nRemove punctuation.\nRemove words containing numbers.\nRemove standalone numbers.\nRemove leading and trailing spaces.\nLowercase the remaining words.\nRemove any single-length text remaining.\n\nAs prescribed in the linked section in the Introduction on this page, the author AutoModerator will be removed before proceeding."
  },
  {
    "objectID": "vectorization_reddit.html#strategy---vectorizing",
    "href": "vectorization_reddit.html#strategy---vectorizing",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Following queues from the Data Vectorizing - NewsAPI page, this initial pass will lemmatize the data and use CountVectorizer() from the scikit-learn library, and compare the different aggregation schemas. The TfidfVectorizer() could be invaluable in later use cases, as the length of the text content for many of the Reddit aggregation schemas vary wildly.\n\nLemmatizing is still a useful data dimensionality reduction technique. However, when it comes to Reddit posts, lemmatizing and especially stemmatizing should be used cautiously. News articles mostly use proper and accepted language, structure, and terms, whereas social media and other online community discussion boards, like Reddit, are likely to follow a more unofficial format. Social media may feature slang, text speak, mispellings, and incomplete sentences. Incomplete sentences could be due to single word responses or reactions, or simply due to improperly structured sentences. Nuances in Reddit-type posts could be lost with cleaning and simplification methods.\n\nAdditionally, stopwords will be removed and 200 maximum features will be used."
  },
  {
    "objectID": "vectorization_reddit.html#vectorizing---thread-author-schema",
    "href": "vectorization_reddit.html#vectorizing---thread-author-schema",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      author\n      url\n      subreddit\n      search\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "vectorization_reddit.html#vectorizing---subreddit-author-schema",
    "href": "vectorization_reddit.html#vectorizing---subreddit-author-schema",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      author\n      subreddit\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "vectorization_reddit.html#vectorizing---threads-schema",
    "href": "vectorization_reddit.html#vectorizing---threads-schema",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      url\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      living\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "vectorization_reddit.html#vectorizing---subreddits-schema",
    "href": "vectorization_reddit.html#vectorizing---subreddits-schema",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      subreddit\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      living\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "vectorization_reddit.html#vectorizing---authors-schema",
    "href": "vectorization_reddit.html#vectorizing---authors-schema",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      author\n      able\n      account\n      actually\n      administration\n      ago\n      american\n      aren\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      borrower\n      business\n      buy\n      car\n      card\n      care\n      career\n      case\n      change\n      class\n      college\n      come\n      company\n      congress\n      cost\n      country\n      court\n      credit\n      day\n      debt\n      decision\n      degree\n      democrat\n      did\n      didn\n      different\n      doe\n      doesn\n      doing\n      dollar\n      don\n      education\n      end\n      exactly\n      federal\n      feel\n      financial\n      forgive\n      forgiven\n      forgiveness\n      free\n      fuck\n      future\n      getting\n      going\n      good\n      got\n      government\n      graduate\n      great\n      ha\n      happen\n      hard\n      having\n      help\n      high\n      higher\n      home\n      hope\n      house\n      idea\n      idr\n      income\n      instead\n      isn\n      issue\n      job\n      just\n      kid\n      know\n      law\n      le\n      left\n      let\n      life\n      like\n      likely\n      literally\n      live\n      living\n      loan\n      lol\n      long\n      look\n      lot\n      low\n      lower\n      major\n      make\n      making\n      maybe\n      mean\n      million\n      minimum\n      mohela\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      old\n      paid\n      parent\n      party\n      pay\n      paying\n      payment\n      people\n      person\n      plan\n      point\n      poor\n      ppp\n      predatory\n      president\n      pretty\n      price\n      principal\n      private\n      probably\n      problem\n      program\n      pslf\n      public\n      rate\n      real\n      really\n      reason\n      repayment\n      republican\n      rich\n      right\n      said\n      save\n      say\n      saying\n      school\n      score\n      service\n      shit\n      start\n      started\n      state\n      stop\n      student\n      sure\n      taking\n      tax\n      term\n      thing\n      think\n      thought\n      time\n      took\n      total\n      trump\n      try\n      trying\n      tuition\n      understand\n      university\n      use\n      used\n      ve\n      vote\n      wa\n      want\n      way\n      went\n      won\n      work\n      worked\n      working\n      wouldn\n      yeah\n      year\n      yes\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "vectorization_reddit.html#vectorizing---additional-parameters",
    "href": "vectorization_reddit.html#vectorizing---additional-parameters",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      author\n      url\n      subreddit\n      search\n      debt\n      don\n      just\n      like\n      loan\n      make\n      pay\n      people\n      student\n      wa\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "scripts/clustering/clustering_r.html",
    "href": "scripts/clustering/clustering_r.html",
    "title": "Hierarchical Clustering using R",
    "section": "",
    "text": "Code\n# import libraries\nlibrary(tm)\nlibrary(stringr)\nlibrary(wordcloud)\nlibrary(slam)\nlibrary(quanteda)\nlibrary(SnowballC)\nlibrary(arules)\nlibrary(proxy)\nlibrary(cluster)\nlibrary(stringi)\nlibrary(proxy)\nlibrary(Matrix)\nlibrary(tidytext)\nlibrary(plyr)\nlibrary(ggplot2)\nlibrary(factoextra)\nlibrary(mclust)\nlibrary(textstem)\nlibrary(amap)\nlibrary(networkD3)\nlibrary(dendextend)\n\n\nWarning message:\n\"package 'tm' was built under R version 4.3.3\"\nLoading required package: NLP\n\nWarning message:\n\"package 'NLP' was built under R version 4.3.3\"\nWarning message:\n\"package 'wordcloud' was built under R version 4.3.3\"\nLoading required package: RColorBrewer\n\nWarning message:\n\"package 'slam' was built under R version 4.3.3\"\nWarning message:\n\"package 'quanteda' was built under R version 4.3.3\"\nPackage version: 4.2.0\nUnicode version: 13.0\nICU version: 69.1\n\nParallel computing: 16 of 16 threads used.\n\nSee https://quanteda.io for tutorials and examples.\n\n\nAttaching package: 'quanteda'\n\n\nThe following object is masked from 'package:tm':\n\n    stopwords\n\n\nThe following objects are masked from 'package:NLP':\n\n    meta, meta&lt;-\n\n\nWarning message:\n\"package 'arules' was built under R version 4.3.3\"\nLoading required package: Matrix\n\nWarning message:\n\"package 'Matrix' was built under R version 4.3.3\"\n\nAttaching package: 'arules'\n\n\nThe following object is masked from 'package:tm':\n\n    inspect\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\n\nWarning message:\n\"package 'proxy' was built under R version 4.3.2\"\n\nAttaching package: 'proxy'\n\n\nThe following object is masked from 'package:Matrix':\n\n    as.matrix\n\n\nThe following objects are masked from 'package:stats':\n\n    as.dist, dist\n\n\nThe following object is masked from 'package:base':\n\n    as.matrix\n\n\nWarning message:\n\"package 'tidytext' was built under R version 4.3.3\"\nWarning message:\n\"package 'plyr' was built under R version 4.3.2\"\nWarning message:\n\"package 'ggplot2' was built under R version 4.3.3\"\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:NLP':\n\n    annotate\n\n\nWarning message:\n\"package 'factoextra' was built under R version 4.3.3\"\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nWarning message:\n\"package 'mclust' was built under R version 4.3.3\"\nPackage 'mclust' version 6.1.1\nType 'citation(\"mclust\")' for citing this R package in publications.\n\nWarning message:\n\"package 'textstem' was built under R version 4.3.3\"\nLoading required package: koRpus.lang.en\n\nWarning message:\n\"package 'koRpus.lang.en' was built under R version 4.3.3\"\nLoading required package: koRpus\n\nWarning message:\n\"package 'koRpus' was built under R version 4.3.3\"\nLoading required package: sylly\n\nWarning message:\n\"package 'sylly' was built under R version 4.3.3\"\nFor information on available language packages for 'koRpus', run\n\n  available.koRpus.lang()\n\nand see ?install.koRpus.lang()\n\n\n\nAttaching package: 'koRpus'\n\n\nThe following objects are masked from 'package:quanteda':\n\n    tokens, types\n\n\nThe following object is masked from 'package:tm':\n\n    readTagged\n\n\nWarning message:\n\"package 'amap' was built under R version 4.3.3\"\nWarning message:\n\"package 'networkD3' was built under R version 4.3.3\"\nWarning message:\n\"package 'dendextend' was built under R version 4.3.3\"\n\n---------------------\nWelcome to dendextend version 1.19.0\nType citation('dendextend') for how to cite the package.\n\nType browseVignettes(package = 'dendextend') for the package vignette.\nThe github page is: https://github.com/talgalili/dendextend/\n\nSuggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues\nYou may ask questions at stackoverflow, use the r and dendextend tags: \n     https://stackoverflow.com/questions/tagged/dendextend\n\n    To suppress this message use:  suppressPackageStartupMessages(library(dendextend))\n---------------------\n\n\n\nAttaching package: 'dendextend'\n\n\nThe following object is masked from 'package:stats':\n\n    cutree"
  },
  {
    "objectID": "scripts/clustering/clustering_r.html#newsapi",
    "href": "scripts/clustering/clustering_r.html#newsapi",
    "title": "Hierarchical Clustering using R",
    "section": "NewsAPI",
    "text": "NewsAPI\n\n\nCode\n# extract corpus\nnewsapi &lt;- Corpus(DirSource('corpus_newsapi'))\n\n\n\n\nCode\n# create document term matrix\ndtm_newsapi &lt;- as.matrix(DocumentTermMatrix(newsapi))\n\n\n\n\nCode\n# calculate cosine similarity distance\ncosine_newsapi &lt;- dist(dtm_newsapi, method='cosine')\n\n\n\n\nCode\n# create clusters\nclusters_newsapi &lt;- hclust(cosine_newsapi, method='ward.D2')\n\n\n\n\nCode\n# create cuts (2 - 6)\ncut_2_newsapi &lt;- cutree(clusters_newsapi, k=2)\ncut_3_newsapi &lt;- cutree(clusters_newsapi, k=3)\ncut_4_newsapi &lt;- cutree(clusters_newsapi, k=4)\ncut_5_newsapi &lt;- cutree(clusters_newsapi, k=5)\ncut_6_newsapi &lt;- cutree(clusters_newsapi, k=6)\n\n\n\n\nCode\n# create results dataframe\nresults_newsapi &lt;- data.frame(\n    File = rownames(dtm_newsapi),\n    cluster_2 = cut_2_newsapi,\n    cluster_3 = cut_3_newsapi,\n    cluster_4 = cut_4_newsapi,\n    cluster_5 = cut_5_newsapi,\n    cluster_6 = cut_6_newsapi\n)"
  },
  {
    "objectID": "scripts/clustering/clustering_r.html#newsapi---reddit-base",
    "href": "scripts/clustering/clustering_r.html#newsapi---reddit-base",
    "title": "Hierarchical Clustering using R",
    "section": "NewsApi - Reddit Base",
    "text": "NewsApi - Reddit Base\n\n\nCode\n# extract corpus\nnewsapi_reddit_base &lt;- Corpus(DirSource('corpus_newsapi_reddit_base'))\n\n\n\n\nCode\n# create document term matrix\ndtm_newsapi_reddit_base &lt;- DocumentTermMatrix(newsapi_reddit_base)\n\n\n\n\nCode\n# calculate tfidf\ntfidf_newsapi_reddit_base &lt;- as.matrix(weightTfIdf(dtm_newsapi_reddit_base))\n\n\n\n\nCode\n# calculate cosine similarity distance\ncosine_newsapi_reddit_base &lt;- dist(tfidf_newsapi_reddit_base, method='cosine')\n\n\n\n\nCode\n# create clusters\nclusters_newsapi_reddit_base &lt;- hclust(cosine_newsapi_reddit_base, method='ward.D2')\n\n\n\n\nCode\n# create cuts (2 - 6)\ncut_2_newsapi_reddit_base &lt;- cutree(clusters_newsapi_reddit_base, k=2)\ncut_3_newsapi_reddit_base &lt;- cutree(clusters_newsapi_reddit_base, k=3)\ncut_4_newsapi_reddit_base &lt;- cutree(clusters_newsapi_reddit_base, k=4)\ncut_5_newsapi_reddit_base &lt;- cutree(clusters_newsapi_reddit_base, k=5)\ncut_6_newsapi_reddit_base &lt;- cutree(clusters_newsapi_reddit_base, k=6)\n\n\n\n\nCode\n# create results dataframe\nresults_newsapi_reddit_base &lt;- data.frame(\n    File = rownames(tfidf_newsapi_reddit_base),\n    cluster_2 = cut_2_newsapi_reddit_base,\n    cluster_3 = cut_3_newsapi_reddit_base,\n    cluster_4 = cut_4_newsapi_reddit_base,\n    cluster_5 = cut_5_newsapi_reddit_base,\n    cluster_6 = cut_6_newsapi_reddit_base\n)\n\n\n\n\nCode\n# save results dataframe\nwrite.csv(results_newsapi_reddit_base, 'hierarchical_results/hierarchical_newsapi_reddit_base.csv', row.names=FALSE)"
  },
  {
    "objectID": "scripts/clustering/clustering_r.html#newsapi---reddit-author",
    "href": "scripts/clustering/clustering_r.html#newsapi---reddit-author",
    "title": "Hierarchical Clustering using R",
    "section": "NewsApi - Reddit Author",
    "text": "NewsApi - Reddit Author\n\n\nCode\n# extract corpus\nnewsapi_reddit_author &lt;- Corpus(DirSource('corpus_newsapi_reddit_author'))\n\n\n\n\nCode\n# create document term matrix\ndtm_newsapi_reddit_author &lt;- DocumentTermMatrix(newsapi_reddit_author)\n\n\n\n\nCode\n# calculate tfidf\ntfidf_newsapi_reddit_author &lt;- as.matrix(weightTfIdf(dtm_newsapi_reddit_author))\n\n\n\n\nCode\n# calculate cosine similarity distance\ncosine_newsapi_reddit_author &lt;- dist(tfidf_newsapi_reddit_author, method='cosine')\n\n\n\n\nCode\n# create clusters\nclusters_newsapi_reddit_author &lt;- hclust(cosine_newsapi_reddit_author, method='ward.D2')\n\n\n\n\nCode\n# create cuts (2 - 6)\ncut_2_newsapi_reddit_author &lt;- cutree(clusters_newsapi_reddit_author, k=2)\ncut_3_newsapi_reddit_author &lt;- cutree(clusters_newsapi_reddit_author, k=3)\ncut_4_newsapi_reddit_author &lt;- cutree(clusters_newsapi_reddit_author, k=4)\ncut_5_newsapi_reddit_author &lt;- cutree(clusters_newsapi_reddit_author, k=5)\ncut_6_newsapi_reddit_author &lt;- cutree(clusters_newsapi_reddit_author, k=6)\n\n\n\n\nCode\n# create results dataframe\nresults_newsapi_reddit_author &lt;- data.frame(\n    File = rownames(tfidf_newsapi_reddit_author),\n    cluster_2 = cut_2_newsapi_reddit_author,\n    cluster_3 = cut_3_newsapi_reddit_author,\n    cluster_4 = cut_4_newsapi_reddit_author,\n    cluster_5 = cut_5_newsapi_reddit_author,\n    cluster_6 = cut_6_newsapi_reddit_author\n)\n\n\n\n\nCode\n# save results dataframe\nwrite.csv(results_newsapi_reddit_author, 'hierarchical_results/hierarchical_newsapi_reddit_author.csv', row.names=FALSE)"
  },
  {
    "objectID": "scripts/clustering/clustering_r.html#newsapi---reddit-threads",
    "href": "scripts/clustering/clustering_r.html#newsapi---reddit-threads",
    "title": "Hierarchical Clustering using R",
    "section": "NewsApi - Reddit Threads",
    "text": "NewsApi - Reddit Threads\n\n\nCode\n# extract corpus\nnewsapi_reddit_thread &lt;- Corpus(DirSource('corpus_newsapi_reddit_thread'))\n\n\n\n\nCode\n# create document term matrix\ndtm_newsapi_reddit_thread &lt;- DocumentTermMatrix(newsapi_reddit_thread)\n\n\n\n\nCode\n# calculate tfidf\ntfidf_newsapi_reddit_thread &lt;- as.matrix(weightTfIdf(dtm_newsapi_reddit_thread))\n\n\n\n\nCode\n# calculate cosine similarity distance\ncosine_newsapi_reddit_thread &lt;- dist(tfidf_newsapi_reddit_thread, method='cosine')\n\n\n\n\nCode\n# create clusters\nclusters_newsapi_reddit_thread &lt;- hclust(cosine_newsapi_reddit_thread, method='ward.D2')\n\n\n\n\nCode\n# create cuts (2 - 6)\ncut_2_newsapi_reddit_thread &lt;- cutree(clusters_newsapi_reddit_thread, k=2)\ncut_3_newsapi_reddit_thread &lt;- cutree(clusters_newsapi_reddit_thread, k=3)\ncut_4_newsapi_reddit_thread &lt;- cutree(clusters_newsapi_reddit_thread, k=4)\ncut_5_newsapi_reddit_thread &lt;- cutree(clusters_newsapi_reddit_thread, k=5)\ncut_6_newsapi_reddit_thread &lt;- cutree(clusters_newsapi_reddit_thread, k=6)\n\n\n\n\nCode\n# create results dataframe\nresults_newsapi_reddit_thread &lt;- data.frame(\n    File = rownames(tfidf_newsapi_reddit_thread),\n    cluster_2 = cut_2_newsapi_reddit_thread,\n    cluster_3 = cut_3_newsapi_reddit_thread,\n    cluster_4 = cut_4_newsapi_reddit_thread,\n    cluster_5 = cut_5_newsapi_reddit_thread,\n    cluster_6 = cut_6_newsapi_reddit_thread\n)\n\n\n\n\nCode\n# save results dataframe\nwrite.csv(results_newsapi_reddit_thread, 'hierarchical_results/hierarchical_newsapi_reddit_thread.csv', row.names=FALSE)"
  },
  {
    "objectID": "scripts/clustering/clustering_r.html#newsapi---reddit-subreddits",
    "href": "scripts/clustering/clustering_r.html#newsapi---reddit-subreddits",
    "title": "Hierarchical Clustering using R",
    "section": "NewsApi - Reddit Subreddits",
    "text": "NewsApi - Reddit Subreddits\n\n\nCode\n# extract corpus\nnewsapi_reddit_subreddit &lt;- Corpus(DirSource('corpus_newsapi_reddit_subreddit'))\n\n\n\n\nCode\n# create document term matrix\ndtm_newsapi_reddit_subreddit &lt;- DocumentTermMatrix(newsapi_reddit_subreddit)\n\n\n\n\nCode\n# calculate tfidf\ntfidf_newsapi_reddit_subreddit &lt;- as.matrix(weightTfIdf(dtm_newsapi_reddit_subreddit))\n\n\n\n\nCode\n# calculate cosine similarity distance\ncosine_newsapi_reddit_subreddit &lt;- dist(tfidf_newsapi_reddit_subreddit, method='cosine')\n\n\n\n\nCode\n# create clusters\nclusters_newsapi_reddit_subreddit &lt;- hclust(cosine_newsapi_reddit_subreddit, method='ward.D2')\n\n\n\n\nCode\n# create cuts (2 - 6)\ncut_2_newsapi_reddit_subreddit &lt;- cutree(clusters_newsapi_reddit_subreddit, k=2)\ncut_3_newsapi_reddit_subreddit &lt;- cutree(clusters_newsapi_reddit_subreddit, k=3)\ncut_4_newsapi_reddit_subreddit &lt;- cutree(clusters_newsapi_reddit_subreddit, k=4)\ncut_5_newsapi_reddit_subreddit &lt;- cutree(clusters_newsapi_reddit_subreddit, k=5)\ncut_6_newsapi_reddit_subreddit &lt;- cutree(clusters_newsapi_reddit_subreddit, k=6)\n\n\n\n\nCode\n# create results dataframe\nresults_newsapi_reddit_subreddit &lt;- data.frame(\n    File = rownames(tfidf_newsapi_reddit_subreddit),\n    cluster_2 = cut_2_newsapi_reddit_subreddit,\n    cluster_3 = cut_3_newsapi_reddit_subreddit,\n    cluster_4 = cut_4_newsapi_reddit_subreddit,\n    cluster_5 = cut_5_newsapi_reddit_subreddit,\n    cluster_6 = cut_6_newsapi_reddit_subreddit\n)\n\n\n\n\nCode\n# save results dataframe\nwrite.csv(results_newsapi_reddit_subreddit, 'hierarchical_results/hierarchical_newsapi_reddit_subreddit.csv', row.names=FALSE)\n\n\n\n\nCode\nnews_subreddit &lt;- as.dendrogram(clusters_newsapi_reddit_subreddit)\n\n\n\n\nCode\n# function to rename the labels\nlabels(news_subreddit) &lt;- sapply(labels(news_subreddit), function(x) {\n  if (startsWith(x, \"Reddit\")) {\n    return(\"Reddit\")\n  } else if (startsWith(x, \"NewsAPI\")) {\n    return(\"NewsAPI\")\n  } else {\n    # retain label if not Reddit or NewsAPI  \n    return(x)\n  }\n})\n\n\n\n\nCode\n# function to color the labels\nlabel_colors &lt;- function(node) {\n  label &lt;- attr(node, \"label\")\n  if (!is.null(label)) {\n    if (label == \"Reddit\") {\n      attr(node, \"nodePar\") &lt;- list(lab.col = \"red\")\n    } else if (label == \"NewsAPI\") {\n      attr(node, \"nodePar\") &lt;- list(lab.col = \"blue\")\n    }\n  }\n  return(node)\n}\n\n\n\n\nCode\n# apply the color function to the dendrogram labels\nnews_subreddit &lt;- dendrapply(news_subreddit, label_colors)\n\n\n\n\nCode\n# plot dendrogram with altered labels\npar(mar = c(1, 1, 1, 7))\nplot(news_subreddit, horiz = TRUE, axes = FALSE, main=\"NewsAPI Source Schema vs. Reddit Subreddit Schema\")\nabline(v = 350, lty = 2)"
  },
  {
    "objectID": "modeling_prep.html",
    "href": "modeling_prep.html",
    "title": "Modeling - Preparation",
    "section": "",
    "text": "This section will focus on supervised machine learning. Specifically, classification with the following families will be used:\n\nNaive Bayes\nDecision Trees\nSupport Vector Machines\n\nSupervised Machine Learning models require labeled data, or known tags on the data to train the model. Additionally, when teaching the models, the data is split into disjoint training and testing sets. In essence, the models learn from the training set and then are tested on unseen data. This helps to prevent overfitting and simulates applying the model on real-world data.\nThis is what the exploratory and unsupervised methods in the previous sections have been leading to. The idea is to begin with the NewsAPI data labeled with political bias by news organization. After creating acceptable models, they will be applied to the Reddit data in an attempt to project political bias on Reddit authors. Given the ultimate goal of finding positive and negative sentiment on the topic of student loan forgiveness, and the fact that the sentiment is roughly split along politcal bias, aiming to classify by political bias will be a decent indicator of sentiment."
  },
  {
    "objectID": "modeling_prep.html#newsapi",
    "href": "modeling_prep.html#newsapi",
    "title": "Modeling - Preparation",
    "section": "NewsAPI",
    "text": "NewsAPI\nAs a reminder, the political labels are:\n\nLeft\nLean Left\nCenter\nLean Right\nRight\n\nThere will be several aggregations of the labels used:\n\n5 Labels (strictly all five)\n3 Labels\n\nLean Left combined into Left\nCenter\nLean Right combined into Right\n\n3 Labels Strict\n\nStrictly Left\nStrictly Center\nStrictly Right\n\n2 Labels\n\nLean Left combined into Left\nLean Right combined into Right\n\n2 Labels Strict\n\nStrictly Left\nStrictly Right\n\n\nEach of these aggregations will be transformed into labeled 1000 word vectorized versions."
  },
  {
    "objectID": "modeling_prep.html#reddit",
    "href": "modeling_prep.html#reddit",
    "title": "Modeling - Preparation",
    "section": "Reddit",
    "text": "Reddit\nThe Reddit data will remain unlabeled, as this will be where the models are applied to project political bias onto authors. However, the Reddit data will be transformed into word vectorized versions with no limit on the maximum word count."
  },
  {
    "objectID": "modeling_prep.html#newsapi-data",
    "href": "modeling_prep.html#newsapi-data",
    "title": "Modeling - Preparation",
    "section": "NewsAPI Data",
    "text": "NewsAPI Data\nAfter the text data is cleaned, stopwords removed, and lemmatized, CountVectorization is performed and the labels were reappended to this. A sample of this data looks like:\n\n\n\n\n    \n      \n      BIAS\n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nFrom this vectorized version of the data, rows will be aggregated or dropped dependong on the 5, 3, or 2 strategy outlined above."
  },
  {
    "objectID": "modeling_prep.html#reddit-data",
    "href": "modeling_prep.html#reddit-data",
    "title": "Modeling - Preparation",
    "section": "Reddit Data",
    "text": "Reddit Data\nAfter the text data is cleaned, stopwords removed, and lemmatized, CountVectorization is and labels were not appended. A sample of this data looks like:\n\n\n\n\n    \n      \n      aa\n      abandon\n      abandoned\n      ability\n      able\n      abortion\n      abraham\n      absolute\n      absolutely\n      absolve\n      abusing\n      academically\n      accelerated\n      accept\n      acceptable\n      acceptance\n      accepted\n      accepting\n      access\n      accessible\n      accident\n      accidentally\n      accommodation\n      accomplished\n      accomplishment\n      accordance\n      according\n      accordingly\n      account\n      accountability\n      accountable\n      accountant\n      accounting\n      accreditation\n      accrual\n      accrued\n      accrues\n      accruing\n      accumulate\n      accumulated\n      accumulating\n      accumulation\n      accurate\n      aced\n      achieve\n      achieves\n      act\n      action\n      active\n      actively\n      actual\n      actually\n      ad\n      adam\n      adapt\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      addressed\n      addressing\n      adjudication\n      adjust\n      adjustable\n      adjusted\n      adjustment\n      admin\n      administration\n      administrative\n      administratively\n      administrator\n      admiral\n      admission\n      admit\n      admitting\n      adopt\n      adoption\n      adoreing\n      adoring\n      adult\n      advantage\n      advantageous\n      advice\n      advocacy\n      advocate\n      advocating\n      af\n      affair\n      affect\n      affected\n      afford\n      affordable\n      africa\n      ag\n      age\n      agency\n      agenda\n      week\n      weekend\n      weekly\n      weighing\n      weird\n      weirdly\n      welcome\n      welfare\n      wellwishes\n      went\n      west\n      western\n      whats\n      wheel\n      white\n      whopping\n      wich\n      widely\n      wife\n      wiggle\n      wild\n      wildly\n      willful\n      willfully\n      william\n      willing\n      win\n      wind\n      window\n      wing\n      winner\n      wiped\n      wiping\n      wire\n      wise\n      wisely\n      wiser\n      wish\n      wished\n      withdraw\n      withdrawn\n      witherspoon\n      wo\n      woman\n      won\n      wonder\n      wondered\n      wonderful\n      wont\n      word\n      wording\n      work\n      workaround\n      workarounds\n      worked\n      worker\n      workerslatest\n      workforce\n      working\n      world\n      worldnews\n      worried\n      worry\n      worse\n      worst\n      worth\n      worthiness\n      worthless\n      wouldn\n      wound\n      wow\n      wrinkle\n      write\n      writing\n      written\n      wrong\n      wrote\n      wtfhow\n      xfinity\n      yale\n      yap\n      yard\n      yea\n      yeah\n      year\n      yearly\n      yep\n      yes\n      yesterday\n      yield\n      york\n      youdid\n      young\n      younger\n      youngest\n      youth\n      youtube\n      yr\n      zero\n      zillion\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_prep.html#five-labels",
    "href": "modeling_prep.html#five-labels",
    "title": "Modeling - Preparation",
    "section": "Five Labels",
    "text": "Five Labels\n\nTraining Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nTesting Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_prep.html#three-labels",
    "href": "modeling_prep.html#three-labels",
    "title": "Modeling - Preparation",
    "section": "Three Labels",
    "text": "Three Labels\n\nTraining Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nTesting Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_prep.html#strict-three-labels",
    "href": "modeling_prep.html#strict-three-labels",
    "title": "Modeling - Preparation",
    "section": "Strict Three Labels",
    "text": "Strict Three Labels\n\nTraining Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nTesting Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_prep.html#two-labels",
    "href": "modeling_prep.html#two-labels",
    "title": "Modeling - Preparation",
    "section": "Two Labels",
    "text": "Two Labels\n\nTraining Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nTesting Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_prep.html#strict-two-labels",
    "href": "modeling_prep.html#strict-two-labels",
    "title": "Modeling - Preparation",
    "section": "Strict Two Labels",
    "text": "Strict Two Labels\n\nTraining Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nTesting Data and Labels Example\nData\n\n\n\n\n    \n      \n      ability\n      able\n      academic\n      acceptance\n      access\n      according\n      account\n      act\n      acting\n      action\n      activity\n      actually\n      ad\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administration\n      administrative\n      advantage\n      affect\n      affected\n      affordable\n      age\n      agency\n      agenda\n      agent\n      ago\n      agreement\n      agricultural\n      agriculture\n      ahead\n      ai\n      aid\n      aim\n      air\n      allow\n      allowed\n      allowing\n      allows\n      amazon\n      america\n      american\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      ap\n      app\n      application\n      apply\n      approach\n      approved\n      area\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      association\n      attack\n      attempt\n      attorney\n      authority\n      available\n      average\n      avoid\n      away\n      bad\n      balance\n      bank\n      bankruptcy\n      based\n      basis\n      began\n      begin\n      believe\n      benefit\n      best\n      better\n      biden\n      big\n      biggest\n      billion\n      billionaire\n      bird\n      black\n      blagojevich\n      block\n      blocked\n      blue\n      board\n      body\n      book\n      tech\n      technology\n      tell\n      temporarily\n      temporary\n      term\n      tetfund\n      texas\n      thing\n      think\n      thought\n      thousand\n      threat\n      thursday\n      time\n      tip\n      title\n      today\n      told\n      took\n      tool\n      total\n      track\n      trade\n      transfer\n      transgender\n      treasury\n      treatment\n      tried\n      trillion\n      trump\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      twitter\n      type\n      typically\n      ultimately\n      uncertainty\n      unclear\n      undergraduate\n      understand\n      union\n      united\n      university\n      update\n      usaid\n      use\n      used\n      user\n      using\n      valuable\n      value\n      vance\n      various\n      versus\n      vice\n      video\n      view\n      virginia\n      vote\n      voter\n      vought\n      want\n      wanted\n      war\n      washington\n      watch\n      water\n      way\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      win\n      wing\n      wo\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worth\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      yoy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nLabel\n\n\n\n\n    \n      \n      BIAS\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_prep.html#five-labels-1",
    "href": "modeling_prep.html#five-labels-1",
    "title": "Modeling - Preparation",
    "section": "Five Labels",
    "text": "Five Labels"
  },
  {
    "objectID": "modeling_prep.html#three-labels-1",
    "href": "modeling_prep.html#three-labels-1",
    "title": "Modeling - Preparation",
    "section": "Three Labels",
    "text": "Three Labels"
  },
  {
    "objectID": "modeling_prep.html#strict-three-labels-1",
    "href": "modeling_prep.html#strict-three-labels-1",
    "title": "Modeling - Preparation",
    "section": "Strict Three Labels",
    "text": "Strict Three Labels"
  },
  {
    "objectID": "modeling_prep.html#two-labels-1",
    "href": "modeling_prep.html#two-labels-1",
    "title": "Modeling - Preparation",
    "section": "Two Labels",
    "text": "Two Labels"
  },
  {
    "objectID": "modeling_prep.html#strict-two-labels-1",
    "href": "modeling_prep.html#strict-two-labels-1",
    "title": "Modeling - Preparation",
    "section": "Strict Two Labels",
    "text": "Strict Two Labels"
  },
  {
    "objectID": "modeling_dt.html",
    "href": "modeling_dt.html",
    "title": "Modeling - Decision Trees",
    "section": "",
    "text": "Decision Trees are a heuristic based classification model which are useful for capturing non-linear trends and patterns in data. The heuristic aspect means that it follows a set of rules to provide an answer, whereas an algorithm follows steps to provide an answer which is always optimal. The tree aspect comes from the flowchart-like structure which features nodes and branches depending on decisions calculated from the data, constructing a logical pathway for classification.\n\n\n\n\n\nThe example above, from this documentation, shows how decision trees can partition non-linear data. This is two-dimensional dataset example, but the same idea holds true for higher dimensions.\n\n\n\n\n\nThe example above, from this website, shows the flowchart like structure, illustrating how a decision can be made by splitting logically on a criteria.\nNotice that this example uses both qualitative and quantitative data. Decision Trees are effective on even mixed data. In fact, given at least a single column of quantitivate data, there are an infinite number of trees that can be made depending on how the quantitiatve variables are split. In addition to an infinite number of ways to split the quantitative variables (especially continuous data), tree depth can add to the complexity of a model.\nTrees can be shallow or deep, meaning the number of branches and subsequent nodes that are allowed. A tree can be split until each node is pure or even only contains a single value. Purity in a node refers to the amount of labels within it. For example, given a decision tree whose task is to model a binary label dataset, if a node contains only a single label type, it is considered pure. Investigating the purity (and impurity) of a node is how “goodness” of split is measured. How are criteria for a split formed and how does this relate to purity? The common heuristics that are used in this process are:\n\nGini\nEntropy\nInformation Gain\n\nGini and Entropy calculate the impurity of a node, and Information Gain measures the overall impurity after a split is made and either Gini or Impurity is calculated. The attribute with the highest Information Gain is chosen for the split.\nThis section of the analysis will specifically use Decision Tree Classification sklearn documentation."
  },
  {
    "objectID": "modeling_dt.html#hyperparameter-results",
    "href": "modeling_dt.html#hyperparameter-results",
    "title": "Modeling - Decision Trees",
    "section": "Hyperparameter Results",
    "text": "Hyperparameter Results\n\n\n\n\n    \n      \n      Label\n      mean_test_score\n      rank_test_score\n      param_criterion\n      param_max_depth\n      param_max_features\n      mean_fit_time\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_dt.html#hyperparameter-best-results",
    "href": "modeling_dt.html#hyperparameter-best-results",
    "title": "Modeling - Decision Trees",
    "section": "Hyperparameter Best Results",
    "text": "Hyperparameter Best Results\n\n\n\n\n    \n      \n      Criterion\n      Max_Depth\n      Max_Features\n      Model\n      Accuracy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nNote that the combined search has rankings for each model based not only on accuracy but aspects such model fitting time. The Hyperparameter Best Results will be used for the following modeling."
  },
  {
    "objectID": "modeling_dt.html#five-labels",
    "href": "modeling_dt.html#five-labels",
    "title": "Modeling - Decision Trees",
    "section": "Five Labels",
    "text": "Five Labels\n\n\n\n\n\n\n\n\n\n\nThis model across all five labels resulted in a 55.95% accuracy. It was particularly weak in predicting Lean Right correctly. The root node of the tree was \\(news \\leq 1.5\\)."
  },
  {
    "objectID": "modeling_dt.html#three-labels",
    "href": "modeling_dt.html#three-labels",
    "title": "Modeling - Decision Trees",
    "section": "Three Labels",
    "text": "Three Labels\n\n\n\n\n\n\n\n\n\n\nWhen combining Lean Left and Lean Right with Left and Right, respectively, the model accuracy was just below 60%. The root node of the tree was \\(fox \\leq 1.5\\)."
  },
  {
    "objectID": "modeling_dt.html#strict-three-labels",
    "href": "modeling_dt.html#strict-three-labels",
    "title": "Modeling - Decision Trees",
    "section": "Strict Three Labels",
    "text": "Strict Three Labels\n\n\n\n\n\n\n\n\n\n\nWhen the leaning political biases weren’t combined, the accuracy of the model had a slight increase of about 5%. The root node of the tree was \\(news \\leq 1.5\\)."
  },
  {
    "objectID": "modeling_dt.html#two-labels",
    "href": "modeling_dt.html#two-labels",
    "title": "Modeling - Decision Trees",
    "section": "Two Labels",
    "text": "Two Labels\n\n\n\n\n\n\n\n\n\n\nWhen combining Lean Left and Lean Right with Left and Right, respectively, and dropping the Center label, the model accuracy was almost 61%. There were still a high amount of incorrect predictions, which could be reflective on the leanings. The root node of the tree was \\(required \\leq 0.5\\)."
  },
  {
    "objectID": "modeling_dt.html#strict-two-labels",
    "href": "modeling_dt.html#strict-two-labels",
    "title": "Modeling - Decision Trees",
    "section": "Strict Two Labels",
    "text": "Strict Two Labels\n\n\n\n\n\n\n\n\n\n\nWhen the leaning political biases weren’t combined, and dropping the the Center label, the accuracy of the model increased by almost 20% over all other models. This is a respectable model, and the best performance with the Decision Tree classification in this section. Additionally, it had the simplest tree with the root node being \\(news \\leq 2.5\\)."
  },
  {
    "objectID": "modeling_dt.html#comparing-trees",
    "href": "modeling_dt.html#comparing-trees",
    "title": "Modeling - Decision Trees",
    "section": "Comparing Trees",
    "text": "Comparing Trees\nThe models using different subsets and aggregations of the data prodcued several different trees. Just examining the root node, \\(news\\) was mainly the root node. However, the last tree provided a different split value of \\(2.5\\) versus \\(1.5\\) for the others with that root node. However, the other root nodes were \\(required\\) and \\(fox\\)."
  },
  {
    "objectID": "modeling_dt.html#feature-importance-through-permutation",
    "href": "modeling_dt.html#feature-importance-through-permutation",
    "title": "Modeling - Decision Trees",
    "section": "Feature Importance through Permutation",
    "text": "Feature Importance through Permutation\n\nTwo Features - Strict Two Political Biases\n\n\n\n\n    \n      \n      feature\n      importance\n      absolute_importance\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nAfter combining these features with the missing features from the decision tree nodes, the new model features only 11 words."
  },
  {
    "objectID": "modeling_dt.html#retrained-models",
    "href": "modeling_dt.html#retrained-models",
    "title": "Modeling - Decision Trees",
    "section": "Retrained Models",
    "text": "Retrained Models\nThe retrained models with fewer features had roughly about the same accuracy.\n\nTwo Features"
  },
  {
    "objectID": "modeling_dt.html#reddit-projection-results",
    "href": "modeling_dt.html#reddit-projection-results",
    "title": "Modeling - Decision Trees",
    "section": "Reddit Projection Results",
    "text": "Reddit Projection Results\n\n\n\n\n    \n      \n      Author\n      Predicted Bias Two\n      Threshold Two\n      Threshold\n      Conclusion\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nThe above illustrates the predicions for both the three and two label models as well as their probabilities. This was a strictly two-label projection onto the Reddit data and shows some decently high confidence from the model. Recall that political biases are correlated with sentiment, with the Right having a more negative sentiment and the Left having a more positive sentiment."
  },
  {
    "objectID": "lda.html",
    "href": "lda.html",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "",
    "text": "This section specifically focuses on gaining information by topic modeling through Latent Dirichlet Allocation (LDA). The data used in this section will be the news articles gathered through NewsAPI and Reddit content aggregated into the Author Aggregation Schema.\n\n\nTopic Modeling is a general term for methods to discover groupings or themes within data. A topic is a mixture of words, and a document is a mixture of topics. Topic Modeling aims to uncover these layered similarities between a collection of documents via analyzing the shapes of documents. Just as true quantitative record data can form shapes by examining the matrix formed by vectors over its dimensions, matrices of vectorized documents can form shapes as well.\nLatent Dirichlet Allocation (LDA), specifically, is a popular unsupervised machine learning method of topic modeling. LDA uses a form of the Multinomial Beta Distribution known as the Dirichlet Distribution to perform topic modeling. It should be noted that LDA has a non-uniqueness property, or that words can be featured across multiple topics. Essentially, LDA uses the Dirichlet distribution to find words that occur together to find a topic. However, it does not name or label the topic. When topics are found, a specified number of words can be reported and the topic can be deduced from this."
  },
  {
    "objectID": "lda.html#topic-modeling-and-lda",
    "href": "lda.html#topic-modeling-and-lda",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "",
    "text": "Topic Modeling is a general term for methods to discover groupings or themes within data. A topic is a mixture of words, and a document is a mixture of topics. Topic Modeling aims to uncover these layered similarities between a collection of documents via analyzing the shapes of documents. Just as true quantitative record data can form shapes by examining the matrix formed by vectors over its dimensions, matrices of vectorized documents can form shapes as well.\nLatent Dirichlet Allocation (LDA), specifically, is a popular unsupervised machine learning method of topic modeling. LDA uses a form of the Multinomial Beta Distribution known as the Dirichlet Distribution to perform topic modeling. It should be noted that LDA has a non-uniqueness property, or that words can be featured across multiple topics. Essentially, LDA uses the Dirichlet distribution to find words that occur together to find a topic. However, it does not name or label the topic. When topics are found, a specified number of words can be reported and the topic can be deduced from this."
  },
  {
    "objectID": "lda.html#newsapi",
    "href": "lda.html#newsapi",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "NewsAPI",
    "text": "NewsAPI\n\n\n\n\n    \n      \n      aack\n      ability\n      able\n      abo\n      abolish\n      abortion\n      abuse\n      academic\n      acceptance\n      access\n      according\n      account\n      accountability\n      accruing\n      act\n      acting\n      action\n      activity\n      actually\n      add\n      added\n      adding\n      addition\n      additional\n      additionally\n      address\n      administrati\n      administration\n      administrator\n      adult\n      advance\n      advantage\n      advice\n      advisor\n      advocate\n      aempt\n      aend\n      aended\n      aention\n      affair\n      affect\n      affected\n      afford\n      affordability\n      affordable\n      afp\n      age\n      agency\n      agenda\n      ago\n      agreed\n      ahead\n      ai\n      aid\n      aimed\n      alaska\n      allow\n      allowed\n      allowing\n      allows\n      ally\n      altadena\n      alternati\n      alys\n      amendment\n      america\n      american\n      analysis\n      analyst\n      angeles\n      announced\n      announcement\n      annual\n      answer\n      anti\n      aorney\n      ap\n      appeal\n      appeared\n      appears\n      application\n      applied\n      apply\n      approach\n      approd\n      appropriated\n      approval\n      apr\n      arage\n      area\n      aren\n      argued\n      argument\n      art\n      article\n      ask\n      asked\n      asset\n      assistance\n      associated\n      track\n      trade\n      tradition\n      traditional\n      training\n      transfer\n      transgender\n      transition\n      transportation\n      treasury\n      treatment\n      tried\n      trillion\n      true\n      trump\n      trust\n      try\n      trying\n      tuesday\n      tuition\n      turn\n      turned\n      twier\n      type\n      typically\n      ukraine\n      ultimately\n      unable\n      uncertainty\n      unclear\n      unconstitutional\n      undergraduate\n      understand\n      understanding\n      union\n      unique\n      unirsity\n      united\n      unless\n      unlikely\n      unprecedented\n      update\n      updated\n      usaid\n      use\n      used\n      using\n      usually\n      valuable\n      value\n      vance\n      various\n      vice\n      view\n      ving\n      virginia\n      visit\n      vote\n      voted\n      voter\n      vought\n      wealth\n      website\n      wednesday\n      week\n      went\n      west\n      white\n      wide\n      widespread\n      wildfire\n      willing\n      win\n      wind\n      wing\n      woke\n      woman\n      won\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      workplace\n      world\n      worried\n      worry\n      worse\n      worst\n      worth\n      wouldn\n      wrien\n      wrote\n      year\n      yes\n      yield\n      york\n      young\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "lda.html#reddit-author-schema",
    "href": "lda.html#reddit-author-schema",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Reddit Author Schema",
    "text": "Reddit Author Schema\n\n\n\n\n    \n      \n      ability\n      able\n      abo\n      absolute\n      absolutely\n      accept\n      access\n      according\n      account\n      accrue\n      accrued\n      accruing\n      act\n      actily\n      action\n      actual\n      actually\n      add\n      added\n      additional\n      address\n      adjustment\n      admin\n      administrati\n      administration\n      adult\n      advantage\n      advice\n      aempt\n      aend\n      affect\n      afford\n      affordable\n      age\n      agency\n      ago\n      agree\n      agreed\n      agreement\n      ahead\n      aid\n      allow\n      allowed\n      alys\n      america\n      american\n      answer\n      anti\n      anymore\n      anyy\n      apparently\n      application\n      applied\n      apply\n      applying\n      approd\n      approval\n      arage\n      area\n      aren\n      argue\n      argument\n      art\n      article\n      ask\n      asked\n      asking\n      asset\n      assume\n      assuming\n      authority\n      auto\n      automatically\n      available\n      avoid\n      awesome\n      ay\n      bachelor\n      backed\n      bad\n      bail\n      bailouts\n      balance\n      bank\n      bankruptcy\n      barely\n      base\n      based\n      basic\n      basically\n      bc\n      beer\n      begin\n      beginning\n      belie\n      benefit\n      best\n      bet\n      biden\n      big\n      time\n      today\n      told\n      ton\n      took\n      topic\n      tord\n      tords\n      total\n      totally\n      track\n      trade\n      training\n      transfer\n      tried\n      trillion\n      true\n      truly\n      trump\n      trust\n      try\n      trying\n      tuition\n      turn\n      type\n      typically\n      undergrad\n      undergraduate\n      understand\n      understanding\n      unfair\n      unfortunately\n      unirsity\n      unless\n      unsubsidized\n      update\n      usa\n      usage\n      use\n      used\n      useless\n      user\n      using\n      usually\n      va\n      value\n      various\n      view\n      ving\n      vote\n      voted\n      voter\n      voting\n      wealth\n      wealthy\n      website\n      week\n      weird\n      welfare\n      went\n      weren\n      white\n      wide\n      wife\n      wild\n      willing\n      win\n      wing\n      wipe\n      wiped\n      wish\n      woman\n      won\n      wonder\n      word\n      work\n      worked\n      worker\n      workforce\n      working\n      world\n      worried\n      worry\n      worse\n      worst\n      worth\n      worthless\n      wouldn\n      wrien\n      write\n      wrong\n      yeah\n      year\n      yep\n      yes\n      young\n      younger\n      yr\n      yup\n      zero\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "lda.html#newsapi-results",
    "href": "lda.html#newsapi-results",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "NewsAPI Results",
    "text": "NewsAPI Results"
  },
  {
    "objectID": "lda.html#newsapi-discussion",
    "href": "lda.html#newsapi-discussion",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "NewsAPI Discussion",
    "text": "NewsAPI Discussion\n15 words over 3 topics were discovered within the NewsAPI documents. The first image shows the words per topic, and the second image shows the frequencies of the words by weighting compared to the vocabulary in the vectorized dataset.\nManual Topic Labeling:\n\nFirst Topic: Financially Related\n\nWords like “loan”, “borrower”, “plan”, “payment”, “forginess” (forgiveness), “debt”, “credit”, and “repayment” are not only selected by LDA, but have high frequencies particular to this topic.\n\nSecond Topic: Government Related\n\nWords like “trump”, “federal”, “president”, “gornment” (government), “state”, “american”, and “agency” have high frequencies particular to this topic.\n\nThird Topic: School Related\n\nWords like “rate”, “college”, “unirsity” (university), “college”, “faculty”, “earnings”, “enrollment”, “employment”, “graduation”, “median”, and “ratio” have high frequencies particular to this topic.\n\n\nSince each document is a collection of topics itself, it is logical that words are repeated and some of the frequencies aren’t a perfect indicator of the topic. However, by analyzing the high frequency words, those seem to be decent choices."
  },
  {
    "objectID": "lda.html#reddit-author-schema-results",
    "href": "lda.html#reddit-author-schema-results",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Reddit Author Schema Results",
    "text": "Reddit Author Schema Results"
  },
  {
    "objectID": "lda.html#reddit-author-schema-discussion",
    "href": "lda.html#reddit-author-schema-discussion",
    "title": "Latent Dirichlet Allocation (LDA)",
    "section": "Reddit Author Schema Discussion",
    "text": "Reddit Author Schema Discussion\n15 words over 3 topics were discovered within the NewsAPI documents. The first image shows the words per topic, and the second image shows the frequencies of the words by weighting compared to the vocabulary in the vectorized dataset.\nManual Topic Labeling:\n\nFirst Topic: Monthly Payments and Credit Scores Related\n\nWords like “payment”, “credit”, “month”, “payment”, “balance”, “score”, and “account” have high frequencies particular to this topic.\n\nSecond Topic: Political Divide on Forgiveness Related\n\nWords like “forginess” or “forgin” (forgiveness), “biden”, “republican”, “trump”, and “vote” have high frequencies particular to this topic.\n\nThird Topic: Debt from Degree or School Related\n\nWords like “debt”, “college”, “money”, “school” and “degree” have high frequencies particular to this topic.\n\n\nAgain, certain high frequency words across all documents are expected to be repeated, such as “loan”. However, there are some clear topic choices knowing the overall climate surrounding this."
  },
  {
    "objectID": "data_reddit.html",
    "href": "data_reddit.html",
    "title": "Data Acquisition - Reddit",
    "section": "",
    "text": "Aside from reporting done by news media organizations, Student Loan Forgiveness is a toipc on social media and public discussion platforms such as Reddit. Reddit consists of communities known as Subreddits, where users can participate in topic specific online discourse. Although some users choose to tag their accounts with political bias leanings such as liberal or conservative which could help determine sentiment on certain topics, this isn’t prevalent enough to use as a proper label. However, different sentiments and biases could be found for individual users and Subreddits. Additionally, interesting associations and networks could be formed between the users and communities.\n\nNote that the terms “Reddit Users”, “Reddit Usernames”, and “Reddit Authors” will be used synonymously throughout this analysis.\n\n\n\nTo obtain the data from Reddit, web scraping and an API was utilized. A web scraping function was built which mimics a general search within the Reddit platform and returns the URLs for threads on a given search query. The URLs were then iterated through with the API, which returned the posts, comments, replies, and other supporting information.\nThree search queries were made:\n\nStudent Loan Forgiveness\nStudent Loans\nIs a College Degree Worth It\n\nHowever, Is a College Degree Worth It data may not be utilized, as it’s not as precise to the topic of this analysis.\nSpecific objects and tags were created with API calls, and ultimately returned as a dataframe.\nSample of the Raw Reddit Data\n\n\n\n\n    \n      \n      url\n      title\n      original\n      self\n      post_date\n      comments\n      author\n      id\n      upvotes\n      content\n      comment_date\n      replying_to\n      subreddit\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\nThe main cleaning that was required was for the content; posts, comments, and replies. The following processing occurred:\n\nIf the first post on a thread returned blank, this likely indicated a link to an outside source. In this case, the title was used as the content.\nValues within the content strings such as line breaks and reply indicators were removed.\nEmail addresses and links were removed.\nEmojis and other non-ASCII characters were removed.\nDuplicate posts were common and were removed.\n\n\nThis initial cleaning step still left the individual posts, comments, or replies as rows themselves.\n\nGiven that quite a few rows of content could be a few words, or even a single word reply, the idea for the next processing step was to have content from each author for each thread aggregated together. Additionally, data maps were created to track communications between authors. For each thread, the following restructuring was performed for a given author:\n\nLists:\n\nUpvotes (can be positive or negative)\nPosting Dates\nContent (posts, comments, and replies)\nReplies To (who the author has replied to)\nReplies From (who responded to the author)\n\nValues for Specific Threads:\n\nURL\nTitle\nSubreddit\nAuthor (Reddit username)\nOriginal Author (boolean for if the author started the thread - made the original post)\n\n\nA sample of the Restructured Reddit data\n\n\n\n\n    \n      \n      url\n      title\n      subreddit\n      author\n      original_author\n      author_upvotes\n      author_dates\n      author_content\n      author_content_aggregated\n      replies_to\n      replies_from\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_reddit.html#gathering-the-data",
    "href": "data_reddit.html#gathering-the-data",
    "title": "Data Acquisition - Reddit",
    "section": "",
    "text": "To obtain the data from Reddit, web scraping and an API was utilized. A web scraping function was built which mimics a general search within the Reddit platform and returns the URLs for threads on a given search query. The URLs were then iterated through with the API, which returned the posts, comments, replies, and other supporting information.\nThree search queries were made:\n\nStudent Loan Forgiveness\nStudent Loans\nIs a College Degree Worth It\n\nHowever, Is a College Degree Worth It data may not be utilized, as it’s not as precise to the topic of this analysis.\nSpecific objects and tags were created with API calls, and ultimately returned as a dataframe.\nSample of the Raw Reddit Data\n\n\n\n\n    \n      \n      url\n      title\n      original\n      self\n      post_date\n      comments\n      author\n      id\n      upvotes\n      content\n      comment_date\n      replying_to\n      subreddit\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_reddit.html#cleaning-and-restructuring",
    "href": "data_reddit.html#cleaning-and-restructuring",
    "title": "Data Acquisition - Reddit",
    "section": "",
    "text": "The main cleaning that was required was for the content; posts, comments, and replies. The following processing occurred:\n\nIf the first post on a thread returned blank, this likely indicated a link to an outside source. In this case, the title was used as the content.\nValues within the content strings such as line breaks and reply indicators were removed.\nEmail addresses and links were removed.\nEmojis and other non-ASCII characters were removed.\nDuplicate posts were common and were removed.\n\n\nThis initial cleaning step still left the individual posts, comments, or replies as rows themselves.\n\nGiven that quite a few rows of content could be a few words, or even a single word reply, the idea for the next processing step was to have content from each author for each thread aggregated together. Additionally, data maps were created to track communications between authors. For each thread, the following restructuring was performed for a given author:\n\nLists:\n\nUpvotes (can be positive or negative)\nPosting Dates\nContent (posts, comments, and replies)\nReplies To (who the author has replied to)\nReplies From (who responded to the author)\n\nValues for Specific Threads:\n\nURL\nTitle\nSubreddit\nAuthor (Reddit username)\nOriginal Author (boolean for if the author started the thread - made the original post)\n\n\nA sample of the Restructured Reddit data\n\n\n\n\n    \n      \n      url\n      title\n      subreddit\n      author\n      original_author\n      author_upvotes\n      author_dates\n      author_content\n      author_content_aggregated\n      replies_to\n      replies_from\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_reddit.html#top-ten-authors-across-scenarios",
    "href": "data_reddit.html#top-ten-authors-across-scenarios",
    "title": "Data Acquisition - Reddit",
    "section": "Top Ten Authors Across Scenarios",
    "text": "Top Ten Authors Across Scenarios\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom left to right in the above images:\n\nMost Frequent Authors Across Reddit Queries: illustrates the top ten most frequent for the total number of posts, comments, and replies by author across all Reddit threads.\nMost Frequent Authors Across Multiple Threads: illustrates the the top ten authors who have at least a single piece of content in a Reddit thread.\nMost Frequent Authors Across Multiple Subreddits: illustrates the the top ten authors who have at least a single piece of content in a Subreddit.\n\nThere were a few takeaways from these illustrations. Although many authors only had a single appearance, there were a few who frequently post in these Reddit topic-specific communities. One user is actually a Reddit sanctioned bot, known as a Moderator, which helps to control inappropriate or misplaced discussion. This bot, AutoModerator, has its post shown in orange above and will be removed in the subsequent analyses."
  },
  {
    "objectID": "arm.html",
    "href": "arm.html",
    "title": "Association Rule Mining (ARM)",
    "section": "",
    "text": "Association Rule Mining (ARM) is a technique used to find and quantify relationships within sets, specifically the occurrence of events together. Relationships are represented by rules, which are direction based implications of subsets. In other words, probabilistically quantifying if and how much a set or subset implies another set or subset.\n\\(Rule: consequent \\rightarrow antecent\\)\nThe main quantifying measures of cooccurence are:\n\nSupport: the proportion of an item or set of items occuring together out of the entire dataset.\n\nRange: [0, 1]\n\nConfidence: quantifies the likelihood an itemset’s consequent occurs given its antecent (conditional probability of a consequent occurring giving its antecent).\n\nRange: [0, 1]\n\nLift: assesses the performance of an association rule by quantifying an improvement (or degradation) from the initial prediction, where the initial prediction is the support of the antecent.\n\nLift &lt; 1: indicates a negative correlation, or the rules are simply uncorrelated.\nLift = 1: indicates independence between the rules, absolutely no correlation. If something is everywhere, it is nowhere. If an item is in every single set as antecedent, lift will always be 1.\nLift &gt; 1: indicates a positive correlation, or improvement of the initial rule. This shows a valid association. A high lift value inidcates that the association rule is more significant, the itemsets are highly dependent on each other.\n\n\n\n\nSupport for a Single Itemset\n\\[Support(I) = \\frac{\\text{number of sets containing I}}{\\text{total number of sets}}\\]\nSupport for an Association Rule\n\\[Support(A \\rightarrow C) = \\frac{\\text{number of sets containing A and C}}{\\text{total number of sets}}\\]\nConfidence for an Association Rule\n\\[Confidence(A \\rightarrow C) = \\frac{\\text{number of sets containing A and C}}{\\text{proportion of sets containing A}}\\]\n\\[Confidence(A \\rightarrow C) = P(C|A) = \\frac{P(CA)}{P(A)}\\]\nNote: the intersection is not the traditional probability definition of intersection, but that it contains every element in itemsets A and C.\nLift for an Association Rule\n\\[Lift(A \\rightarrow C) = \\frac{Confidence(C \\rightarrow A)}{Support(C)} = \\frac{Support(C \\rightarrow A)}{Support(C)Support(A)}\\]\n\nARM is another unsupervised machine learning method. The goal is to explore the data further and gain information by examining associations. The goal is to use the NewsAPI news articles and one of the Reddit schemas in an attempt to better understand what is being discussed."
  },
  {
    "objectID": "arm.html#formulas",
    "href": "arm.html#formulas",
    "title": "Association Rule Mining (ARM)",
    "section": "",
    "text": "Support for a Single Itemset\n\\[Support(I) = \\frac{\\text{number of sets containing I}}{\\text{total number of sets}}\\]\nSupport for an Association Rule\n\\[Support(A \\rightarrow C) = \\frac{\\text{number of sets containing A and C}}{\\text{total number of sets}}\\]\nConfidence for an Association Rule\n\\[Confidence(A \\rightarrow C) = \\frac{\\text{number of sets containing A and C}}{\\text{proportion of sets containing A}}\\]\n\\[Confidence(A \\rightarrow C) = P(C|A) = \\frac{P(CA)}{P(A)}\\]\nNote: the intersection is not the traditional probability definition of intersection, but that it contains every element in itemsets A and C.\nLift for an Association Rule\n\\[Lift(A \\rightarrow C) = \\frac{Confidence(C \\rightarrow A)}{Support(C)} = \\frac{Support(C \\rightarrow A)}{Support(C)Support(A)}\\]\n\nARM is another unsupervised machine learning method. The goal is to explore the data further and gain information by examining associations. The goal is to use the NewsAPI news articles and one of the Reddit schemas in an attempt to better understand what is being discussed."
  },
  {
    "objectID": "arm.html#newsapi-transaction-data-snippet",
    "href": "arm.html#newsapi-transaction-data-snippet",
    "title": "Association Rule Mining (ARM)",
    "section": "NewsAPI Transaction Data Snippet",
    "text": "NewsAPI Transaction Data Snippet"
  },
  {
    "objectID": "arm.html#reddit-author-schema-transaction-data-snippet",
    "href": "arm.html#reddit-author-schema-transaction-data-snippet",
    "title": "Association Rule Mining (ARM)",
    "section": "Reddit Author Schema Transaction Data Snippet",
    "text": "Reddit Author Schema Transaction Data Snippet"
  },
  {
    "objectID": "arm.html#newsapi",
    "href": "arm.html#newsapi",
    "title": "Association Rule Mining (ARM)",
    "section": "NewsAPI",
    "text": "NewsAPI\nRules were created with:\n\nMinimum Support: 0.3\nMinimum Confidence: 0.6\nRules Created: 33,349"
  },
  {
    "objectID": "arm.html#reddit",
    "href": "arm.html#reddit",
    "title": "Association Rule Mining (ARM)",
    "section": "Reddit",
    "text": "Reddit\nRules were created with:\n\nMinimum Support: 0.05\nMinimum Confidence: 0.2\nRules Created: 232"
  },
  {
    "objectID": "arm.html#newsapi-top-support",
    "href": "arm.html#newsapi-top-support",
    "title": "Association Rule Mining (ARM)",
    "section": "NewsAPI Top Support",
    "text": "NewsAPI Top Support\n\n\n\n\n    \n      \n      rules\n      support\n      confidence\n      coverage\n      lift\n      count\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "arm.html#newsapi-top-confidence",
    "href": "arm.html#newsapi-top-confidence",
    "title": "Association Rule Mining (ARM)",
    "section": "NewsAPI Top Confidence",
    "text": "NewsAPI Top Confidence\n\n\n\n\n    \n      \n      rules\n      support\n      confidence\n      coverage\n      lift\n      count\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "arm.html#newsapi-top-lift",
    "href": "arm.html#newsapi-top-lift",
    "title": "Association Rule Mining (ARM)",
    "section": "NewsAPI Top Lift",
    "text": "NewsAPI Top Lift\n\n\n\n\n    \n      \n      rules\n      support\n      confidence\n      coverage\n      lift\n      count\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "arm.html#reddit-top-support",
    "href": "arm.html#reddit-top-support",
    "title": "Association Rule Mining (ARM)",
    "section": "Reddit Top Support",
    "text": "Reddit Top Support\n\n\n\n\n    \n      \n      rules\n      support\n      confidence\n      coverage\n      lift\n      count\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "arm.html#reddit-top-confidence",
    "href": "arm.html#reddit-top-confidence",
    "title": "Association Rule Mining (ARM)",
    "section": "Reddit Top Confidence",
    "text": "Reddit Top Confidence\n\n\n\n\n    \n      \n      rules\n      support\n      confidence\n      coverage\n      lift\n      count\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "arm.html#reddit-top-lift",
    "href": "arm.html#reddit-top-lift",
    "title": "Association Rule Mining (ARM)",
    "section": "Reddit Top Lift",
    "text": "Reddit Top Lift\n\n\n\n\n    \n      \n      rules\n      support\n      confidence\n      coverage\n      lift\n      count\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "arm.html#news-article-network",
    "href": "arm.html#news-article-network",
    "title": "Association Rule Mining (ARM)",
    "section": "News Article Network",
    "text": "News Article Network\nDue to a large amount of rules even with relatively high minimum thresholds, a network was visualized in a manner which illustrated different angles. Taking rules with Lift &gt; 1 (valid assocations), the 5 top rules by Lift from each quartile form the network."
  },
  {
    "objectID": "arm.html#reddit-author-schema-network",
    "href": "arm.html#reddit-author-schema-network",
    "title": "Association Rule Mining (ARM)",
    "section": "Reddit Author Schema Network",
    "text": "Reddit Author Schema Network\nGiven less total rules created with the Reddit assocations, the top 20 rules by Lift were used to create the network."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Author",
    "section": "",
    "text": "Hi, my name is Carl Klein, and I’m the author of this analysis. Below are links to some of my other projects along my data science journey."
  },
  {
    "objectID": "clustering.html",
    "href": "clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "This section will focus on clustering the text data described in Data Acquisition - NewsAPI and Data Acquisition - Reddit. As a whole, clustering is an unsupervised machine learning technique, which means that it attempts to find patterns, trends, and groupings within unlabeled data. Specifically, clustering is useful for identifying and partitioning the data through similarity measures. As such, there isn’t a candid expectation for what will be found, aside from what data is similar and what data is dissimilar. However, given the topic of student loan forgiveness has a strong divide along political party lines, politcal bias will be a subsequent point of exploration."
  },
  {
    "objectID": "clustering.html#clustering-methodologies",
    "href": "clustering.html#clustering-methodologies",
    "title": "Clustering",
    "section": "Clustering Methodologies",
    "text": "Clustering Methodologies\nK-Means Clustering and Hierarchical Clustering will be used to analyze news articles and Reddit posts. These are both unsupervised machine learning methods, meaning they take unlabeled data. Specifically, unlabeled numerical data.\n\nK-Means Clustering: A clustering algorithm with the goal of partitioning a dataset into a specified number of clusters, where each point belongs to the cluster with the nearest mean. Distance and “closeness” are usually evaluated via Euclidean Distance, as will be done with this analysis. In this scenario, each point will be a vectorized version of the text within a news article or reddit post (or subsequent aggregations). Vectorizing text data produces high dimension datasets. Using techniques based on Eucldiean Distance for high dimensionality data usually doesn’t produce great results. With that in mind, the vectorized data will be reduced via Principal Component Analysis (PCA). PCA is a data reduction technique which projects the data into an Eigenspace using a covariance matrix. The Eigenspace is built from orthonormal vectors known as Eigenvectors (or principal components) which actually represent the direction of explained variance from the original data. The strength (or magnitude) of explained variance is illustrated by the vector’s associated Eigenvalue. PCA will be used to reduce the high dimensional text data into just three principal components.\nHiearachical Clustering: A clustering algorithm which determines cluster assignments by building a hierachy and which can be illustrated with a tree-based visualization known as a dendrogram. This is a useful exploratory tool which shows where different groups of data are partitioned, highlighting similar and disimilar data at different levels. PCA will not be used in this process. Instead, a distance measure known as Cosine Similarity will be used on the vectorized text data as a whole. The Cosine Similarity measure is adept for high dimensional data.\n\n\nDistance Metrics\nEuclidean Distance\nGiven points \\(p\\) and \\(q\\) in any real dimensional space, Euclidean Distance is calculated by:\n\\[d(p, q) = \\sqrt{(p - q)^2}\\]\nCosine Similarity\nGiven vectors \\(x\\) and \\(y\\) in any real dimensional space, Cosine Similarity is calculated by:\n\\[S_C (x, y) = \\frac{x \\cdot y}{||x|| \\cdot ||y||}\\]"
  },
  {
    "objectID": "clustering.html#data-preparation",
    "href": "clustering.html#data-preparation",
    "title": "Clustering",
    "section": "Data Preparation",
    "text": "Data Preparation\nThere will be 4 different initial vectorized versions of the text data used:\n\nMaximum Features: A numerical vectorized version of the entire vocabulary.\nTenth of Maximum Features: A numerical vectorized version with a tenth of the maximum features of the entire vocabulary.\nIterative Latent Dirichlet Allocation (LDA):\n\nIdea: Latent Dirichlet Allocation will be iteratively performed such that each topic will have unique words. This will begin with the words from a Tenth of Maximum features wordset, and then \\(n\\) unique words across \\(t\\) topics up to a total of \\(m\\) desired words will be found. In essence, \\(m = t * n\\).\n3-Topic Iterative LDA with 150 Words will be used.\n5-Topic Iterative LDA with 150 Words will be used.\nNote that a section on a proper LDA Analysis is located here.\n\n\n\nThe strategy is to begin with K-Means, find the vectorized version which produces the best results for for K-Means, and use that vectorized version to initiate Hiearchical Clustering. Since K-Means will be utilizing PCA, the dataset produced from the PCA projection will be shown for all, and then just the best vectorized versions will be shown.\n\n\nNewsAPI Data\nFor the NewsAPI data (i.e., news articles), the data will be subset to the news sources which an overall political bias is known for, any labels will be removed, and then the vectorized versions will be created. The analysis will begin with the KMeans version, so the PCA projected data will be shown.\nData Before Transformations\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nNewsAPI PCA Maximum Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nNewsAPI PCA Tenth Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nNewsAPI PCA 3-Topic LDA Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nNewsAPI PCA 5-Topic Iterative LDA Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\nReddit Data\nFor the Reddit data, different aggregation schemas as described in the linked sections in the Overview will be used:\n\nReddit Base Schema: Author’s posts within a thread are aggregated.\nReddit Author Schema: Author’s posts across all threads are aggregated.\nReddit Thread Schema: Posts within a thread are aggregated.\nReddit Subreddit Schema: Threads within a Subreddit are aggregated.\n\n\nFor efficiency, just the snippets of the dataset which had the best K-Means clustering result will be illustrated. However, the snippets of all the versions can be found here.\n\nData Before Transformations\n\n\n\n\n    \n      \n      url\n      title\n      subreddit\n      author\n      original_author\n      author_upvotes\n      author_dates\n      author_content\n      author_content_aggregated\n      replies_to\n      replies_from\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nReddit PCA Base Schema: 5-Topic Iterative LDA Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nReddit PCA Author Schema: 5-Topic Iterative LDA Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nReddit PCA Thread Schema: 3-Topic Iterative LDA Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nReddit PCA Subreddit Schema: 3-Topic Iterative LDA Snippet\n\n\n\n\n    \n      \n      component_1\n      component_2\n      component_3\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "clustering.html#principal-component-analysis",
    "href": "clustering.html#principal-component-analysis",
    "title": "Clustering",
    "section": "Principal Component Analysis",
    "text": "Principal Component Analysis\nAgain, for K-Means Clusering, Principal Component Analysis (PCA) was used to reduce the text data to 3-Dimensions. The initial results show that when the maximum features were the entire vocabulary, there was a much lower amount of explained variance throughout the three principal components.\n\nNewsAPI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Base Schema\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Author Schema\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Thread Schema\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Subreddit Schema"
  },
  {
    "objectID": "clustering.html#silhouette-coefficients",
    "href": "clustering.html#silhouette-coefficients",
    "title": "Clustering",
    "section": "Silhouette Coefficients",
    "text": "Silhouette Coefficients\nSilhouette Coefficients are a decent metric to evaluate how well K-Means clustering has performed. It measures how similar a point is to the cluster it was assigned compared to the other clusters. A coefficient value has a range from -1 to 1, where 1 indicates a great cluster assignment. It’s calculated using the average values between points. Scikit-Learn’s documentation gives a brief summary of the calculation.\n\n\\(a\\): average intra-cluster distance\n\\(b\\): average nearest-cluster distance\n\\(s\\): silhouette score\n\\(S\\): average silhouette score over the entire dataset\n\\(n\\): size of sample (number of points in the entire dataset)\n\n\\(s = \\frac{b-a}{max(a, b)}\\)\n\\(S = \\frac{\\sum_{i=1}^n{s}}{n}\\)\n\nNewsAPI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Base Schema\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Author Schema\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Thread Schema\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReddit Subreddit Schema"
  },
  {
    "objectID": "clustering.html#choosing-cluster-values",
    "href": "clustering.html#choosing-cluster-values",
    "title": "Clustering",
    "section": "Choosing Cluster Values",
    "text": "Choosing Cluster Values\nChoosing the best combination of cluster value (k) and dataset combination took some further visualization. Although the Maximum Features (i.e., vectorized across all vocabulary, projected onto three component PCA, and then clustered with K-Means) produced some of the highest coefficient values, this was misleading. The combination resulted in a dense cluster near the origin of the principal components, with a few distant and isolated outliers. Of course the silhouette scores would be high in that case. In general, when there were a few distant and isolated outliers, a high silhouette score is misleading. Additionally, some of the lower k values (i.e., k=2 or k=3) didn’t separate the data in a meaningfuly manner after visual inspect.\n\nDue to this, a full visual inspection of the NewsAPI clustering results will be performed including how the best clustering and dataset combination was chosen. However, the Reddit clustering results will just illustrate the best clustering and dataset combination.\n\n\nNewsAPI\nPotential Vectorizing and Cluster Combinations:\n\nMaximum Dataset: 3 clusters\nTenth Dataset: 2 clusters or 6 clusters\n3 Topic Iterative LDA: 4 clusters\n5 Topic Iterative LDA: 2 or 4 clusters\n\n\nMaximum Dataset with 3 Clusters\n\n\n\n\n\nTenth Dataset with 2 Clusters\n\n\n\n\n\nTenth Dataset with 6 Clusters\n\n\n\n\n\n3 Topic Iterative LDA with 4 clusters\n\n\n\n\n\n5 Topic Iterative LDA with 2 clusters\n\n\n\n\n\n5 Topic Iterative LDA with 4 clusters\n\n\n\n\n\nChoice\nGiven the above visualizations, the 3 Topic Iterative LDA with 4 Clusters was chosen. Although there was an outlier which had its own cluster, this version partitioned the overall cluster around the origin well. Along with this, it had a relatively high silhouette score.\n\nA similar process was performed for the Reddit datasets, but for efficiency, just the choices will be presented. It seems that for all, the maximum features across the entire vocabular had the same result.\n\n\n\n\nReddit Base Schema\n5 Topic Iterative LDA with 3 Clusters\n\n\n\n\n\nReddit Author Schema\n5 Topic Iterative LDA with 4 Clusters\n\n\n\n\n\nReddit Thread Schema\n3 Topic Iterative LDA with 4 Clusters\n\n\n\n\n\nReddit Subreddit Schema\n3 Topic Iterative LDA with 2 Clusters"
  },
  {
    "objectID": "clustering.html#final-k-means-discussion",
    "href": "clustering.html#final-k-means-discussion",
    "title": "Clustering",
    "section": "Final K-Means Discussion",
    "text": "Final K-Means Discussion\nOverall, the Iterative LDA word sets performed well due to the outliers with higher amounts of text data, even with performing PCA. The question is, what was actually driving these splits? To better understand this, word clouds were created for the best dataset - cluster choice combinations. In general, there were splits between politics, money, and students’ and graduates’ lifestyles.\n\nNewsAPI\n\n\n\n\n\n\n\nReddit Base Schema\n\n\n\n\n\n\n\nReddit Author Schema\n\n\n\n\n\n\n\nReddit Thread Schema\n\n\n\n\n\n\n\nReddit Subreddit Schema"
  },
  {
    "objectID": "clustering.html#bringing-in-labels",
    "href": "clustering.html#bringing-in-labels",
    "title": "Clustering",
    "section": "Bringing in Labels",
    "text": "Bringing in Labels\nThere was some “success” with K-Means in that it did group together different variations of word frequency combinations well. However, how did it perform on clustering political bias with the news sources?\nUnfortunately, this led to highly imbalanced clusters which didn’t tend to pick up on the politcal undertones. Or, the political undertones weren’t present."
  },
  {
    "objectID": "clustering.html#newsapi-data-1",
    "href": "clustering.html#newsapi-data-1",
    "title": "Clustering",
    "section": "NewsAPI Data",
    "text": "NewsAPI Data\n\n\n\n\n    \n      \n      enrollment\n      going\n      life\n      doge\n      state\n      forbearance\n      process\n      social\n      biden\n      repayment\n      congress\n      earnings\n      bankruptcy\n      rubin\n      example\n      make\n      application\n      graduation\n      buyback\n      class\n      just\n      term\n      department\n      forginess\n      act\n      debt\n      equity\n      right\n      idr\n      median\n      acceptance\n      work\n      tord\n      security\n      income\n      loan\n      sa\n      ll\n      offer\n      office\n      unirsity\n      school\n      pause\n      expert\n      time\n      american\n      home\n      buy\n      apply\n      democrat\n      policy\n      cnge\n      think\n      budget\n      official\n      people\n      program\n      donald\n      memo\n      like\n      grant\n      credit\n      business\n      employment\n      total\n      pslf\n      score\n      president\n      pay\n      action\n      don\n      said\n      available\n      help\n      balance\n      public\n      administration\n      drin\n      ratio\n      option\n      cllenge\n      law\n      plan\n      new\n      white\n      insurance\n      agency\n      house\n      college\n      news\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "clustering.html#newsapi-clustering-results",
    "href": "clustering.html#newsapi-clustering-results",
    "title": "Clustering",
    "section": "NewsAPI Clustering Results",
    "text": "NewsAPI Clustering Results\nUsing the columns (words) from that dataset, the cleaned and lemmatized news articles were further reduced to only those features. These reduced articles were then saved as text files in a corpus folder for the hierarchical clustering process in R. The corpus can be found here.\nAfter performing hiearchical clustering, the results from 2-6 levels in the hiearchical structure were saved for further analysis.\n\n\n\n\n    \n      \n      File\n      cluster_2\n      cluster_3\n      cluster_4\n      cluster_5\n      cluster_6\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "clustering.html#newsapi-clustering-analysis",
    "href": "clustering.html#newsapi-clustering-analysis",
    "title": "Clustering",
    "section": "NewsAPI Clustering Analysis",
    "text": "NewsAPI Clustering Analysis\nGiven that there were a few hundred files within the corpus, it can be difficult to make sense of the initial dendrogram or results. However, if the labels are brought back in, this might help make better sense of how and why the splits are made. Political Bias was brought back in, resulting in an actual divergence among clusters according to where the majority of the labels were at. In this case, a majority of articles from sources with known political bias.\n\n\n\n\n\nAt the hiearchical level of six, there is a potential indicator that the methodology did pick up on political undertones. In fact, each of the political biases can have a unique pairing of clusters at this level. Compared to K-Means clustering, there is a much better balance overall.\n\n\n\n\n\n\n\n\n\n\nBy declaring these cluster pairs as seen above, the dendrogram can roughly be partitioned as such aditionally."
  },
  {
    "objectID": "clustering.html#reddit-extension",
    "href": "clustering.html#reddit-extension",
    "title": "Clustering",
    "section": "Reddit Extension",
    "text": "Reddit Extension\nIn an attempt to extend political bias to the Reddit text data, the NewsAPI dataset was further subset where political bias and cluster aligned, and then the different Reddit schemas were appended on for repeated clustering. To be specific, the Reddit data was reduced to the same optimal wordset and then saved into a same corpus folder as the further subset NewsAPI.\n\nContent Distributions\nTo help balance the clustering process, a few changes were made due to the distributions of the document lengths between news articles and reddit posts.\nOriginal Distribution\n\n\n\n\n\nThe following measures were taken for balancing:\n\nReddit Base Schema downsized to NewsAPI Number of Articles\nReddit Author Schema downsized to NewsAPI Number of Articles\nNewsAPI Articles aggregated by Source to better match Number of Reddit Threads and Reddit Subreddits.\n\nFollowing the above measures, the corpuses were created:\n\nNewsAPI with Reddit Author Schema Subset\nNewsAPI Source Schema with Reddit Thread Schema Subset\nNewsAPI Source Schema with Reddit Subreddit Schema Subset\n\nUpdated Distribution"
  },
  {
    "objectID": "clustering.html#newsapi-reddit-extension-clustering-analysis",
    "href": "clustering.html#newsapi-reddit-extension-clustering-analysis",
    "title": "Clustering",
    "section": "NewsAPI-Reddit Extension Clustering Analysis",
    "text": "NewsAPI-Reddit Extension Clustering Analysis\nEven after downsampling and aggregating, the distributions of content length of the cleaned, lemmatized, and informative versions were still far apart. Therefore, TF-IDF vectorization was performed. TF-IDF is a normalization technique which helps to mitigate the effect of document size variations. Essentially, it allocates a heavier weight towards words of higher importance rather than using counts like CountVectorizer.\nAfter performing Hierarchical Clustering on the combined schemas, the expectation of obtaining political bias was not what was revealed. Instead, there was a bifurcation between the news articles and Reddit posts! This is apparent as early as two to three hierarchical levels.\n\nNpte: The Reddit Base Schema wasn’t actually ran for this.\n\n\nNewsAPI with Reddit Author Schema Subset\n\n\n\n\n\n\n\nNewsAPI Source Schema with Reddit Thread Schema Subset\n\n\n\n\n\n\n\nNewsAPI Source Schema with Reddit Subreddit Schema Subset\n\n\n\n\n\nDue to the legibility of dendrograms, the NewsAPI Source Schema with Reddit Subreddit Schema Subset was plotted with labels altered to show their origins."
  },
  {
    "objectID": "data_newsapi.html",
    "href": "data_newsapi.html",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "Part of the discussion surrounding Student Loan Forgiveness is reporting done by different news media organizations. These organizations generally align with a certain political bias. With the realization that those on the left tend to support at least some form of student loan relief and those on the right tend to oppose it, it could be fruitful to pair news organizations with their political biases. To achieve this, two main sources of news related data were explored:\n\nNewsAPI\nAllSides\n\n\n\nNewsAPI maintains an application programming interfacae (API) which returns information on articles published around the world. For the scope of this analysis, two search parameters were queried from the API. Articles were gathered using the specific search parameter of student loan forgiveness, as well as a general search parameter of student loans.\nThe data is initially returned in JSON format, which was then turned into a dataframe and subsequently exported into a csv file.\n\n\nSample of the Raw NewsAPI Data\n\n\n\n\n    \n      \n      author\n      content\n      description\n      date\n      source\n      title\n      url\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nAuthor: author of the article, sometimes source of the article\nContent: truncated version of the article itself\nDescription: summary of the article\nDate: date the article was published\nSource: news media organization the article was published by\nTitle: the title of the article\nURL: the url of the article on the news media organization source\n\n\n\n\n\nOne limitation of NewsAPI is that it doesn’t return the content of an article in its entirety. To account for this, numerous web scrapers were built for sources which were both scraping-eligible and appropriate for this analysis. This did reduce the number of sources and articles in the data from NewsAPI, but did result in complete article content. A list of eligible sources and their related scrapers were iteratively called to populate a dataframe with the individual paragraphs from the url associated with the article. Procedurally, the initial scrape through will result in a dataframe where each paragraph from each scrapable article will have its own row.\n\n\nSample of the Raw Scraped Data\n\n\n\n\n    \n      \n      source\n      url\n      paragraph\n      paragraph_num\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nAbove shows a random sample of 5 rows after the scraping. Paragraphs and paragraph numbers are new compared to the initial extracted data.\n\nUsing the source and url columns as keys, these can recombined with the columns from the original extraction of data for future use.\n\n\n\n\n\nAllSides is a community-based political bias rating platform for media. Using web scraping, biases for the sources in the scraped data were accumulated. Pages (urls) containing information on sources were scraped and manually found, which were then in turned scraped for their specific bias ratings.\n\n\nSample of the AllSides Data\n\n\n\n\n    \n      \n      source\n      url\n      Bias Numeric\n      Bias Specific\n      Type\n      Region\n      Website\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nSource: Source as listed in AllSides\nURL: link to the rating page in AllSides\nBias Numeric: numeric version of the bias, from -6 to +6 (left to right, respectively)\nBias Specific: categorically binned version of the bias\nType: news media, etc.\nRegion: regional location and coverage area of the news media organization\nWebsite: home website of the news media organization\n\n\n\n\n\nThus far, raw data from an API and an array of web scraping functions has been gathered. This can now be combined together to create potential labels for the complete articles associated with the NewsAPI extraction. However, some data cleaning should be performed first.\n\n\nNewsAPI Extraction:\n\nauthor\n\nduplicate authors from same article removed\nemail addresses and links removed\nstripped of leading, trailing, and multiple spaces\nif author is None then the entry from the source column is used in place\nline breaks removed\n\ndescription\n\ntext before and including delimiters removed (i.e. delimiter could be “Date and Location –”)\n\ncontent\n\nignored in lieu of scraping the entire itself\n\n\nNewsAPI Scraping Extraction:\n\nparagraph\n\nblank paragraphs removed\nnon-breaking space characters removed\nstripped of leading, trailing, and multiple spaces\nall paragraphs of a single article combined post-cleaning\n\n\nAllSides Bias Ratings: no cleaning was required for this data, however a source-to-source map was created, as some sources were not identical between NewsAPI and AllSides.\n\n\n\nThe data with the completely scraped articles still retained the columns for source and url. Using those, the cleaned extraction columns can be merged in as well as the biases.\nThis resulted in a dataframe with the following columns:\n\nsource\nurl\narticle\nsource_bias\nBias Numeric\nBias Specific\nauthor\ndate\ntitle\nsearch\n\nWhere the text data itself will likely be article, but interesting analyses could be extended by using description or title.\nThe main label of interest will be Bias Specific, but interesting analyses could hinge upon using source, author, date, or search (topic query parameter).\nSample of the Final Labeled Data\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_newsapi.html#newsapi-extraction",
    "href": "data_newsapi.html#newsapi-extraction",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "NewsAPI maintains an application programming interfacae (API) which returns information on articles published around the world. For the scope of this analysis, two search parameters were queried from the API. Articles were gathered using the specific search parameter of student loan forgiveness, as well as a general search parameter of student loans.\nThe data is initially returned in JSON format, which was then turned into a dataframe and subsequently exported into a csv file.\n\n\nSample of the Raw NewsAPI Data\n\n\n\n\n    \n      \n      author\n      content\n      description\n      date\n      source\n      title\n      url\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nAuthor: author of the article, sometimes source of the article\nContent: truncated version of the article itself\nDescription: summary of the article\nDate: date the article was published\nSource: news media organization the article was published by\nTitle: the title of the article\nURL: the url of the article on the news media organization source"
  },
  {
    "objectID": "data_newsapi.html#newsapi-scraping",
    "href": "data_newsapi.html#newsapi-scraping",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "One limitation of NewsAPI is that it doesn’t return the content of an article in its entirety. To account for this, numerous web scrapers were built for sources which were both scraping-eligible and appropriate for this analysis. This did reduce the number of sources and articles in the data from NewsAPI, but did result in complete article content. A list of eligible sources and their related scrapers were iteratively called to populate a dataframe with the individual paragraphs from the url associated with the article. Procedurally, the initial scrape through will result in a dataframe where each paragraph from each scrapable article will have its own row.\n\n\nSample of the Raw Scraped Data\n\n\n\n\n    \n      \n      source\n      url\n      paragraph\n      paragraph_num\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nAbove shows a random sample of 5 rows after the scraping. Paragraphs and paragraph numbers are new compared to the initial extracted data.\n\nUsing the source and url columns as keys, these can recombined with the columns from the original extraction of data for future use."
  },
  {
    "objectID": "data_newsapi.html#allsides-bias-data",
    "href": "data_newsapi.html#allsides-bias-data",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "AllSides is a community-based political bias rating platform for media. Using web scraping, biases for the sources in the scraped data were accumulated. Pages (urls) containing information on sources were scraped and manually found, which were then in turned scraped for their specific bias ratings.\n\n\nSample of the AllSides Data\n\n\n\n\n    \n      \n      source\n      url\n      Bias Numeric\n      Bias Specific\n      Type\n      Region\n      Website\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nSource: Source as listed in AllSides\nURL: link to the rating page in AllSides\nBias Numeric: numeric version of the bias, from -6 to +6 (left to right, respectively)\nBias Specific: categorically binned version of the bias\nType: news media, etc.\nRegion: regional location and coverage area of the news media organization\nWebsite: home website of the news media organization"
  },
  {
    "objectID": "data_newsapi.html#cleaning-and-merging",
    "href": "data_newsapi.html#cleaning-and-merging",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "Thus far, raw data from an API and an array of web scraping functions has been gathered. This can now be combined together to create potential labels for the complete articles associated with the NewsAPI extraction. However, some data cleaning should be performed first.\n\n\nNewsAPI Extraction:\n\nauthor\n\nduplicate authors from same article removed\nemail addresses and links removed\nstripped of leading, trailing, and multiple spaces\nif author is None then the entry from the source column is used in place\nline breaks removed\n\ndescription\n\ntext before and including delimiters removed (i.e. delimiter could be “Date and Location –”)\n\ncontent\n\nignored in lieu of scraping the entire itself\n\n\nNewsAPI Scraping Extraction:\n\nparagraph\n\nblank paragraphs removed\nnon-breaking space characters removed\nstripped of leading, trailing, and multiple spaces\nall paragraphs of a single article combined post-cleaning\n\n\nAllSides Bias Ratings: no cleaning was required for this data, however a source-to-source map was created, as some sources were not identical between NewsAPI and AllSides.\n\n\n\nThe data with the completely scraped articles still retained the columns for source and url. Using those, the cleaned extraction columns can be merged in as well as the biases.\nThis resulted in a dataframe with the following columns:\n\nsource\nurl\narticle\nsource_bias\nBias Numeric\nBias Specific\nauthor\ndate\ntitle\nsearch\n\nWhere the text data itself will likely be article, but interesting analyses could be extended by using description or title.\nThe main label of interest will be Bias Specific, but interesting analyses could hinge upon using source, author, date, or search (topic query parameter).\nSample of the Final Labeled Data\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Many were told while growing up something along the lines of “Go to college and you’ll be set.” There was a promise that this would guarantee a certain quality of life along with meaningful, lucrative, and stable opportunities. The assurance made the venture seem like a solid investment in both time and money. For some, foregoing college is simply not an option. However, with the rising cost of higher education, a volatile job market, and current salaries not keeping up with inflation, some who bought into this dream via private and federal loans are having buyer’s remorse. Especially when interest is accounted for, some are even unable to afford their payments at all or may take longer than initially expected [1]. This begs the question, should the U.S. government provide student loan relief? Although this is a complex issue with many factors, there are generally two core views. One side argues “yes” or “does support” while the other argues “no” or “doesn’t support”. The stances are also generally drawn along political party lines, with the left supporting the idea more than the right. In fact, one national poll shows 58% of Democrats were in favor of relief, versus just 15% of Republicans [2].\n\n\n\n[3]\n\n\nHow are news organizations reporting on this? What is the general population saying about this? Given that the dichotomy extends along political lines, are there differences in dialogue between left-leaning and right-leaning entities? Points from both sides of the topic were laid out by Becca Stanek from the Week [2]. Some of the arguments supporting student loan forgiveness were of the cruelty surrounding the structure of student loans, improving economic equity, and that the cost of college and living, even when accounting for inflation, have increased dramatically since the generation before current students and recent graduates. Some of the arguments against the idea were unfairness towards those who have paid off their loans, that it doesn’t address the root problem, and that bulk relief may benefit those who actually do not need the financial aid. Many of those with student loan repayments are overwhelmed by the system and struggling to pursue life goals they thought an education would offer them [4]. These life goals include creating emergency or retirement funds, investing, owning a home, and postponing starting a family. However, many opponents ask the question, “Where does this money come from?” A number of Republicans say that government sponsored plans could “increase the federal budget and tax costs.” There’s an echo of unfairness in increasing taxes on those who don’t have student loans or didn’t even attend college.\n\n\n\n[5]\n\n\nIs there any common ground between the two sides of the arguments? What is the “cruelty” in the system that proponents use as an argument? Similarly, what is the underlying problem in the system that won’t be solved by forgiveness that opponents use an argument? In an article published by NPR, Cory Turner investigates fraudulent colleges. The article claims that for-profit trade schools which serve almost a quarter of federal student loan borrowers account for “44%” of defaults [6]. The author cites examples of Corinthian Colleges, ITT Technical Institute, and DeVry University, whose students have “wasted loans and worthless degrees” and may have been misled by false advertising. In the case of fraud, there seems to be more bilateral support. Fraudulent colleges aside, the cost of education is still rising, “having nearly tripled since 1980”, and the back and forth on forgiveness is frustrating to both sides of the discussion [7]. Although a single cause of this increase is hard to pinpoint, some are blaming the student aid system and the educational institutions themselves. A bulk forgiveness would be costly to taxpayers and even eliminating interest could be damaging. Instead, a recommendation was made to tighten lending requirements and condition borrowing limits on a “degree’s economic returns” and a “borrower’s academic merit.”\nAlthough this topic is highly divided along political party lines, it’s a complex issue with positive and negative sentiment stemming from facets outside of political party lines. As of late January of 2025, it was reported that Joe Biden had cancelled the student loans of “more than 5 million Americans - more than any other president in U.S. history.” This was over $180 billion in student loans [8]. However, with a change of administrations from President Biden to President Trump, the rules for debt forgiveness are tightening again. In some cases, the debt forgiveness programs are being eradicated completely. What happens to the individuals who held out on the promise of their student loans being at least partially forgiven and are now being faced with a different financial future? Although negative sentiment could still be placed along political party lines as one side is drastically changing the dynamics again, some are feeling the weight of a broken promise. Some of those who were facing financial hardships in the first place, or had been waiting on the promise of forgiveness, are now becoming the target of an administration set on collecting. For those in default, the government could begin garnishing wages and blocking other benefits some borrowers have become accustomed to. With “an average of 8.15 percent of student loan debt” in default at any given time, the government in colloquially calling in its chit. With access to some of these repayment plans being completely severed, it’s estimated that “some 8 million borrowers are in limbo” and may be facing “higher monthly costs” [9].\nAn aspect shared by both sides of the aisle is that government sanctioned student loan forgiveness doesn’t address the root cause of why its needed in the first place. In an article by Saddique Ansari from Economics Online, its stated that “there was a real rise of 213% in students’ average funding outlays” for traditional state colleges over the last three decades [10]. Some of the drivers of this increase referenced in the article include a decreased support from the states towards their public higher education institutions, administrative costs “outpacing” enrollment, expansion of services and amenities aimed at “enhancing the college experience” and attracting students, and the sheer available of federal student loans. The article states that the ease at which federal aid is procured has given colleges “less incentive to keep costs down”, essentially allowing them to increase attractiveness through these enhancements without considering an economic balance, especially for underrepresented classes seeking “upward mobility”. Even in the face of adversity, domestic and foreign students alike are choosing to pursue higher education in the United States. According to a 2025 ranking by The Times Higher Education, the United States was home to six out of ten of the top prestigious universities in the world. The United States is globally recognized in its academic excellence, research opportunities, and networking opportunities within its academic institutions. However, this obviously comes at a price. How do other countries with renowned higher education institutions compare? According to a 2025 ranking performed by the World Population Review, the United States has consistently fallen by international standards when it comes to education overall. The study indicates that “government spending on education has failed to keep up with inflation” and places the US at spot 31 on its list [11]. In fact, many nations within the European Union (EU) and the European Economic Area (EEA) land much higher in the rankings. Furthermore, many of these countries offer free college tuition to citizens from the EU, EEA, and Switzerland. Even if a student is not a citizen of these regions, the tuition is considerably cheaper compared to the United States [12].\nWhen it comes to student loan forgiveness and the student debt crisis in the United States, arguments from proponents and opponents alike have validity. Progress towards a viable permanent solution depends on a willingness to confront and discuss many complex issues.\n\n\n\n\nReferences\n\n1. Gonzales J (2021) “Like I’m drowning:” Five stories from the student debt crisis - Open Campus — opencampus.org\n\n\n2. Stanek B (2024) The pros and cons of student loan forgiveness — theweek.com\n\n\n3. Turner C (2023) Student loan forgiveness is on the way for more than 800,000 borrowers\n\n\n4. Guevara E (2025) Most Student Loan Borrowers Are Overwhelmed As Repayment Plans Keep Changing — investopedia.com\n\n\n5. Nelson L (2022) The \"fairness\" debate over student loan forgiveness, explained\n\n\n6. Turner C (2022) When colleges defaud students, should the goverment go after school executives\n\n\n7. Arnold N (2022) A Legislative Agenda to Fix The Student Loan System — newsweek.com\n\n\n8. Binkley C (2025) Millions have had student loans canceled under biden — despite the collapse of his forgiveness plan\n\n\n9. Blake S (2025) Student loans update: Government may garnish millions of borrowers’ wages\n\n\n10. Ansari S (2023) The causes and effects of exponentially increasing college tuition rates in the united states\n\n\n11. (2025) Education Rankings by Country 2025 — worldpopulationreview.com\n\n\n12. Goetz L (2025) 6 European Countries With Free College Tuition — investopedia.com"
  },
  {
    "objectID": "modeling_conclusion.html",
    "href": "modeling_conclusion.html",
    "title": "Modeling - Conclusions",
    "section": "",
    "text": "Introduction\nThis section serves as a fusion of the supervised modeling across the previous sections:\n\nModeling Preparation\nNaive Bayes\nDecision Trees\nSupport Vector Machines\n\nSpecifically, this will take the best binary classification models from Naive Bayes, Decision Trees, and Support Vector Machines. These binary classification models were trained and tested on the strict two label political biases (Left and Right). To iterate across the other sections, they ignore Lean Left, Center, and Lean Right political biases. Given that sentiment towards the topic of student loan forgiveness is generally split along political lines, the idea is that correlation can be projected onto Reddit authors, predicted by a collection of their posts.\nIn all three families of the supervised learning models, the strict two label political bias binary classifiers performed the best. By performing probabilistic weighting with the three models, an overall political bias of a Reddit author can be obtained, complete with a probability. Therefore, a general consensus of their sentiment towards this topic can be quantitatively calculated.\n\n\nProbabilistic Weighting\nUsing the binary classifiers for Right and Left political bias from the three models, a conclusive weighting metric was made.\nFor each Reddit Author:\n\n\\(C_L\\): count Left classifications\n\\(C_R\\): count Right classifications\n\\(P_L\\): product of Left probabilities, given a Left classification\n\\(P_R\\): product of Right probabilities, given a Right classification\n\\(W_L = C_L \\cdot P_L\\): weight of Left classifications\n\\(W_R = C_R \\cdot P_R\\): weight of Left classifications\n\nLet Right political bias be a negative value (as to represent negative sentiment), then the following scores and labels can be interpreted:\nSentiment Score (\\(S_s\\)):\n\\[S_s = W_L + (-W_R)\\]\nSentiment Label (\\(S_l\\)):\n\\[S_l = \\begin{cases} S_s \\leq -1 : \\text{Negative} \\\\ -1 &lt; S_s &lt; 1 : \\text{Neutral} \\\\ S_s \\geq 1 : \\text{Positive} \\end{cases}\\]\nUsing this probabilistic weighting method, the following results were derived:\n\n\n\n\n    \n      \n      Author\n      Left\n      Right\n      Sentiment Score\n      Sentiment Label\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nSentiment Gauge Overall\n\n\n\nComparative Gauges\n\n\nPositive Sentiment\n\n\n\n\nNeutral Sentiment\n\n\n\n\nNegative Sentiment\n\n\n\n\n\nReddit Author Content\n\n\n\n\n    \n      \n      Author\n      Content\n      Sentiment Score\n      Sentiment Label\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nConclusion\nBy fusing together several different well performing models built to analyze political bias within text, especially text surrounding the topic of Student Loan Forgiveness, overall weighted political bias scores for Reddit Authors were calculated. Given that this topic is generally split upon political party lines, with the Left (or liberal) showing more positive sentiment towards the topic and the Right (or conservative) showing more negative sentiment towards the topic, political bias was used in an attempt to project overall sentiment on the topic for Reddit Authors. Using this weighted political bias score, authors were grouped into the sentiment classes of Positive, Neutral, and Negative.\nGiven this a complex topic, with arguments from financial equity to blame shifting, there are general undertones in many of the authors’ contents that do appear to be positive or negative. Some of those with positive sentiment share the tones of the benefits of student loan forgiveness and are spiteful towards the Right side of the government for blocking this. Some of the neutral sentiment appears to be more targeted to sharing facts and helping others understand the topic better. The negative sentiment appears to contain authors who don’t want these costs passed onto to tax payers as a whole, have “worked off their loans, so they should too” themes, along with negative sentiment towards student loan forgiveness ultimately being a failed promise, and some that spiteful towards the Left side of the government.\nOverall, having several metrics strung together does seem to produce scores and capture sentiment decently, even given such a complex topic where there is rarely a definitive yes or no.\n\n\n\nCode Links\n\nModeling Functions\nModeling Conclusion"
  },
  {
    "objectID": "modeling_nb.html",
    "href": "modeling_nb.html",
    "title": "Modeling - Naive Bayes",
    "section": "",
    "text": "Naive Bayes is a generative supervised machine learning classification method. The generative aspect indicates that the model learns from the probability of the data given previous knowledge of the label. This is in comparison to a discriminative model, where the goal is to find a function which distinguishes between groups (i.e. Logistic Regression). The supervised aspect means that the model is given labels to learn from. The Naïve aspect comes from the assumption that the categories have conditional independence in order to apply the Bayes’ Theorem. This is a Naive assumption because it’s unlikely that the variables within the data have true independence. For example, consider a model built from customer reviews which is trying to classify if the review was positive or negative. The review might have language such as “happy” and “glad”, which are clearly not independent terms. However, the assumption of independence is made to allow for the calculations to work. This example is one of sentiment analysis, however there are many potential applications of this method as it is implicitly acceptable for n-class classification. For example, predicting weather a label is “true” or “false” is a 2-class problem, but Naive Bayes can extend this to multiple labels. Back to the sentiment analysis example, a review could be “positive”, “negative”, or “neutral”. Applications also include document classification, which could be used to classify an article into categories such as “politics”, “sports”, “entertainment”, among many other overall article types.\nIn general, Naive Bayes uses the conditional independence assumption to apply the Bayes’ Theorem. Essentially, the goal is to find the probability of a label given a datapoint. The Bayes’ Theorem is appropriate for this task and uses several components of the probabilities within the data to calculate this. Namely, the probability of the data itself occurring, the probability of the label occurring, and the conditional probability in the opposite direction (i.e. the probability of the datapoint given a label). Especially in larger datasets, some of the conditional probabilities can be zero. This presents an issue due to the multiplicative calculations required, which would zero out the entire probability. Smoothing techniques are used to account for this, with the Laplacian Correction being the most common. This technique adds 1 to each case’s count. The general smoothing technique adds a specified variable (or alpha) to the count.\nThis section of the analysis will specifically use Multinomial Naive Bayes Classification, which “is suitable for discrete features (e.g., word counts for text classification)” sklearn documentation."
  },
  {
    "objectID": "modeling_nb.html#five-labels",
    "href": "modeling_nb.html#five-labels",
    "title": "Modeling - Naive Bayes",
    "section": "Five Labels",
    "text": "Five Labels\n\n\n\n\n\nThe overall accuracy for testing across all five labels is 48.02%. Lean Left and Lean Right have a hard time being recognized correctly, as illustrated by the other categories being predicted almost as much as the others when that is the true label."
  },
  {
    "objectID": "modeling_nb.html#three-labels",
    "href": "modeling_nb.html#three-labels",
    "title": "Modeling - Naive Bayes",
    "section": "Three Labels",
    "text": "Three Labels\n\n\n\n\n\nWhen combining Lean Left and Lean Right with Left and Right, respectively, the model accuracy increasing to 57.27%. The model has a decent performance, with predictions for the Right political bias outperforming the other prediction categories. Left performed the worse."
  },
  {
    "objectID": "modeling_nb.html#strict-three-labels",
    "href": "modeling_nb.html#strict-three-labels",
    "title": "Modeling - Naive Bayes",
    "section": "Strict Three Labels",
    "text": "Strict Three Labels\n\n\n\n\n\nWhen the leaning political biases weren’t combined, the accuracy of the model increased by almost 20% for the three political bias labels. Overall decent results with its best performance on Center and Right labels, respectively."
  },
  {
    "objectID": "modeling_nb.html#two-labels",
    "href": "modeling_nb.html#two-labels",
    "title": "Modeling - Naive Bayes",
    "section": "Two Labels",
    "text": "Two Labels\n\n\n\n\n\nWhen combining Lean Left and Lean Right with Left and Right, respectively, and dropping the Center label, the model accuracy was 65.85%. There were still a high amount of incorrect predictions, which could be reflective on the leanings."
  },
  {
    "objectID": "modeling_nb.html#strict-two-labels",
    "href": "modeling_nb.html#strict-two-labels",
    "title": "Modeling - Naive Bayes",
    "section": "Strict Two Labels",
    "text": "Strict Two Labels\n\n\n\n\n\nWhen the leaning political biases weren’t combined, and dropping the the Center label, the accuracy of the model increased by almost 20% over the aggregated version of the two labels. This is a respectable model, and the best performance with the Naive Bayes classification in this section."
  },
  {
    "objectID": "modeling_nb.html#feature-importance-through-permutation",
    "href": "modeling_nb.html#feature-importance-through-permutation",
    "title": "Modeling - Naive Bayes",
    "section": "Feature Importance through Permutation",
    "text": "Feature Importance through Permutation\n\nThree Features - Strict Three Political Biases\n\n\n\n\n    \n      \n      feature\n      importance\n      absolute_importance\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nTwo Features - Strict Two Political Biases\n\n\n\n\n    \n      \n      feature\n      importance\n      absolute_importance\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nThe new Three Label Model will have 577 features and the new Two Label Model will have 280 features."
  },
  {
    "objectID": "modeling_nb.html#retrained-models",
    "href": "modeling_nb.html#retrained-models",
    "title": "Modeling - Naive Bayes",
    "section": "Retrained Models",
    "text": "Retrained Models\nThe retrained models with fewer features had roughly about the same accuracy.\n\nThree Features\n\n\n\n\n\n\n\nTwo Features"
  },
  {
    "objectID": "modeling_nb.html#reddit-projection-results",
    "href": "modeling_nb.html#reddit-projection-results",
    "title": "Modeling - Naive Bayes",
    "section": "Reddit Projection Results",
    "text": "Reddit Projection Results\n\n\n\n\n    \n      \n      Author\n      Predicted Bias Three\n      Threshold Three\n      Predicted Bias Two\n      Threshold Two\n      Threshold\n      Conclusion\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nThe above illustrates the predicions for both the three and two label models as well as their probabilities. Combining the results in the last two columns illustrates the overall political bias (including leanings). Recall that the Reddit data is unlabeled, so the probabilities act as sure the model is in predicting the political biases. Recall that political biases are correlated with sentiment, with the Right having a more negative sentiment and the Left having a more positive sentiment."
  },
  {
    "objectID": "modeling_svm.html",
    "href": "modeling_svm.html",
    "title": "Modeling - Support Vector Machines",
    "section": "",
    "text": "Support Vector Machines (SVMs) are supervised learning methods which transform features into a higher dimensional space to separate the labels. The usefulness of an SVM comes from when the input data in its original dimensional space isn’t linearly separable, but in a higher dimensional space there exists a hyperplane which can linearly separate the groups of the data.\nSVMs use a quadratic optimization algorithm, in which the final optimal dual form contains a dot product (or inner product). This allows for the use of kernels, which are functions that return an inner product in a higher dimensional space. Being able to apply kernels is essential, as just the solution to the dot product is needed and doesn’t actually need to be transformed into a higher dimensional space in practice.\nBeing able to use a dot product, and subsequently a kernel, allows for just the solution of the dot product to be used instead of actually transforming the data into a higher dimensional space. This makes SVMs highly efficient. Additionally, SVMs create a margin between the groups in the higher dimensional space. Any point on the margin is known as a support vector. Not only are they computationally efficient but they are also more resistant to outliers and noise due to this. Keep in mind that a single SVM is a binary classifier, however multiple SVMs can be ensembled together for more than a 2-class problem.\nSVMs can only work on labeled numeric data. First, an SVM is a supervised machine learning method. This means, that it can only be used on labeled data in order to train the model. Second, due to the mathematic nature of dot products, and subsequently kernels, the data must be numeric.\nSome of the common SVM kernels are:\n\nRBF (Radial Basis Function or Gaussian)\nPolynomial\nLinear\nSigmoid\n\nThis section of the analysis will specifically use Support Vector Machine Classification sklearn documentation."
  },
  {
    "objectID": "modeling_svm.html#hyperparameter-results",
    "href": "modeling_svm.html#hyperparameter-results",
    "title": "Modeling - Support Vector Machines",
    "section": "Hyperparameter Results",
    "text": "Hyperparameter Results\n\n\n\n\n    \n      \n      Label\n      mean_test_score\n      rank_test_score\n      param_C\n      param_degree\n      param_kernel\n      mean_fit_time\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "modeling_svm.html#hyperparameter-best-results",
    "href": "modeling_svm.html#hyperparameter-best-results",
    "title": "Modeling - Support Vector Machines",
    "section": "Hyperparameter Best Results",
    "text": "Hyperparameter Best Results\n\n\n\n\n    \n      \n      C\n      Degree\n      Kernel\n      Model\n      Accuracy\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nNote that the combined search has rankings for each model based not only on accuracy but aspects such model fitting time. The Hyperparameter Best Results will be used for the following modeling."
  },
  {
    "objectID": "modeling_svm.html#five-labels",
    "href": "modeling_svm.html#five-labels",
    "title": "Modeling - Support Vector Machines",
    "section": "Five Labels",
    "text": "Five Labels\n\n\n\n\n\nThe overall accuracy for testing across all five labels is about 55.51%. Predictions for Center performed overwhelmingly well. This could be indicative to the proportion of the labels as discussed in the modeling preparations page."
  },
  {
    "objectID": "modeling_svm.html#three-labels",
    "href": "modeling_svm.html#three-labels",
    "title": "Modeling - Support Vector Machines",
    "section": "Three Labels",
    "text": "Three Labels\n\n\n\n\n\nWhen combining Lean Left and Lean Right with Left and Right, respectively, the model accuracy increasing to 61.23%. The model has a decent performance, with predictions for the Right political bias outperforming the other prediction categories. Left performed the worse."
  },
  {
    "objectID": "modeling_svm.html#strict-three-labels",
    "href": "modeling_svm.html#strict-three-labels",
    "title": "Modeling - Support Vector Machines",
    "section": "Strict Three Labels",
    "text": "Strict Three Labels\n\n\n\n\n\nWhen the leaning political biases weren’t combined, the accuracy of the model increased by over 10% for the three political bias labels. Overall decent results with its best performance on Right labels."
  },
  {
    "objectID": "modeling_svm.html#two-labels",
    "href": "modeling_svm.html#two-labels",
    "title": "Modeling - Support Vector Machines",
    "section": "Two Labels",
    "text": "Two Labels\n\n\n\n\n\nWhen combining Lean Left and Lean Right with Left and Right, respectively, and dropping the Center label, the model accuracy was about 60%. There were still a high amount of incorrect predictions, which could be reflective on the leanings."
  },
  {
    "objectID": "modeling_svm.html#strict-two-labels",
    "href": "modeling_svm.html#strict-two-labels",
    "title": "Modeling - Support Vector Machines",
    "section": "Strict Two Labels",
    "text": "Strict Two Labels\n\n\n\n\n\nWhen the leaning political biases weren’t combined, and dropping the the Center label, the accuracy of the model was phenomenonal! On this particular training and testing set, there was a 96% accuracy. This is a respectable model, and the best performance with the SVM classification in this section. It should be noted that this model on the grid search performance was only just above 80% accuracy when averaged across several different training and testing datasets."
  },
  {
    "objectID": "modeling_svm.html#feature-importance-through-permutation",
    "href": "modeling_svm.html#feature-importance-through-permutation",
    "title": "Modeling - Support Vector Machines",
    "section": "Feature Importance through Permutation",
    "text": "Feature Importance through Permutation\n\nThree Features - Strict Three Political Biases\n\n\n\n\n    \n      \n      feature\n      importance\n      absolute_importance\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nTwo Features - Strict Two Political Biases\n\n\n\n\n    \n      \n      feature\n      importance\n      absolute_importance\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nThe new Three Label Model will have 198 features and the new Two Label Model will have 351 features."
  },
  {
    "objectID": "modeling_svm.html#retrained-models",
    "href": "modeling_svm.html#retrained-models",
    "title": "Modeling - Support Vector Machines",
    "section": "Retrained Models",
    "text": "Retrained Models\nThe retrained models with fewer features had roughly about the same accuracy (when compared to the grid search cross validation results).\n\nThree Features\n\n\n\n\n\n\n\nTwo Features"
  },
  {
    "objectID": "modeling_svm.html#reddit-projection-results",
    "href": "modeling_svm.html#reddit-projection-results",
    "title": "Modeling - Support Vector Machines",
    "section": "Reddit Projection Results",
    "text": "Reddit Projection Results\n\n\n\n\n    \n      \n      Author\n      Predicted Bias Three\n      Threshold Three\n      Predicted Bias Two\n      Threshold Two\n      Threshold\n      Conclusion\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\nThe above illustrates the predicions for both the three and two label models as well as their probabilities. Combining the results in the last two columns illustrates the overall political bias (including leanings). Recall that the Reddit data is unlabeled, so the probabilities act as sure the model is in predicting the political biases. Recall that political biases are correlated with sentiment, with the Right having a more negative sentiment and the Left having a more positive sentiment."
  },
  {
    "objectID": "scripts/arm/arm_r.html",
    "href": "scripts/arm/arm_r.html",
    "title": "Association Rule Mining (ARM) using R",
    "section": "",
    "text": "Code\n# general settings\noptions(width=1000)\n\n\n\n\nCode\n# import packages\nlibrary(viridis)\nlibrary(arules)\nlibrary(TSP)\nlibrary(data.table)\nlibrary(tcltk)\nlibrary(dplyr)\nlibrary(devtools)\nlibrary(purrr)\nlibrary(tidyr)\nlibrary(visNetwork)\nlibrary(arulesViz)\nlibrary(RColorBrewer)\n\n\nWarning message:\n\"package 'viridis' was built under R version 4.3.3\"\nLoading required package: viridisLite\n\nWarning message:\n\"package 'arules' was built under R version 4.3.3\"\nLoading required package: Matrix\n\nWarning message:\n\"package 'Matrix' was built under R version 4.3.3\"\n\nAttaching package: 'arules'\n\n\nThe following objects are masked from 'package:base':\n\n    abbreviate, write\n\n\nWarning message:\n\"package 'TSP' was built under R version 4.3.3\"\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:data.table':\n\n    between, first, last\n\n\nThe following objects are masked from 'package:arules':\n\n    intersect, recode, setdiff, setequal, union\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nWarning message:\n\"package 'devtools' was built under R version 4.3.3\"\nLoading required package: usethis\n\nWarning message:\n\"package 'usethis' was built under R version 4.3.3\"\n\nAttaching package: 'purrr'\n\n\nThe following object is masked from 'package:data.table':\n\n    transpose\n\n\n\nAttaching package: 'tidyr'\n\n\nThe following objects are masked from 'package:Matrix':\n\n    expand, pack, unpack\n\n\nWarning message:\n\"package 'visNetwork' was built under R version 4.3.3\"\nWarning message:\n\"package 'arulesViz' was built under R version 4.3.3\"\n\n\n\nNewsAPI\n\n\nCode\ntransaction_newsapi &lt;- read.transactions('transaction_data/transaction_newsapi.csv',\n                                         rm.duplicates = FALSE,\n                                         format = 'basket',\n                                         sep = ',',\n                                         cols = NULL)\n\n\n\n\nCode\nrules_newsapi &lt;- arules::apriori(transaction_newsapi, parameter = list(support=0.3, confidence=0.6, minlen=2))\n\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n        0.6    0.1    1 none FALSE            TRUE       5     0.3      2\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 101 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[150 item(s), 339 transaction(s)] done [0.00s].\nsorting and recoding items ... [92 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 4 5 6 7 8 done [0.02s].\nwriting ... [33349 rule(s)] done [0.04s].\ncreating S4 object  ... done [0.01s].\n\n\n\n\nCode\nrules_newsapi_df &lt;- as(rules_newsapi, \"data.frame\")\n\n\n\n\nCode\ntop_support &lt;- sort(rules_newsapi, by='support', decreasing=TRUE)\ninspect(top_support[1:15])\n\n\n     lhs                rhs       support   confidence coverage  lift      count\n[1]  {loan}          =&gt; {student} 0.9587021 0.9878419  0.9705015 1.0209708 325  \n[2]  {student}       =&gt; {loan}    0.9587021 0.9908537  0.9675516 1.0209708 325  \n[3]  {year}          =&gt; {student} 0.7522124 0.9732824  0.7728614 1.0059230 255  \n[4]  {student}       =&gt; {year}    0.7522124 0.7774390  0.9675516 1.0059230 255  \n[5]  {year}          =&gt; {loan}    0.7492625 0.9694656  0.7728614 0.9989327 254  \n[6]  {loan}          =&gt; {year}    0.7492625 0.7720365  0.9705015 0.9989327 254  \n[7]  {loan, year}    =&gt; {student} 0.7433628 0.9921260  0.7492625 1.0253985 252  \n[8]  {student, year} =&gt; {loan}    0.7433628 0.9882353  0.7522124 1.0182728 252  \n[9]  {loan, student} =&gt; {year}    0.7433628 0.7753846  0.9587021 1.0032648 252  \n[10] {federal}       =&gt; {loan}    0.7286136 0.9840637  0.7404130 1.0139745 247  \n[11] {loan}          =&gt; {federal} 0.7286136 0.7507599  0.9705015 1.0139745 247  \n[12] {federal}       =&gt; {student} 0.7256637 0.9800797  0.7404130 1.0129482 246  \n[13] {student}       =&gt; {federal} 0.7256637 0.7500000  0.9675516 1.0129482 246  \n[14] {said}          =&gt; {loan}    0.7197640 0.9878543  0.7286136 1.0178802 244  \n[15] {loan}          =&gt; {said}    0.7197640 0.7416413  0.9705015 1.0178802 244  \n\n\n\n\nCode\nsupport_df &lt;- rules_newsapi_df[order(-rules_newsapi_df$support), ]\ntop_support_df &lt;- head(support_df, 15)\nwrite.csv(top_support_df, 'newsapi_top_support.csv', row.names = FALSE)\n\n\n\n\nCode\ntop_confidence &lt;- sort(rules_newsapi, by='confidence', decreasing=TRUE)\ninspect(top_confidence[1:15])\n\n\n     lhs                     rhs       support   confidence coverage  lift     count\n[1]  {tax}                =&gt; {loan}    0.3097345 1          0.3097345 1.030395 105  \n[2]  {grant}              =&gt; {loan}    0.3303835 1          0.3303835 1.030395 112  \n[3]  {act}                =&gt; {loan}    0.3392330 1          0.3392330 1.030395 115  \n[4]  {likely}             =&gt; {student} 0.3421829 1          0.3421829 1.033537 116  \n[5]  {republican}         =&gt; {student} 0.3864307 1          0.3864307 1.033537 131  \n[6]  {forginess}          =&gt; {loan}    0.4365782 1          0.4365782 1.030395 148  \n[7]  {forginess}          =&gt; {student} 0.4365782 1          0.4365782 1.033537 148  \n[8]  {borrower}           =&gt; {loan}    0.4247788 1          0.4247788 1.030395 144  \n[9]  {borrower}           =&gt; {student} 0.4247788 1          0.4247788 1.033537 144  \n[10] {donald}             =&gt; {trump}   0.4277286 1          0.4277286 1.606635 145  \n[11] {biden}              =&gt; {loan}    0.5191740 1          0.5191740 1.030395 176  \n[12] {education}          =&gt; {student} 0.6017699 1          0.6017699 1.033537 204  \n[13] {free, loan}         =&gt; {student} 0.3097345 1          0.3097345 1.033537 105  \n[14] {repayment, student} =&gt; {loan}    0.3067847 1          0.3067847 1.030395 104  \n[15] {student, tax}       =&gt; {loan}    0.3067847 1          0.3067847 1.030395 104  \n\n\n\n\nCode\nconfidence_df &lt;- rules_newsapi_df[order(-rules_newsapi_df$confidence), ]\ntop_confidence_df &lt;- head(confidence_df, 15)\nwrite.csv(top_confidence_df, 'newsapi_top_confidence.csv', row.names = FALSE)\n\n\n\n\nCode\ntop_lift &lt;- sort(rules_newsapi, by='lift', decreasing=TRUE)\ninspect(top_lift[1:15])\n\n\n     lhs                                       rhs      support   confidence coverage  lift     count\n[1]  {administration, house, student}       =&gt; {white}  0.3451327 0.8068966  0.4277286 1.967899 117  \n[2]  {administration, house, loan}          =&gt; {white}  0.3421829 0.8055556  0.4247788 1.964628 116  \n[3]  {administration, house, loan, student} =&gt; {white}  0.3421829 0.8055556  0.4247788 1.964628 116  \n[4]  {house, president, student, trump}     =&gt; {white}  0.3008850 0.8031496  0.3746313 1.958761 102  \n[5]  {administration, house}                =&gt; {white}  0.3451327 0.8013699  0.4306785 1.954420 117  \n[6]  {house, president, trump}              =&gt; {white}  0.3008850 0.7968750  0.3775811 1.943458 102  \n[7]  {house, loan, trump}                   =&gt; {white}  0.3097345 0.7954545  0.3893805 1.939993 105  \n[8]  {house, loan, student, trump}          =&gt; {white}  0.3097345 0.7954545  0.3893805 1.939993 105  \n[9]  {house, student, trump}                =&gt; {white}  0.3126844 0.7910448  0.3952802 1.929239 106  \n[10] {house, trump}                         =&gt; {white}  0.3126844 0.7851852  0.3982301 1.914948 106  \n[11] {federal, office, president, trump}    =&gt; {donald} 0.3038348 0.7984496  0.3805310 1.866720 103  \n[12] {house, loan, president}               =&gt; {white}  0.3156342 0.7642857  0.4129794 1.863977 107  \n[13] {house, loan, president, student}      =&gt; {white}  0.3156342 0.7642857  0.4129794 1.863977 107  \n[14] {house, president, student}            =&gt; {white}  0.3185841 0.7605634  0.4188791 1.854899 108  \n[15] {office, president, trump}             =&gt; {donald} 0.3274336 0.7928571  0.4129794 1.853645 111  \n\n\n\n\nCode\nlift_df &lt;- rules_newsapi_df[order(-rules_newsapi_df$lift), ]\ntop_lift_df &lt;- head(lift_df, 15)\nwrite.csv(top_lift_df, 'newsapi_top_lift.csv', row.names = FALSE)\n\n\n\n\nCode\nhigh_lift &lt;- subset(top_lift, lift &gt; 1)\n\n\n\n\nCode\nhigh_lift_quartiles &lt;- quantile(quality(high_lift)$lift, probs = c(0.25, 0.5, 0.75))\n\n\n\n\nCode\nhigh_lift_quartiles\n\n\n25%1.0335365853658550%1.1467230443974675%1.27181274900398\n\n\n\n\nCode\nfirst_quartile_lift &lt;- subset(high_lift, lift &lt;= high_lift_quartiles[1])\nsecond_quartile_lift &lt;- subset(high_lift, lift &lt;= high_lift_quartiles[2])\nthird_quartile_lift &lt;- subset(high_lift, lift &lt;= high_lift_quartiles[3])\nfourth_quartile_lift &lt;- subset(high_lift, lift &gt; high_lift_quartiles[3])\n\n\n\n\nCode\ntop_quartile_rules &lt;- c(first_quartile_lift[1:5],\n                        second_quartile_lift[1:5],\n                        third_quartile_lift[1:5],\n                        fourth_quartile_lift[1:5])\n\n\n\n\nCode\ninspect(top_quartile_rules)\n\n\n     lhs                                                       rhs          support   confidence coverage  lift     count\n[1]  {likely}                                               =&gt; {student}    0.3421829 1.0000000  0.3421829 1.033537 116  \n[2]  {republican}                                           =&gt; {student}    0.3864307 1.0000000  0.3864307 1.033537 131  \n[3]  {forginess}                                            =&gt; {student}    0.4365782 1.0000000  0.4365782 1.033537 148  \n[4]  {borrower}                                             =&gt; {student}    0.4247788 1.0000000  0.4247788 1.033537 144  \n[5]  {education}                                            =&gt; {student}    0.6017699 1.0000000  0.6017699 1.033537 204  \n[6]  {biden}                                                =&gt; {program}    0.3775811 0.7272727  0.5191740 1.146723 128  \n[7]  {donald, president}                                    =&gt; {program}    0.3067847 0.7272727  0.4218289 1.146723 104  \n[8]  {biden, loan}                                          =&gt; {program}    0.3775811 0.7272727  0.5191740 1.146723 128  \n[9]  {plan, student}                                        =&gt; {program}    0.4247788 0.7272727  0.5840708 1.146723 144  \n[10] {donald, president, trump}                             =&gt; {program}    0.3067847 0.7272727  0.4218289 1.146723 104  \n[11] {office, said, student, trump}                         =&gt; {federal}    0.3333333 0.9416667  0.3539823 1.271813 113  \n[12] {administration, education, president, trump}          =&gt; {federal}    0.3333333 0.9416667  0.3539823 1.271813 113  \n[13] {loan, office, said, student, trump}                   =&gt; {federal}    0.3333333 0.9416667  0.3539823 1.271813 113  \n[14] {administration, education, president, student, trump} =&gt; {federal}    0.3333333 0.9416667  0.3539823 1.271813 113  \n[15] {office, said}                                         =&gt; {department} 0.3038348 0.7202797  0.4218289 1.271744 103  \n[16] {administration, house, student}                       =&gt; {white}      0.3451327 0.8068966  0.4277286 1.967899 117  \n[17] {administration, house, loan}                          =&gt; {white}      0.3421829 0.8055556  0.4247788 1.964628 116  \n[18] {administration, house, loan, student}                 =&gt; {white}      0.3421829 0.8055556  0.4247788 1.964628 116  \n[19] {house, president, student, trump}                     =&gt; {white}      0.3008850 0.8031496  0.3746313 1.958761 102  \n[20] {administration, house}                                =&gt; {white}      0.3451327 0.8013699  0.4306785 1.954420 117  \n\n\n\n\nCode\n# network plotting - set lhs, rhs\ntop_quartile_df &lt;- as(top_quartile_rules, 'data.frame')\ntop_quartile_df$lhs &lt;- labels(lhs(top_quartile_rules))\ntop_quartile_df$rhs &lt;- labels(rhs(top_quartile_rules))\n\n\n\n\nnodes\nnodes &lt;- data.frame(id = unique(c(top_quartile_df\\(lhs, top_quartile_df\\)rhs)), label = unique(c(top_quartile_df\\(lhs, top_quartile_df\\)rhs)))\n\n\nedges\nedges &lt;- data.frame(from = top_quartile_df\\(lhs, to = top_quartile_df\\)rhs)\nnewsapi_network &lt;- visNetwork(nodes, edges) %&gt;% visEdges(arrows = ‘to’) %&gt;% visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE)\nnewsapi_network\nvisSave(newsapi_network, file = ‘newsapi_network.html’)\n\n\nCode\n# Add quartile information to top_quartile_df\ntop_quartile_df$quartile &lt;- rep(c(\"Q1\", \"Q2\", \"Q3\", \"Q4\"), each = 5)\n\n# Nodes with size based on quartile\nnodes &lt;- data.frame(id = unique(c(top_quartile_df$lhs, top_quartile_df$rhs)),\n                    label = unique(c(top_quartile_df$lhs, top_quartile_df$rhs)),\n                    size = ifelse(unique(c(top_quartile_df$lhs, top_quartile_df$rhs)) %in% top_quartile_df$rhs[top_quartile_df$quartile == \"Q1\"], 10,\n                                  ifelse(unique(c(top_quartile_df$lhs, top_quartile_df$rhs)) %in% top_quartile_df$rhs[top_quartile_df$quartile == \"Q2\"], 20,\n                                         ifelse(unique(c(top_quartile_df$lhs, top_quartile_df$rhs)) %in% top_quartile_df$rhs[top_quartile_df$quartile == \"Q3\"], 30, 40))),\n                    color = ifelse(unique(c(top_quartile_df$lhs, top_quartile_df$rhs)) %in% top_quartile_df$lhs, \"lightsalmon\", \"lightblue\"))\n\n# Edges\nedges &lt;- data.frame(from = top_quartile_df$lhs, to = top_quartile_df$rhs)\n\n# Create legend nodes with circular shapes\nlegend_nodes &lt;- data.frame(label = c(\"Q1 (Lift &lt;= 1.03)\", \"Q2 (Lift &lt;= 1.15)\", \"Q3 (Lift &lt;= 1.27)\", \"Q4 (Lift &gt; 1.27)\"),\n                           size = c(10, 20, 30, 40),\n                           color = \"lightblue\",\n                           shape = \"dot\")\n\n# Plot network without legend nodes\nnewsapi_network &lt;- visNetwork(nodes, edges) %&gt;% \n  visEdges(arrows = 'to') %&gt;% \n  visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE) %&gt;%\n  visNodes(label = NULL) %&gt;%\n  visLegend(addNodes = legend_nodes, useGroups = FALSE, main = \"Lift Quartiles\")\n\n\n\n\nCode\nnewsapi_network\n\n\n\n\n    \n        \n        \n\n\n\n    \n    \n        \n\n    \n\n\n\n\n\nCode\nvisSave(newsapi_network, file = 'newsapi_network.html')\n\n\n\n\nReddit - Author\n\n\nCode\ntransaction_reddit &lt;- read.transactions('transaction_data/transaction_reddit_author.csv',\n                                        rm.duplicates = FALSE,\n                                        format = 'basket',\n                                        sep = ',',\n                                        cols = NULL)\n\n\n\n\nCode\nrules_reddit &lt;- arules::apriori(transaction_reddit, parameter = list(support=0.05, confidence=0.2, minlen=2))\n\n\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen maxlen target  ext\n        0.2    0.1    1 none FALSE            TRUE       5    0.05      2     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 304 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[150 item(s), 6087 transaction(s)] done [0.00s].\nsorting and recoding items ... [54 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [232 rule(s)] done [0.01s].\ncreating S4 object  ... done [0.00s].\n\n\n\n\nCode\nrules_reddit_df &lt;- as(rules_reddit, \"data.frame\")\n\n\n\n\nCode\ntop_support &lt;- sort(rules_reddit, by='support', decreasing=TRUE)\ninspect(top_support[1:15])\n\n\n     lhs            rhs       support   confidence coverage  lift     count\n[1]  {student}   =&gt; {loan}    0.2493839 0.8945197  0.2787909 1.897853 1518 \n[2]  {loan}      =&gt; {student} 0.2493839 0.5291042  0.4713323 1.897853 1518 \n[3]  {year}      =&gt; {loan}    0.1537703 0.6455172  0.2382126 1.369559  936 \n[4]  {loan}      =&gt; {year}    0.1537703 0.3262461  0.4713323 1.369559  936 \n[5]  {pay}       =&gt; {loan}    0.1476918 0.6557257  0.2252341 1.391217  899 \n[6]  {loan}      =&gt; {pay}     0.1476918 0.3133496  0.4713323 1.391217  899 \n[7]  {people}    =&gt; {loan}    0.1412847 0.5624591  0.2511911 1.193339  860 \n[8]  {loan}      =&gt; {people}  0.1412847 0.2997560  0.4713323 1.193339  860 \n[9]  {just}      =&gt; {loan}    0.1356990 0.5720222  0.2372269 1.213628  826 \n[10] {loan}      =&gt; {just}    0.1356990 0.2879052  0.4713323 1.213628  826 \n[11] {like}      =&gt; {loan}    0.1112206 0.5836207  0.1905701 1.238236  677 \n[12] {loan}      =&gt; {like}    0.1112206 0.2359707  0.4713323 1.238236  677 \n[13] {debt}      =&gt; {loan}    0.1099064 0.5868421  0.1872844 1.245071  669 \n[14] {loan}      =&gt; {debt}    0.1099064 0.2331823  0.4713323 1.245071  669 \n[15] {forginess} =&gt; {loan}    0.1062921 0.8107769  0.1310991 1.720181  647 \n\n\n\n\nCode\nsupport_df &lt;- rules_reddit_df[order(-rules_reddit_df$support), ]\ntop_support_df &lt;- head(support_df, 15)\nwrite.csv(top_support_df, 'reddit_top_support.csv', row.names = FALSE)\n\n\n\n\nCode\ntop_confidence &lt;- sort(rules_reddit, by='confidence', decreasing=TRUE)\ninspect(top_confidence[1:15])\n\n\n     lhs                     rhs    support    confidence coverage   lift     count\n[1]  {payment, student}   =&gt; {loan} 0.05092821 0.9779180  0.05207820 2.074795  310 \n[2]  {forginess, student} =&gt; {loan} 0.06768523 0.9716981  0.06965665 2.061599  412 \n[3]  {pay, student}       =&gt; {loan} 0.08394940 0.9428044  0.08904222 2.000296  511 \n[4]  {paid, student}      =&gt; {loan} 0.05404961 0.9400000  0.05749959 1.994346  329 \n[5]  {make, student}      =&gt; {loan} 0.06177099 0.9353234  0.06604239 1.984424  376 \n[6]  {student, year}      =&gt; {loan} 0.08707081 0.9281961  0.09380647 1.969303  530 \n[7]  {just, student}      =&gt; {loan} 0.07869230 0.9229287  0.08526368 1.958127  479 \n[8]  {people, student}    =&gt; {loan} 0.08378512 0.9222423  0.09084935 1.956671  510 \n[9]  {money, student}     =&gt; {loan} 0.05306391 0.9124294  0.05815673 1.935851  323 \n[10] {like, student}      =&gt; {loan} 0.06850665 0.9104803  0.07524232 1.931716  417 \n[11] {don, student}       =&gt; {loan} 0.06193527 0.9084337  0.06817808 1.927374  377 \n[12] {student}            =&gt; {loan} 0.24938393 0.8945197  0.27879087 1.897853 1518 \n[13] {college, student}   =&gt; {loan} 0.05322819 0.8686327  0.06127813 1.842930  324 \n[14] {debt, student}      =&gt; {loan} 0.07425661 0.8496241  0.08739938 1.802601  452 \n[15] {forgin}             =&gt; {loan} 0.06867094 0.8244576  0.08329226 1.749206  418 \n\n\n\n\nCode\nconfidence_df &lt;- rules_reddit_df[order(-rules_reddit_df$confidence), ]\ntop_confidence_df &lt;- head(confidence_df, 15)\nwrite.csv(top_confidence_df, 'reddit_top_confidence.csv', row.names = FALSE)\n\n\n\n\nCode\ntop_lift &lt;- sort(rules_reddit, by='lift', decreasing=TRUE)\ninspect(top_lift[1:15])\n\n\n     lhs                   rhs         support    confidence coverage   lift     count\n[1]  {loan, year}       =&gt; {payment}   0.05388533 0.3504274  0.15377033 2.643186 328  \n[2]  {loan, payment}    =&gt; {year}      0.05388533 0.5815603  0.09265648 2.441350 328  \n[3]  {debt, loan}       =&gt; {student}   0.07425661 0.6756353  0.10990636 2.423448 452  \n[4]  {college, loan}    =&gt; {student}   0.05322819 0.6652977  0.08000657 2.386369 324  \n[5]  {forginess, loan}  =&gt; {student}   0.06768523 0.6367852  0.10629210 2.284096 412  \n[6]  {loan, make}       =&gt; {student}   0.06177099 0.6362098  0.09709216 2.282032 376  \n[7]  {payment}          =&gt; {year}      0.07097092 0.5353160  0.13257762 2.247220 432  \n[8]  {year}             =&gt; {payment}   0.07097092 0.2979310  0.23821258 2.247220 432  \n[9]  {like, loan}       =&gt; {student}   0.06850665 0.6159527  0.11122063 2.209372 417  \n[10] {don, loan}        =&gt; {student}   0.06193527 0.6120130  0.10119928 2.195240 377  \n[11] {debt, loan}       =&gt; {pay}       0.05306391 0.4828102  0.10990636 2.143593 323  \n[12] {loan, people}     =&gt; {student}   0.08378512 0.5930233  0.14128471 2.127126 510  \n[13] {just, loan}       =&gt; {student}   0.07869230 0.5799031  0.13569903 2.080065 479  \n[14] {payment, student} =&gt; {loan}      0.05092821 0.9779180  0.05207820 2.074795 310  \n[15] {loan, student}    =&gt; {forginess} 0.06768523 0.2714097  0.24938393 2.070265 412  \n\n\n\n\nCode\nlift_df &lt;- rules_reddit_df[order(-rules_reddit_df$lift), ]\ntop_lift_df &lt;- head(lift_df, 15)\nwrite.csv(top_lift_df, 'reddit_top_lift.csv', row.names = FALSE)\n\n\n\n\nCode\n# network plotting - set lhs, rhs\nnetwork_df &lt;- as(top_lift[1:20], 'data.frame')\nnetwork_df$lhs &lt;- labels(lhs(top_lift[1:20]))\nnetwork_df$rhs &lt;- labels(rhs(top_lift[1:20]))\n\n\n\n\nCode\n# nodes\nnodes &lt;- data.frame(id = unique(c(network_df$lhs, network_df$rhs)),\n                    label = unique(c(network_df$lhs, network_df$rhs)))\n\n# edges\nedges &lt;- data.frame(from = network_df$lhs, to = network_df$rhs)\n\n\n\n\nCode\nreddit_network &lt;- visNetwork(nodes, edges) %&gt;% visEdges(arrows = 'to') %&gt;% visOptions(highlightNearest = TRUE, nodesIdSelection = TRUE)\n\n\n\n\nCode\nreddit_network\n\n\n\n\n    \n        \n        \n\n\n\n    \n    \n        \n\n    \n\n\n\n\n\nCode\nvisSave(reddit_network, file = 'reddit_network.html')"
  },
  {
    "objectID": "vectorization_newsapi.html",
    "href": "vectorization_newsapi.html",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Using the data which has been prepared and merged with potential labels, as seen in Data Acquisition - NewsAPI, a few more steps can be taken to turn the news articles into numerical representations which can then be used for further analyses and machine learning applications.\n\n\nRecall that the prepared data looks like this:\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nThe main label of interest for this data is Political Bias, however other potential labels include:\n\nNews Organization Source\nAuthor\nDate\nSearch Query Parameter\n\nThe data itself will be the News Article, however other potential data sources include:\n\nTitle\nDescription\n\nThis page will focus just on the entire News Article for data, but it could be worth comparing Title and Description in the future.\nFor this text data, the additional preprocessing will take place for each article:\n\nRemove line breaks.\nRemove punctuation.\nRemove words containing numbers.\nRemove standalone numbers.\nRemove leading and trailing spaces.\nLowercase the remaining words.\nRemove any single-length text remaining.\n\n\n\n\nNow that the articles have been properly prepared to create a vectorized dataframe, several versions will be created. Namely, word count dataframes will be created using CountVectorizer() and normalized word count dataframes will be created using TfidfVectorizer(), both from scikit-learn. Stopwords will be removed using these functions as well. Dataframes will be further subsetted along the political bias labels. Lemmatizing and Stemming will also be used to create different versions available for further analyses. One additional option could be further versions of maximum words allowed in a dataframe.\n\n\n\nThese versions will vectorize the completely preprocessed NewsAPI data in its entirety, remove stopwords and use a maximum of 200 features. A sample of the datasets for both vectorized versions will be shown. Additionally, a wordcloud and a top ten feature visualization will be shown for the CountVectorizer() versions.\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      acceptance\n      access\n      according\n      account\n      act\n      administration\n      agencies\n      agency\n      aid\n      american\n      americans\n      assistance\n      balance\n      bank\n      based\n      best\n      biden\n      billion\n      borrowers\n      budget\n      business\n      card\n      changes\n      class\n      college\n      company\n      congress\n      cost\n      costs\n      country\n      court\n      cr\n      credit\n      crore\n      current\n      data\n      day\n      debt\n      december\n      department\n      development\n      did\n      does\n      doge\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      employees\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expenses\n      faculty\n      family\n      federal\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      funds\n      future\n      going\n      good\n      government\n      graduation\n      grants\n      growth\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      institutions\n      insurance\n      job\n      just\n      know\n      ll\n      loan\n      loans\n      location\n      long\n      low\n      lower\n      make\n      making\n      management\n      market\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      months\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      office\n      options\n      order\n      pause\n      pay\n      payment\n      payments\n      people\n      personal\n      plan\n      plans\n      policy\n      power\n      president\n      private\n      process\n      profit\n      program\n      programs\n      provide\n      pslf\n      public\n      quarter\n      rate\n      rates\n      ratio\n      read\n      real\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      rs\n      said\n      save\n      say\n      says\n      school\n      schools\n      sector\n      security\n      seen\n      service\n      services\n      set\n      social\n      spending\n      start\n      state\n      states\n      student\n      students\n      support\n      tax\n      term\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      want\n      washington\n      way\n      week\n      white\n      work\n      workers\n      working\n      year\n      years\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      acceptance\n      access\n      according\n      account\n      act\n      administration\n      agencies\n      agency\n      aid\n      american\n      americans\n      assistance\n      balance\n      bank\n      based\n      best\n      biden\n      billion\n      borrowers\n      budget\n      business\n      card\n      changes\n      class\n      college\n      company\n      congress\n      cost\n      costs\n      country\n      court\n      cr\n      credit\n      crore\n      current\n      data\n      day\n      debt\n      december\n      department\n      development\n      did\n      does\n      doge\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      employees\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expenses\n      faculty\n      family\n      federal\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      funds\n      future\n      going\n      good\n      government\n      graduation\n      grants\n      growth\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      institutions\n      insurance\n      job\n      just\n      know\n      ll\n      loan\n      loans\n      location\n      long\n      low\n      lower\n      make\n      making\n      management\n      market\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      months\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      office\n      options\n      order\n      pause\n      pay\n      payment\n      payments\n      people\n      personal\n      plan\n      plans\n      policy\n      power\n      president\n      private\n      process\n      profit\n      program\n      programs\n      provide\n      pslf\n      public\n      quarter\n      rate\n      rates\n      ratio\n      read\n      real\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      rs\n      said\n      save\n      say\n      says\n      school\n      schools\n      sector\n      security\n      seen\n      service\n      services\n      set\n      social\n      spending\n      start\n      state\n      states\n      student\n      students\n      support\n      tax\n      term\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      want\n      washington\n      way\n      week\n      white\n      work\n      workers\n      working\n      year\n      years\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      access\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      balance\n      bank\n      based\n      benefit\n      biden\n      billion\n      borrower\n      budget\n      business\n      card\n      case\n      challenge\n      change\n      class\n      college\n      come\n      community\n      company\n      congress\n      consumer\n      cost\n      country\n      court\n      cr\n      credit\n      crore\n      cut\n      data\n      day\n      debt\n      december\n      decision\n      democrat\n      department\n      development\n      did\n      doe\n      dollar\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      effort\n      employee\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expert\n      faculty\n      family\n      federal\n      finance\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      future\n      going\n      good\n      government\n      graduation\n      grant\n      group\n      growth\n      ha\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      know\n      law\n      legal\n      life\n      like\n      likely\n      ll\n      loan\n      location\n      long\n      look\n      low\n      lower\n      make\n      making\n      management\n      market\n      mean\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      mortgage\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      offer\n      office\n      opportunity\n      option\n      order\n      pause\n      pay\n      payment\n      people\n      personal\n      plan\n      policy\n      power\n      president\n      price\n      private\n      profit\n      program\n      provide\n      pslf\n      public\n      quarter\n      rate\n      ratio\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      risk\n      rule\n      said\n      save\n      saving\n      say\n      school\n      sector\n      security\n      service\n      social\n      spending\n      start\n      state\n      statement\n      student\n      support\n      tax\n      term\n      thing\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      worker\n      working\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      access\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      balance\n      bank\n      based\n      benefit\n      biden\n      billion\n      borrower\n      budget\n      business\n      card\n      case\n      challenge\n      change\n      class\n      college\n      come\n      community\n      company\n      congress\n      consumer\n      cost\n      country\n      court\n      cr\n      credit\n      crore\n      cut\n      data\n      day\n      debt\n      december\n      decision\n      democrat\n      department\n      development\n      did\n      doe\n      dollar\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      effort\n      employee\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expert\n      faculty\n      family\n      federal\n      finance\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      future\n      going\n      good\n      government\n      graduation\n      grant\n      group\n      growth\n      ha\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      know\n      law\n      legal\n      life\n      like\n      likely\n      ll\n      loan\n      location\n      long\n      look\n      low\n      lower\n      make\n      making\n      management\n      market\n      mean\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      mortgage\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      offer\n      office\n      opportunity\n      option\n      order\n      pause\n      pay\n      payment\n      people\n      personal\n      plan\n      policy\n      power\n      president\n      price\n      private\n      profit\n      program\n      provide\n      pslf\n      public\n      quarter\n      rate\n      ratio\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      risk\n      rule\n      said\n      save\n      saving\n      say\n      school\n      sector\n      security\n      service\n      social\n      spending\n      start\n      state\n      statement\n      student\n      support\n      tax\n      term\n      thing\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      worker\n      working\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      accept\n      access\n      accord\n      account\n      act\n      action\n      addit\n      administr\n      agenc\n      aid\n      allow\n      american\n      ani\n      anoth\n      balanc\n      bank\n      base\n      becaus\n      befor\n      benefit\n      biden\n      billion\n      borrow\n      budget\n      busi\n      card\n      challeng\n      chang\n      colleg\n      come\n      commun\n      compani\n      congress\n      consid\n      consum\n      continu\n      cost\n      countri\n      court\n      creat\n      credit\n      crore\n      current\n      cut\n      data\n      day\n      debt\n      democrat\n      depart\n      develop\n      don\n      dure\n      earn\n      econom\n      educ\n      effect\n      effort\n      employ\n      end\n      enrol\n      equiti\n      execut\n      expect\n      expens\n      faculti\n      famili\n      feder\n      financ\n      financi\n      forgiv\n      free\n      fund\n      futur\n      gener\n      good\n      govern\n      graduat\n      grant\n      growth\n      ha\n      health\n      help\n      hi\n      high\n      higher\n      home\n      hous\n      howev\n      idr\n      impact\n      import\n      includ\n      incom\n      increas\n      job\n      just\n      know\n      law\n      legal\n      like\n      limit\n      live\n      loan\n      locat\n      long\n      look\n      lower\n      major\n      make\n      manag\n      mani\n      market\n      mean\n      million\n      money\n      month\n      monthli\n      nation\n      need\n      new\n      news\n      number\n      offer\n      offic\n      onli\n      opportun\n      option\n      order\n      paus\n      pay\n      payment\n      peopl\n      person\n      plan\n      polici\n      power\n      presid\n      privat\n      process\n      profit\n      program\n      project\n      propos\n      provid\n      pslf\n      public\n      qualifi\n      quarter\n      rate\n      receiv\n      relief\n      remain\n      repay\n      report\n      republican\n      requir\n      research\n      right\n      rs\n      rule\n      said\n      save\n      say\n      school\n      sector\n      secur\n      servic\n      set\n      share\n      sinc\n      social\n      spend\n      start\n      state\n      student\n      support\n      tax\n      term\n      thi\n      think\n      time\n      total\n      tri\n      trump\n      undergradu\n      univers\n      use\n      wa\n      want\n      way\n      week\n      white\n      work\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      accept\n      access\n      accord\n      account\n      act\n      action\n      addit\n      administr\n      agenc\n      aid\n      allow\n      american\n      ani\n      anoth\n      balanc\n      bank\n      base\n      becaus\n      befor\n      benefit\n      biden\n      billion\n      borrow\n      budget\n      busi\n      card\n      challeng\n      chang\n      colleg\n      come\n      commun\n      compani\n      congress\n      consid\n      consum\n      continu\n      cost\n      countri\n      court\n      creat\n      credit\n      crore\n      current\n      cut\n      data\n      day\n      debt\n      democrat\n      depart\n      develop\n      don\n      dure\n      earn\n      econom\n      educ\n      effect\n      effort\n      employ\n      end\n      enrol\n      equiti\n      execut\n      expect\n      expens\n      faculti\n      famili\n      feder\n      financ\n      financi\n      forgiv\n      free\n      fund\n      futur\n      gener\n      good\n      govern\n      graduat\n      grant\n      growth\n      ha\n      health\n      help\n      hi\n      high\n      higher\n      home\n      hous\n      howev\n      idr\n      impact\n      import\n      includ\n      incom\n      increas\n      job\n      just\n      know\n      law\n      legal\n      like\n      limit\n      live\n      loan\n      locat\n      long\n      look\n      lower\n      major\n      make\n      manag\n      mani\n      market\n      mean\n      million\n      money\n      month\n      monthli\n      nation\n      need\n      new\n      news\n      number\n      offer\n      offic\n      onli\n      opportun\n      option\n      order\n      paus\n      pay\n      payment\n      peopl\n      person\n      plan\n      polici\n      power\n      presid\n      privat\n      process\n      profit\n      program\n      project\n      propos\n      provid\n      pslf\n      public\n      qualifi\n      quarter\n      rate\n      receiv\n      relief\n      remain\n      repay\n      report\n      republican\n      requir\n      research\n      right\n      rs\n      rule\n      said\n      save\n      say\n      school\n      sector\n      secur\n      servic\n      set\n      share\n      sinc\n      social\n      spend\n      start\n      state\n      student\n      support\n      tax\n      term\n      thi\n      think\n      time\n      total\n      tri\n      trump\n      undergradu\n      univers\n      use\n      wa\n      want\n      way\n      week\n      white\n      work\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\n\nLemmatizing seems to aggregate the text data while retaining meaning in the words. For instance, students are aggregated into student and loans into loan, while words like education are not reduced to something like educ. Moving forward, lemmatizing is a logical preprocessing step. Thus, lemmatization and stopwords removal for creating vectorized versions of the political bias will be used. For the process of creating subsets of the political bias data, there are two methods that could be used.\n\nSubsetting Second: vectorize the entire dataset \\(\\rightarrow\\) append labels \\(\\rightarrow\\) subset on political bias\nSubsetting First: subset the dataset on political bias \\(\\rightarrow\\) vectorize the subset \\(\\rightarrow\\) append labels\n\nBy subsetting first, the maximum word count will be reflective of the corpus associated with the respective political bias. Therefore, it might be more useful in this comparative analysis to subset first.\nAdditionally, CountVectorizer() will be used over TfidfVectorizer() for a first pass. A normalized version of the features could be useful in some cases, such as when dealing with varying sizes of content (i.e. total word count of content), but feature appearance counts will be more useful for visualizing in this analysis.\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      abolition\n      act\n      administration\n      agency\n      aggregated\n      america\n      american\n      assistance\n      athlete\n      biden\n      big\n      billion\n      billionaire\n      budget\n      business\n      california\n      car\n      care\n      case\n      change\n      child\n      civil\n      class\n      clear\n      come\n      company\n      congress\n      control\n      country\n      court\n      cut\n      day\n      debt\n      democracy\n      democrat\n      democratic\n      department\n      did\n      district\n      doe\n      doesn\n      doing\n      dollar\n      don\n      donald\n      economic\n      education\n      effect\n      effort\n      election\n      employee\n      end\n      family\n      far\n      federal\n      feel\n      financial\n      food\n      forecast\n      free\n      freeze\n      fund\n      funding\n      going\n      good\n      gop\n      government\n      grant\n      group\n      ha\n      hard\n      having\n      health\n      help\n      home\n      house\n      idea\n      including\n      income\n      individual\n      industry\n      insurance\n      ivy\n      job\n      just\n      justice\n      kid\n      kind\n      know\n      labor\n      law\n      le\n      league\n      likely\n      limit\n      little\n      loan\n      long\n      lot\n      maga\n      major\n      majority\n      make\n      making\n      marcotte\n      mean\n      meat\n      medicaid\n      member\n      memo\n      men\n      middle\n      million\n      money\n      month\n      musk\n      national\n      need\n      new\n      number\n      office\n      opportunity\n      order\n      parent\n      party\n      past\n      pause\n      pay\n      people\n      percent\n      person\n      place\n      plan\n      point\n      policy\n      political\n      poor\n      power\n      prediction\n      president\n      program\n      project\n      provide\n      public\n      question\n      read\n      really\n      repeal\n      republican\n      right\n      said\n      sargent\n      say\n      school\n      security\n      senate\n      service\n      social\n      spending\n      start\n      state\n      student\n      suicide\n      support\n      tariff\n      tax\n      term\n      thing\n      think\n      time\n      told\n      trillion\n      trump\n      trying\n      tuesday\n      union\n      use\n      ve\n      vote\n      voter\n      wa\n      want\n      way\n      wealth\n      week\n      white\n      woman\n      work\n      working\n      workplace\n      world\n      wrote\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      access\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      assistance\n      bank\n      bankruptcy\n      based\n      benefit\n      best\n      biden\n      big\n      billion\n      borrower\n      brennan\n      budget\n      business\n      california\n      card\n      case\n      cfpb\n      change\n      child\n      college\n      come\n      community\n      company\n      congress\n      consumer\n      cost\n      country\n      court\n      credit\n      cut\n      data\n      day\n      debt\n      decision\n      democrat\n      department\n      did\n      doe\n      doesn\n      doge\n      dollar\n      don\n      donald\n      education\n      effort\n      employee\n      end\n      equity\n      executive\n      expert\n      family\n      federal\n      feel\n      finance\n      financial\n      forgiveness\n      free\n      fund\n      funding\n      future\n      getting\n      going\n      good\n      got\n      government\n      grant\n      group\n      ha\n      hard\n      health\n      help\n      high\n      higher\n      home\n      house\n      including\n      income\n      information\n      insurance\n      investment\n      issue\n      job\n      just\n      know\n      like\n      likely\n      line\n      ll\n      loan\n      long\n      look\n      low\n      make\n      making\n      management\n      margaret\n      market\n      mcmahon\n      mean\n      member\n      memo\n      million\n      money\n      month\n      mortgage\n      musk\n      national\n      need\n      new\n      number\n      offer\n      office\n      option\n      order\n      parent\n      pause\n      pay\n      payment\n      people\n      plan\n      policy\n      political\n      power\n      president\n      price\n      private\n      process\n      program\n      project\n      public\n      question\n      rate\n      really\n      relief\n      repayment\n      report\n      republican\n      retirement\n      right\n      risk\n      rule\n      said\n      save\n      saving\n      say\n      school\n      secretary\n      security\n      senator\n      service\n      social\n      spending\n      start\n      state\n      student\n      support\n      sure\n      tax\n      team\n      term\n      thing\n      think\n      time\n      today\n      told\n      took\n      trump\n      trying\n      tuesday\n      university\n      use\n      used\n      ve\n      vought\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      worker\n      working\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      acceptance\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      application\n      apply\n      available\n      average\n      balance\n      based\n      benefit\n      best\n      biden\n      billion\n      borrower\n      budget\n      business\n      buy\n      buyback\n      card\n      case\n      certain\n      challenge\n      change\n      check\n      college\n      come\n      congress\n      consumer\n      cost\n      court\n      credit\n      current\n      cut\n      data\n      day\n      debt\n      department\n      different\n      doe\n      don\n      driven\n      earnings\n      education\n      eligible\n      employment\n      end\n      enrolled\n      enrollment\n      example\n      executive\n      expense\n      expert\n      faculty\n      federal\n      financial\n      forbearance\n      forgiven\n      forgiveness\n      free\n      freeze\n      fund\n      funding\n      future\n      goal\n      good\n      gov\n      government\n      graduation\n      grant\n      ha\n      health\n      help\n      high\n      higher\n      hold\n      home\n      house\n      icr\n      idr\n      impact\n      important\n      including\n      income\n      increase\n      individual\n      information\n      initiative\n      like\n      likely\n      ll\n      loan\n      location\n      long\n      look\n      low\n      lower\n      make\n      making\n      market\n      mean\n      median\n      medical\n      memo\n      million\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      offer\n      office\n      official\n      opportunity\n      option\n      order\n      pause\n      pay\n      paye\n      paying\n      payment\n      people\n      period\n      personal\n      plan\n      policy\n      power\n      president\n      process\n      processing\n      program\n      provide\n      pslf\n      public\n      qualify\n      qualifying\n      rate\n      ratio\n      read\n      receive\n      recent\n      relief\n      repayment\n      report\n      republican\n      research\n      review\n      right\n      rubin\n      rule\n      said\n      save\n      saving\n      say\n      school\n      score\n      security\n      service\n      servicer\n      set\n      social\n      spending\n      start\n      state\n      step\n      student\n      studentaid\n      sure\n      tax\n      term\n      time\n      total\n      transfer\n      trump\n      undergraduate\n      university\n      use\n      ve\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      acceptance\n      access\n      according\n      act\n      action\n      administration\n      agency\n      aid\n      america\n      american\n      ap\n      area\n      art\n      best\n      biden\n      billion\n      bond\n      borrower\n      bradley\n      budget\n      business\n      california\n      campus\n      care\n      center\n      change\n      child\n      city\n      class\n      close\n      college\n      come\n      community\n      congress\n      control\n      cost\n      country\n      course\n      court\n      credit\n      cut\n      data\n      day\n      debt\n      decade\n      decision\n      democrat\n      department\n      doe\n      doge\n      don\n      donald\n      earnings\n      education\n      effort\n      employee\n      employment\n      end\n      enrollment\n      executive\n      expectancy\n      faculty\n      fafsa\n      family\n      federal\n      financial\n      forgiveness\n      free\n      friday\n      funding\n      going\n      government\n      graduation\n      grant\n      group\n      growth\n      ha\n      health\n      high\n      higher\n      home\n      house\n      including\n      income\n      increase\n      information\n      institute\n      institution\n      issue\n      job\n      judge\n      just\n      know\n      known\n      live\n      loan\n      local\n      located\n      location\n      look\n      low\n      lower\n      make\n      making\n      massachusetts\n      median\n      million\n      monday\n      money\n      month\n      musk\n      nation\n      national\n      nearly\n      need\n      new\n      nonprofit\n      number\n      offer\n      office\n      opportunity\n      order\n      organization\n      parent\n      pause\n      pay\n      payment\n      people\n      personal\n      plan\n      policy\n      power\n      president\n      press\n      private\n      program\n      project\n      provide\n      public\n      rate\n      ratio\n      record\n      relief\n      repayment\n      report\n      republican\n      research\n      resource\n      right\n      said\n      save\n      say\n      school\n      science\n      secretary\n      security\n      service\n      social\n      spending\n      state\n      status\n      strong\n      student\n      study\n      support\n      supreme\n      tax\n      team\n      technology\n      term\n      think\n      time\n      today\n      trillion\n      trump\n      undergraduate\n      university\n      use\n      virginia\n      virginian\n      wa\n      want\n      washington\n      way\n      week\n      white\n      woman\n      work\n      worker\n      working\n      world\n      year\n      york\n      young\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      access\n      act\n      action\n      administration\n      agency\n      agenda\n      aid\n      amendment\n      america\n      american\n      assistance\n      attorney\n      benefit\n      biden\n      billion\n      birthright\n      border\n      borrower\n      branch\n      business\n      called\n      campaign\n      case\n      catholic\n      change\n      child\n      citizen\n      citizenship\n      civil\n      click\n      college\n      come\n      congress\n      constitution\n      constitutional\n      cost\n      country\n      court\n      crisis\n      cut\n      daily\n      data\n      day\n      debt\n      decision\n      dei\n      democracy\n      democrat\n      democratic\n      department\n      did\n      digital\n      district\n      doe\n      doge\n      dollar\n      don\n      donald\n      education\n      effort\n      elect\n      election\n      end\n      executive\n      failed\n      family\n      federal\n      final\n      financial\n      foreign\n      forgiveness\n      foundation\n      fox\n      free\n      freeze\n      fund\n      funding\n      general\n      getty\n      going\n      government\n      grant\n      group\n      ha\n      harris\n      help\n      higher\n      house\n      illegal\n      image\n      immigrant\n      immigration\n      individual\n      issue\n      law\n      left\n      legal\n      legislation\n      like\n      loan\n      long\n      look\n      make\n      mean\n      medium\n      member\n      memo\n      military\n      million\n      money\n      musk\n      nation\n      national\n      nearly\n      need\n      new\n      news\n      obama\n      office\n      official\n      order\n      organization\n      parent\n      party\n      past\n      pause\n      pay\n      people\n      plan\n      policy\n      political\n      post\n      power\n      president\n      presidential\n      press\n      private\n      program\n      provide\n      public\n      read\n      regulation\n      related\n      relief\n      rep\n      report\n      reporter\n      republican\n      right\n      rule\n      ruling\n      said\n      say\n      school\n      second\n      secretary\n      security\n      service\n      social\n      spending\n      state\n      student\n      support\n      supreme\n      tax\n      taxpayer\n      term\n      thing\n      think\n      time\n      told\n      took\n      treasury\n      trillion\n      trump\n      tuesday\n      united\n      university\n      use\n      vance\n      ve\n      vice\n      wa\n      want\n      washington\n      way\n      week\n      white\n      wing\n      work\n      worker\n      world\n      year\n      york\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words"
  },
  {
    "objectID": "vectorization_newsapi.html#strategy---further-preprocessing",
    "href": "vectorization_newsapi.html#strategy---further-preprocessing",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Recall that the prepared data looks like this:\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\nThe main label of interest for this data is Political Bias, however other potential labels include:\n\nNews Organization Source\nAuthor\nDate\nSearch Query Parameter\n\nThe data itself will be the News Article, however other potential data sources include:\n\nTitle\nDescription\n\nThis page will focus just on the entire News Article for data, but it could be worth comparing Title and Description in the future.\nFor this text data, the additional preprocessing will take place for each article:\n\nRemove line breaks.\nRemove punctuation.\nRemove words containing numbers.\nRemove standalone numbers.\nRemove leading and trailing spaces.\nLowercase the remaining words.\nRemove any single-length text remaining."
  },
  {
    "objectID": "vectorization_newsapi.html#strategy---vectorizing",
    "href": "vectorization_newsapi.html#strategy---vectorizing",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Now that the articles have been properly prepared to create a vectorized dataframe, several versions will be created. Namely, word count dataframes will be created using CountVectorizer() and normalized word count dataframes will be created using TfidfVectorizer(), both from scikit-learn. Stopwords will be removed using these functions as well. Dataframes will be further subsetted along the political bias labels. Lemmatizing and Stemming will also be used to create different versions available for further analyses. One additional option could be further versions of maximum words allowed in a dataframe."
  },
  {
    "objectID": "vectorization_newsapi.html#vectorizing---overall",
    "href": "vectorization_newsapi.html#vectorizing---overall",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "These versions will vectorize the completely preprocessed NewsAPI data in its entirety, remove stopwords and use a maximum of 200 features. A sample of the datasets for both vectorized versions will be shown. Additionally, a wordcloud and a top ten feature visualization will be shown for the CountVectorizer() versions.\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      acceptance\n      access\n      according\n      account\n      act\n      administration\n      agencies\n      agency\n      aid\n      american\n      americans\n      assistance\n      balance\n      bank\n      based\n      best\n      biden\n      billion\n      borrowers\n      budget\n      business\n      card\n      changes\n      class\n      college\n      company\n      congress\n      cost\n      costs\n      country\n      court\n      cr\n      credit\n      crore\n      current\n      data\n      day\n      debt\n      december\n      department\n      development\n      did\n      does\n      doge\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      employees\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expenses\n      faculty\n      family\n      federal\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      funds\n      future\n      going\n      good\n      government\n      graduation\n      grants\n      growth\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      institutions\n      insurance\n      job\n      just\n      know\n      ll\n      loan\n      loans\n      location\n      long\n      low\n      lower\n      make\n      making\n      management\n      market\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      months\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      office\n      options\n      order\n      pause\n      pay\n      payment\n      payments\n      people\n      personal\n      plan\n      plans\n      policy\n      power\n      president\n      private\n      process\n      profit\n      program\n      programs\n      provide\n      pslf\n      public\n      quarter\n      rate\n      rates\n      ratio\n      read\n      real\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      rs\n      said\n      save\n      say\n      says\n      school\n      schools\n      sector\n      security\n      seen\n      service\n      services\n      set\n      social\n      spending\n      start\n      state\n      states\n      student\n      students\n      support\n      tax\n      term\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      want\n      washington\n      way\n      week\n      white\n      work\n      workers\n      working\n      year\n      years\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      acceptance\n      access\n      according\n      account\n      act\n      administration\n      agencies\n      agency\n      aid\n      american\n      americans\n      assistance\n      balance\n      bank\n      based\n      best\n      biden\n      billion\n      borrowers\n      budget\n      business\n      card\n      changes\n      class\n      college\n      company\n      congress\n      cost\n      costs\n      country\n      court\n      cr\n      credit\n      crore\n      current\n      data\n      day\n      debt\n      december\n      department\n      development\n      did\n      does\n      doge\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      employees\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expenses\n      faculty\n      family\n      federal\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      funds\n      future\n      going\n      good\n      government\n      graduation\n      grants\n      growth\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      institutions\n      insurance\n      job\n      just\n      know\n      ll\n      loan\n      loans\n      location\n      long\n      low\n      lower\n      make\n      making\n      management\n      market\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      months\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      office\n      options\n      order\n      pause\n      pay\n      payment\n      payments\n      people\n      personal\n      plan\n      plans\n      policy\n      power\n      president\n      private\n      process\n      profit\n      program\n      programs\n      provide\n      pslf\n      public\n      quarter\n      rate\n      rates\n      ratio\n      read\n      real\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      rs\n      said\n      save\n      say\n      says\n      school\n      schools\n      sector\n      security\n      seen\n      service\n      services\n      set\n      social\n      spending\n      start\n      state\n      states\n      student\n      students\n      support\n      tax\n      term\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      want\n      washington\n      way\n      week\n      white\n      work\n      workers\n      working\n      year\n      years\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "vectorization_newsapi.html#vectorizing---overall-lemmatized",
    "href": "vectorization_newsapi.html#vectorizing---overall-lemmatized",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      access\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      balance\n      bank\n      based\n      benefit\n      biden\n      billion\n      borrower\n      budget\n      business\n      card\n      case\n      challenge\n      change\n      class\n      college\n      come\n      community\n      company\n      congress\n      consumer\n      cost\n      country\n      court\n      cr\n      credit\n      crore\n      cut\n      data\n      day\n      debt\n      december\n      decision\n      democrat\n      department\n      development\n      did\n      doe\n      dollar\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      effort\n      employee\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expert\n      faculty\n      family\n      federal\n      finance\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      future\n      going\n      good\n      government\n      graduation\n      grant\n      group\n      growth\n      ha\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      know\n      law\n      legal\n      life\n      like\n      likely\n      ll\n      loan\n      location\n      long\n      look\n      low\n      lower\n      make\n      making\n      management\n      market\n      mean\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      mortgage\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      offer\n      office\n      opportunity\n      option\n      order\n      pause\n      pay\n      payment\n      people\n      personal\n      plan\n      policy\n      power\n      president\n      price\n      private\n      profit\n      program\n      provide\n      pslf\n      public\n      quarter\n      rate\n      ratio\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      risk\n      rule\n      said\n      save\n      saving\n      say\n      school\n      sector\n      security\n      service\n      social\n      spending\n      start\n      state\n      statement\n      student\n      support\n      tax\n      term\n      thing\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      worker\n      working\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      access\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      balance\n      bank\n      based\n      benefit\n      biden\n      billion\n      borrower\n      budget\n      business\n      card\n      case\n      challenge\n      change\n      class\n      college\n      come\n      community\n      company\n      congress\n      consumer\n      cost\n      country\n      court\n      cr\n      credit\n      crore\n      cut\n      data\n      day\n      debt\n      december\n      decision\n      democrat\n      department\n      development\n      did\n      doe\n      dollar\n      don\n      donald\n      driven\n      earnings\n      economic\n      education\n      effort\n      employee\n      employment\n      end\n      enrollment\n      equity\n      example\n      executive\n      expected\n      expert\n      faculty\n      family\n      federal\n      finance\n      financial\n      forbearance\n      forgiveness\n      free\n      fund\n      funding\n      future\n      going\n      good\n      government\n      graduation\n      grant\n      group\n      growth\n      ha\n      health\n      help\n      high\n      higher\n      home\n      house\n      idr\n      including\n      income\n      increase\n      india\n      information\n      know\n      law\n      legal\n      life\n      like\n      likely\n      ll\n      loan\n      location\n      long\n      look\n      low\n      lower\n      make\n      making\n      management\n      market\n      mean\n      median\n      memo\n      million\n      money\n      month\n      monthly\n      mortgage\n      musk\n      national\n      need\n      net\n      new\n      news\n      number\n      offer\n      office\n      opportunity\n      option\n      order\n      pause\n      pay\n      payment\n      people\n      personal\n      plan\n      policy\n      power\n      president\n      price\n      private\n      profit\n      program\n      provide\n      pslf\n      public\n      quarter\n      rate\n      ratio\n      relief\n      repayment\n      report\n      republican\n      research\n      right\n      risk\n      rule\n      said\n      save\n      saving\n      say\n      school\n      sector\n      security\n      service\n      social\n      spending\n      start\n      state\n      statement\n      student\n      support\n      tax\n      term\n      thing\n      think\n      time\n      total\n      trump\n      undergraduate\n      university\n      use\n      ve\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      worker\n      working\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "vectorization_newsapi.html#vectorizing---overall-stemmatized",
    "href": "vectorization_newsapi.html#vectorizing---overall-stemmatized",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Labeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      accept\n      access\n      accord\n      account\n      act\n      action\n      addit\n      administr\n      agenc\n      aid\n      allow\n      american\n      ani\n      anoth\n      balanc\n      bank\n      base\n      becaus\n      befor\n      benefit\n      biden\n      billion\n      borrow\n      budget\n      busi\n      card\n      challeng\n      chang\n      colleg\n      come\n      commun\n      compani\n      congress\n      consid\n      consum\n      continu\n      cost\n      countri\n      court\n      creat\n      credit\n      crore\n      current\n      cut\n      data\n      day\n      debt\n      democrat\n      depart\n      develop\n      don\n      dure\n      earn\n      econom\n      educ\n      effect\n      effort\n      employ\n      end\n      enrol\n      equiti\n      execut\n      expect\n      expens\n      faculti\n      famili\n      feder\n      financ\n      financi\n      forgiv\n      free\n      fund\n      futur\n      gener\n      good\n      govern\n      graduat\n      grant\n      growth\n      ha\n      health\n      help\n      hi\n      high\n      higher\n      home\n      hous\n      howev\n      idr\n      impact\n      import\n      includ\n      incom\n      increas\n      job\n      just\n      know\n      law\n      legal\n      like\n      limit\n      live\n      loan\n      locat\n      long\n      look\n      lower\n      major\n      make\n      manag\n      mani\n      market\n      mean\n      million\n      money\n      month\n      monthli\n      nation\n      need\n      new\n      news\n      number\n      offer\n      offic\n      onli\n      opportun\n      option\n      order\n      paus\n      pay\n      payment\n      peopl\n      person\n      plan\n      polici\n      power\n      presid\n      privat\n      process\n      profit\n      program\n      project\n      propos\n      provid\n      pslf\n      public\n      qualifi\n      quarter\n      rate\n      receiv\n      relief\n      remain\n      repay\n      report\n      republican\n      requir\n      research\n      right\n      rs\n      rule\n      said\n      save\n      say\n      school\n      sector\n      secur\n      servic\n      set\n      share\n      sinc\n      social\n      spend\n      start\n      state\n      student\n      support\n      tax\n      term\n      thi\n      think\n      time\n      total\n      tri\n      trump\n      undergradu\n      univers\n      use\n      wa\n      want\n      way\n      week\n      white\n      work\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud\n\n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      accept\n      access\n      accord\n      account\n      act\n      action\n      addit\n      administr\n      agenc\n      aid\n      allow\n      american\n      ani\n      anoth\n      balanc\n      bank\n      base\n      becaus\n      befor\n      benefit\n      biden\n      billion\n      borrow\n      budget\n      busi\n      card\n      challeng\n      chang\n      colleg\n      come\n      commun\n      compani\n      congress\n      consid\n      consum\n      continu\n      cost\n      countri\n      court\n      creat\n      credit\n      crore\n      current\n      cut\n      data\n      day\n      debt\n      democrat\n      depart\n      develop\n      don\n      dure\n      earn\n      econom\n      educ\n      effect\n      effort\n      employ\n      end\n      enrol\n      equiti\n      execut\n      expect\n      expens\n      faculti\n      famili\n      feder\n      financ\n      financi\n      forgiv\n      free\n      fund\n      futur\n      gener\n      good\n      govern\n      graduat\n      grant\n      growth\n      ha\n      health\n      help\n      hi\n      high\n      higher\n      home\n      hous\n      howev\n      idr\n      impact\n      import\n      includ\n      incom\n      increas\n      job\n      just\n      know\n      law\n      legal\n      like\n      limit\n      live\n      loan\n      locat\n      long\n      look\n      lower\n      major\n      make\n      manag\n      mani\n      market\n      mean\n      million\n      money\n      month\n      monthli\n      nation\n      need\n      new\n      news\n      number\n      offer\n      offic\n      onli\n      opportun\n      option\n      order\n      paus\n      pay\n      payment\n      peopl\n      person\n      plan\n      polici\n      power\n      presid\n      privat\n      process\n      profit\n      program\n      project\n      propos\n      provid\n      pslf\n      public\n      qualifi\n      quarter\n      rate\n      receiv\n      relief\n      remain\n      repay\n      report\n      republican\n      requir\n      research\n      right\n      rs\n      rule\n      said\n      save\n      say\n      school\n      sector\n      secur\n      servic\n      set\n      share\n      sinc\n      social\n      spend\n      start\n      state\n      student\n      support\n      tax\n      term\n      thi\n      think\n      time\n      total\n      tri\n      trump\n      undergradu\n      univers\n      use\n      wa\n      want\n      way\n      week\n      white\n      work\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)"
  },
  {
    "objectID": "vectorization_newsapi.html#vectorizing---political-bias",
    "href": "vectorization_newsapi.html#vectorizing---political-bias",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Lemmatizing seems to aggregate the text data while retaining meaning in the words. For instance, students are aggregated into student and loans into loan, while words like education are not reduced to something like educ. Moving forward, lemmatizing is a logical preprocessing step. Thus, lemmatization and stopwords removal for creating vectorized versions of the political bias will be used. For the process of creating subsets of the political bias data, there are two methods that could be used.\n\nSubsetting Second: vectorize the entire dataset \\(\\rightarrow\\) append labels \\(\\rightarrow\\) subset on political bias\nSubsetting First: subset the dataset on political bias \\(\\rightarrow\\) vectorize the subset \\(\\rightarrow\\) append labels\n\nBy subsetting first, the maximum word count will be reflective of the corpus associated with the respective political bias. Therefore, it might be more useful in this comparative analysis to subset first.\nAdditionally, CountVectorizer() will be used over TfidfVectorizer() for a first pass. A normalized version of the features could be useful in some cases, such as when dealing with varying sizes of content (i.e. total word count of content), but feature appearance counts will be more useful for visualizing in this analysis.\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      abolition\n      act\n      administration\n      agency\n      aggregated\n      america\n      american\n      assistance\n      athlete\n      biden\n      big\n      billion\n      billionaire\n      budget\n      business\n      california\n      car\n      care\n      case\n      change\n      child\n      civil\n      class\n      clear\n      come\n      company\n      congress\n      control\n      country\n      court\n      cut\n      day\n      debt\n      democracy\n      democrat\n      democratic\n      department\n      did\n      district\n      doe\n      doesn\n      doing\n      dollar\n      don\n      donald\n      economic\n      education\n      effect\n      effort\n      election\n      employee\n      end\n      family\n      far\n      federal\n      feel\n      financial\n      food\n      forecast\n      free\n      freeze\n      fund\n      funding\n      going\n      good\n      gop\n      government\n      grant\n      group\n      ha\n      hard\n      having\n      health\n      help\n      home\n      house\n      idea\n      including\n      income\n      individual\n      industry\n      insurance\n      ivy\n      job\n      just\n      justice\n      kid\n      kind\n      know\n      labor\n      law\n      le\n      league\n      likely\n      limit\n      little\n      loan\n      long\n      lot\n      maga\n      major\n      majority\n      make\n      making\n      marcotte\n      mean\n      meat\n      medicaid\n      member\n      memo\n      men\n      middle\n      million\n      money\n      month\n      musk\n      national\n      need\n      new\n      number\n      office\n      opportunity\n      order\n      parent\n      party\n      past\n      pause\n      pay\n      people\n      percent\n      person\n      place\n      plan\n      point\n      policy\n      political\n      poor\n      power\n      prediction\n      president\n      program\n      project\n      provide\n      public\n      question\n      read\n      really\n      repeal\n      republican\n      right\n      said\n      sargent\n      say\n      school\n      security\n      senate\n      service\n      social\n      spending\n      start\n      state\n      student\n      suicide\n      support\n      tariff\n      tax\n      term\n      thing\n      think\n      time\n      told\n      trillion\n      trump\n      trying\n      tuesday\n      union\n      use\n      ve\n      vote\n      voter\n      wa\n      want\n      way\n      wealth\n      week\n      white\n      woman\n      work\n      working\n      workplace\n      world\n      wrote\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      access\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      assistance\n      bank\n      bankruptcy\n      based\n      benefit\n      best\n      biden\n      big\n      billion\n      borrower\n      brennan\n      budget\n      business\n      california\n      card\n      case\n      cfpb\n      change\n      child\n      college\n      come\n      community\n      company\n      congress\n      consumer\n      cost\n      country\n      court\n      credit\n      cut\n      data\n      day\n      debt\n      decision\n      democrat\n      department\n      did\n      doe\n      doesn\n      doge\n      dollar\n      don\n      donald\n      education\n      effort\n      employee\n      end\n      equity\n      executive\n      expert\n      family\n      federal\n      feel\n      finance\n      financial\n      forgiveness\n      free\n      fund\n      funding\n      future\n      getting\n      going\n      good\n      got\n      government\n      grant\n      group\n      ha\n      hard\n      health\n      help\n      high\n      higher\n      home\n      house\n      including\n      income\n      information\n      insurance\n      investment\n      issue\n      job\n      just\n      know\n      like\n      likely\n      line\n      ll\n      loan\n      long\n      look\n      low\n      make\n      making\n      management\n      margaret\n      market\n      mcmahon\n      mean\n      member\n      memo\n      million\n      money\n      month\n      mortgage\n      musk\n      national\n      need\n      new\n      number\n      offer\n      office\n      option\n      order\n      parent\n      pause\n      pay\n      payment\n      people\n      plan\n      policy\n      political\n      power\n      president\n      price\n      private\n      process\n      program\n      project\n      public\n      question\n      rate\n      really\n      relief\n      repayment\n      report\n      republican\n      retirement\n      right\n      risk\n      rule\n      said\n      save\n      saving\n      say\n      school\n      secretary\n      security\n      senator\n      service\n      social\n      spending\n      start\n      state\n      student\n      support\n      sure\n      tax\n      team\n      term\n      thing\n      think\n      time\n      today\n      told\n      took\n      trump\n      trying\n      tuesday\n      university\n      use\n      used\n      ve\n      vought\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      worker\n      working\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      able\n      acceptance\n      according\n      account\n      act\n      action\n      administration\n      agency\n      aid\n      american\n      application\n      apply\n      available\n      average\n      balance\n      based\n      benefit\n      best\n      biden\n      billion\n      borrower\n      budget\n      business\n      buy\n      buyback\n      card\n      case\n      certain\n      challenge\n      change\n      check\n      college\n      come\n      congress\n      consumer\n      cost\n      court\n      credit\n      current\n      cut\n      data\n      day\n      debt\n      department\n      different\n      doe\n      don\n      driven\n      earnings\n      education\n      eligible\n      employment\n      end\n      enrolled\n      enrollment\n      example\n      executive\n      expense\n      expert\n      faculty\n      federal\n      financial\n      forbearance\n      forgiven\n      forgiveness\n      free\n      freeze\n      fund\n      funding\n      future\n      goal\n      good\n      gov\n      government\n      graduation\n      grant\n      ha\n      health\n      help\n      high\n      higher\n      hold\n      home\n      house\n      icr\n      idr\n      impact\n      important\n      including\n      income\n      increase\n      individual\n      information\n      initiative\n      like\n      likely\n      ll\n      loan\n      location\n      long\n      look\n      low\n      lower\n      make\n      making\n      market\n      mean\n      median\n      medical\n      memo\n      million\n      money\n      month\n      monthly\n      mortgage\n      need\n      new\n      number\n      offer\n      office\n      official\n      opportunity\n      option\n      order\n      pause\n      pay\n      paye\n      paying\n      payment\n      people\n      period\n      personal\n      plan\n      policy\n      power\n      president\n      process\n      processing\n      program\n      provide\n      pslf\n      public\n      qualify\n      qualifying\n      rate\n      ratio\n      read\n      receive\n      recent\n      relief\n      repayment\n      report\n      republican\n      research\n      review\n      right\n      rubin\n      rule\n      said\n      save\n      saving\n      say\n      school\n      score\n      security\n      service\n      servicer\n      set\n      social\n      spending\n      start\n      state\n      step\n      student\n      studentaid\n      sure\n      tax\n      term\n      time\n      total\n      transfer\n      trump\n      undergraduate\n      university\n      use\n      ve\n      wa\n      want\n      washington\n      way\n      week\n      white\n      work\n      year\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      acceptance\n      access\n      according\n      act\n      action\n      administration\n      agency\n      aid\n      america\n      american\n      ap\n      area\n      art\n      best\n      biden\n      billion\n      bond\n      borrower\n      bradley\n      budget\n      business\n      california\n      campus\n      care\n      center\n      change\n      child\n      city\n      class\n      close\n      college\n      come\n      community\n      congress\n      control\n      cost\n      country\n      course\n      court\n      credit\n      cut\n      data\n      day\n      debt\n      decade\n      decision\n      democrat\n      department\n      doe\n      doge\n      don\n      donald\n      earnings\n      education\n      effort\n      employee\n      employment\n      end\n      enrollment\n      executive\n      expectancy\n      faculty\n      fafsa\n      family\n      federal\n      financial\n      forgiveness\n      free\n      friday\n      funding\n      going\n      government\n      graduation\n      grant\n      group\n      growth\n      ha\n      health\n      high\n      higher\n      home\n      house\n      including\n      income\n      increase\n      information\n      institute\n      institution\n      issue\n      job\n      judge\n      just\n      know\n      known\n      live\n      loan\n      local\n      located\n      location\n      look\n      low\n      lower\n      make\n      making\n      massachusetts\n      median\n      million\n      monday\n      money\n      month\n      musk\n      nation\n      national\n      nearly\n      need\n      new\n      nonprofit\n      number\n      offer\n      office\n      opportunity\n      order\n      organization\n      parent\n      pause\n      pay\n      payment\n      people\n      personal\n      plan\n      policy\n      power\n      president\n      press\n      private\n      program\n      project\n      provide\n      public\n      rate\n      ratio\n      record\n      relief\n      repayment\n      report\n      republican\n      research\n      resource\n      right\n      said\n      save\n      say\n      school\n      science\n      secretary\n      security\n      service\n      social\n      spending\n      state\n      status\n      strong\n      student\n      study\n      support\n      supreme\n      tax\n      team\n      technology\n      term\n      think\n      time\n      today\n      trillion\n      trump\n      undergraduate\n      university\n      use\n      virginia\n      virginian\n      wa\n      want\n      washington\n      way\n      week\n      white\n      woman\n      work\n      worker\n      working\n      world\n      year\n      york\n      young\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words\n\n\n\n\n\n\nLabeled Sample\n\n\n\n\n    \n      \n      source\n      author\n      Bias Specific\n      Bias Numeric\n      date\n      search\n      access\n      act\n      action\n      administration\n      agency\n      agenda\n      aid\n      amendment\n      america\n      american\n      assistance\n      attorney\n      benefit\n      biden\n      billion\n      birthright\n      border\n      borrower\n      branch\n      business\n      called\n      campaign\n      case\n      catholic\n      change\n      child\n      citizen\n      citizenship\n      civil\n      click\n      college\n      come\n      congress\n      constitution\n      constitutional\n      cost\n      country\n      court\n      crisis\n      cut\n      daily\n      data\n      day\n      debt\n      decision\n      dei\n      democracy\n      democrat\n      democratic\n      department\n      did\n      digital\n      district\n      doe\n      doge\n      dollar\n      don\n      donald\n      education\n      effort\n      elect\n      election\n      end\n      executive\n      failed\n      family\n      federal\n      final\n      financial\n      foreign\n      forgiveness\n      foundation\n      fox\n      free\n      freeze\n      fund\n      funding\n      general\n      getty\n      going\n      government\n      grant\n      group\n      ha\n      harris\n      help\n      higher\n      house\n      illegal\n      image\n      immigrant\n      immigration\n      individual\n      issue\n      law\n      left\n      legal\n      legislation\n      like\n      loan\n      long\n      look\n      make\n      mean\n      medium\n      member\n      memo\n      military\n      million\n      money\n      musk\n      nation\n      national\n      nearly\n      need\n      new\n      news\n      obama\n      office\n      official\n      order\n      organization\n      parent\n      party\n      past\n      pause\n      pay\n      people\n      plan\n      policy\n      political\n      post\n      power\n      president\n      presidential\n      press\n      private\n      program\n      provide\n      public\n      read\n      regulation\n      related\n      relief\n      rep\n      report\n      reporter\n      republican\n      right\n      rule\n      ruling\n      said\n      say\n      school\n      second\n      secretary\n      security\n      service\n      social\n      spending\n      state\n      student\n      support\n      supreme\n      tax\n      taxpayer\n      term\n      thing\n      think\n      time\n      told\n      took\n      treasury\n      trillion\n      trump\n      tuesday\n      united\n      university\n      use\n      vance\n      ve\n      vice\n      wa\n      want\n      washington\n      way\n      week\n      white\n      wing\n      work\n      worker\n      world\n      year\n      york\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.3.0 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\nWordcloud \n\n\n\nMost Frequent Words"
  }
]