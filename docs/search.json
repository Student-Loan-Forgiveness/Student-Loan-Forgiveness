[
  {
    "objectID": "vectorization_reddit.html",
    "href": "vectorization_reddit.html",
    "title": "Vectorization - Reddit",
    "section": "",
    "text": "Introduction"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "The introduction page for the student loan forgiveness text analysis project."
  },
  {
    "objectID": "data_newsapi.html",
    "href": "data_newsapi.html",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "Part of the discussion surrounding Student Loan Forgiveness is reporting done by different news media organizations. These organizations generally align with a certain political bias. With the realization that those on the left tend to support at least some form of student loan relief and those on the right tend to oppose it, it could be fruitful to pair news organizations with their political biases. To achieve this, two main sources of news related data were explored:\n\nNewsAPI\nAllSides\n\n\n\nNewsAPI maintains an application programming interfacae (API) which returns information on articles published around the world. For the scope of this analysis, two search parameters were queried from the API. Articles were gathered using the specific search parameter of student loan forgiveness, as well as a general search parameter of student loans.\nThe data is initially returned in JSON format, which was then turned into a dataframe and subsequently exported into a csv file.\n\n\nSample of the Raw NewsAPI Data:\n\n\n\n\n    \n      \n      author\n      content\n      description\n      date\n      source\n      title\n      url\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nauthor\ncontent\ndescription\ndate\nsource\ntitle\nurl\n\n\n\n\n\nOne limitation of NewsAPI is that it doesn’t return the content of an article in its entirety. To account for this, numerous web scrapers were built for sources which were both scraping-eligible and appropriate for this analysis. This did reduce the number of sources and articles in the data from NewsAPI, but did result in complete article content. A list of eligible sources and their related scrapers were iteratively called to populate a dataframe with the individual paragraphs from the url associated with the article. To specify, the initial scrape through will result in a dataframe where each paragraph from the each scrapable article will have its own row.\n\n\nSample of the Raw Scraped Data:\n\n\n\n\n    \n      \n      source\n      url\n      paragraph\n      paragraph_num\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nsource\nurl\nparagraph\nparagraph_num\n\n\nUsing the source and url columns as keys, these can recombined with the columns from the original extraction of data for future use.\n\n\n\n\n\nAllSides is a community-based political bias rating platform for media. Using web scraping, biases for the sources in the scraped data were accumulated. Pages (urls) containing information on sources were scraped and manually found, which were then in turned scraped for their specific bias ratings.\n\n\nSample of the AllSides Data:\n\n\n\n\n    \n      \n      source\n      url\n      Bias Numeric\n      Bias Specific\n      Type\n      Region\n      Website\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nsource\nurl\nBias Numeric\nBias Specific\nType\nRegion\nWebsite\n\n\n\n\n\nThus far, raw data from an API and an array of web scraping functions has been gathered. This can now be combined together to create potential labels for the complete articles associated with the NewsAPI extraction. However, some data cleaning should be performed first.\n\n\nNewsAPI Extraction:\n\nauthor\n\nduplicate authors from same article removed\nemail addresses and links removed\nstripped of leading, trailing, and multiple spaces\nif author is None then the entry from the source column is used in place\nline breaks removed\n\ndescription\n\ntext before and including delimiters removed (i.e. delimiter could be “Date and Location –”)\n\ncontent\n\nignored in lieu of scraping the entire itself\n\n\nNewsAPI Scraping Extraction:\n\nparagraph\n\nblank paragraphs removed\nnon-breaking space characters removed (i.e. “”)\nstripped of leading, trailing, and multiple spaces\nall paragraphs of a single article combined post-cleaning\n\n\nAllSides Bias Ratings: no cleaning was required for this data, however a source-to-source map was created, as some sources were not identical between NewsAPI and AllSides.\n\n\n\nThe data with the completely scraped articles still retained the columns for source and url. Using those, the cleaned extraction columns can be merged in as well as the biases.\nThis resulted in a dataframe with the following columns:\n\nsource\nurl\narticle\nsource_bias\nBias Numeric\nBias Specific\nauthor\ndate\ntitle\nsearch\n\nWhere the text data itself will likely be article, but interesting analyses could be extended by using description or title.\nThe main label of interest will be Bais Specific, but interesting analyses could hinge upon using source, author, date, or search (topic query parameter).\nSample of the final labeled data:\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_newsapi.html#newsapi-extraction",
    "href": "data_newsapi.html#newsapi-extraction",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "NewsAPI maintains an application programming interfacae (API) which returns information on articles published around the world. For the scope of this analysis, two search parameters were queried from the API. Articles were gathered using the specific search parameter of student loan forgiveness, as well as a general search parameter of student loans.\nThe data is initially returned in JSON format, which was then turned into a dataframe and subsequently exported into a csv file.\n\n\nSample of the Raw NewsAPI Data:\n\n\n\n\n    \n      \n      author\n      content\n      description\n      date\n      source\n      title\n      url\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nauthor\ncontent\ndescription\ndate\nsource\ntitle\nurl"
  },
  {
    "objectID": "data_newsapi.html#newsapi-scraping",
    "href": "data_newsapi.html#newsapi-scraping",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "One limitation of NewsAPI is that it doesn’t return the content of an article in its entirety. To account for this, numerous web scrapers were built for sources which were both scraping-eligible and appropriate for this analysis. This did reduce the number of sources and articles in the data from NewsAPI, but did result in complete article content. A list of eligible sources and their related scrapers were iteratively called to populate a dataframe with the individual paragraphs from the url associated with the article. To specify, the initial scrape through will result in a dataframe where each paragraph from the each scrapable article will have its own row.\n\n\nSample of the Raw Scraped Data:\n\n\n\n\n    \n      \n      source\n      url\n      paragraph\n      paragraph_num\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nsource\nurl\nparagraph\nparagraph_num\n\n\nUsing the source and url columns as keys, these can recombined with the columns from the original extraction of data for future use."
  },
  {
    "objectID": "data_newsapi.html#allsides-bias-data",
    "href": "data_newsapi.html#allsides-bias-data",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "AllSides is a community-based political bias rating platform for media. Using web scraping, biases for the sources in the scraped data were accumulated. Pages (urls) containing information on sources were scraped and manually found, which were then in turned scraped for their specific bias ratings.\n\n\nSample of the AllSides Data:\n\n\n\n\n    \n      \n      source\n      url\n      Bias Numeric\n      Bias Specific\n      Type\n      Region\n      Website\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nColumns:\n\nsource\nurl\nBias Numeric\nBias Specific\nType\nRegion\nWebsite"
  },
  {
    "objectID": "data_newsapi.html#cleaning-and-merging",
    "href": "data_newsapi.html#cleaning-and-merging",
    "title": "Data Acquisition - NewsAPI",
    "section": "",
    "text": "Thus far, raw data from an API and an array of web scraping functions has been gathered. This can now be combined together to create potential labels for the complete articles associated with the NewsAPI extraction. However, some data cleaning should be performed first.\n\n\nNewsAPI Extraction:\n\nauthor\n\nduplicate authors from same article removed\nemail addresses and links removed\nstripped of leading, trailing, and multiple spaces\nif author is None then the entry from the source column is used in place\nline breaks removed\n\ndescription\n\ntext before and including delimiters removed (i.e. delimiter could be “Date and Location –”)\n\ncontent\n\nignored in lieu of scraping the entire itself\n\n\nNewsAPI Scraping Extraction:\n\nparagraph\n\nblank paragraphs removed\nnon-breaking space characters removed (i.e. “”)\nstripped of leading, trailing, and multiple spaces\nall paragraphs of a single article combined post-cleaning\n\n\nAllSides Bias Ratings: no cleaning was required for this data, however a source-to-source map was created, as some sources were not identical between NewsAPI and AllSides.\n\n\n\nThe data with the completely scraped articles still retained the columns for source and url. Using those, the cleaned extraction columns can be merged in as well as the biases.\nThis resulted in a dataframe with the following columns:\n\nsource\nurl\narticle\nsource_bias\nBias Numeric\nBias Specific\nauthor\ndate\ntitle\nsearch\n\nWhere the text data itself will likely be article, but interesting analyses could be extended by using description or title.\nThe main label of interest will be Bais Specific, but interesting analyses could hinge upon using source, author, date, or search (topic query parameter).\nSample of the final labeled data:\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n\nCode\n1 + 1\n\n\n2"
  },
  {
    "objectID": "data_reddit.html",
    "href": "data_reddit.html",
    "title": "Data Acquisition - Reddit",
    "section": "",
    "text": "Aside from reporting done by news media organizations, Student Loan Forgiveness is a toipc on social media and public discussion platforms such as Reddit. Reddit consists of communities known as Subreddits, where users can participate in topic specific online discourse. Although some users choose to tag their accounts with political bias leanings such as liberal or conservative which could help determine sentiment on the certain topics, this isn’t prevalent enough to use as a proper label. However, different sentiments and biases could be found for individual users and Subreddits. Additionally, interesting associations and networks could be formed between the users and communities.\n\nNote that Reddit Users, Reddit Usernames, and Reddit Authors will be used synonymously.\n\n\n\nTo obtain the data from Reddit, web scraping and an API was utilized. A web scraping function was built which mimics a general search within the Reddit platform and returns the URLs for threads on a given search query. The URLs were then be iterated through with the API, which returned the posts, comments, replies, and other supporting information.\nThree search queries were made:\n\nStudent Loan Forgiveness\nStudent Loans\nIs a College Degree Worth It\n\nHowever, Is a College Degree Worth It data may not be utilized, as it’s not as precise to the topic of this analysis.\nSpecific objects and tags were created with API calls, and ultimately returned as a dataframe.\nSample of the Raw Reddit Data:\n\n\n\n\n    \n      \n      url\n      title\n      original\n      self\n      post_date\n      comments\n      author\n      id\n      upvotes\n      content\n      comment_date\n      replying_to\n      subreddit\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\n\n\n\nThe main cleaning that was required was for the content; posts, comments, and replies. The following processing occurred:\n\nIf the first post on a thread returned blank, this likely indicated a link to an outside source. In this case, the title was used as the content.\nValues within the content strings such as line breaks and reply indicators were removed.\nEmail addresses and links were removed.\nEmojis and other non-ASCII characters were removed.\nDuplicate posts were common and were removed.\n\n\nThis initial cleaning step still left the individual posts, comments, or replies as rows themselves.\n\nGiven that quite a few rows of content could be a few words, or even a single word reply, the idea for the next processing step was to have content from each author for each thread aggregated together. Additionally, author maps were created to track who communications were between. For each thread, the following restructuring was performed for a given author:\n\nLists:\n\nUpvotes (can be positive or negative)\nPosting Dates\nContent (posts, comments, and replies)\nReplies To (who the author has replied to)\nReplies From (who responded to the author)\n\nValues for Specific Threads:\n\nURL\nTitle\nSubreddit\nAuthor (Reddit username)\nOriginal Author (boolean for if the author started the thread - made the original post)\n\n\nA sample of the restructured Reddit data:\n\n\n\n\n    \n      \n      url\n      title\n      subreddit\n      author\n      original_author\n      author_upvotes\n      author_dates\n      author_content\n      author_content_aggregated\n      replies_to\n      replies_from\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_reddit.html#gathering-the-data",
    "href": "data_reddit.html#gathering-the-data",
    "title": "Data Acquisition - Reddit",
    "section": "",
    "text": "To obtain the data from Reddit, web scraping and an API was utilized. A web scraping function was built which mimics a general search within the Reddit platform and returns the URLs for threads on a given search query. The URLs were then be iterated through with the API, which returned the posts, comments, replies, and other supporting information.\nThree search queries were made:\n\nStudent Loan Forgiveness\nStudent Loans\nIs a College Degree Worth It\n\nHowever, Is a College Degree Worth It data may not be utilized, as it’s not as precise to the topic of this analysis.\nSpecific objects and tags were created with API calls, and ultimately returned as a dataframe.\nSample of the Raw Reddit Data:\n\n\n\n\n    \n      \n      url\n      title\n      original\n      self\n      post_date\n      comments\n      author\n      id\n      upvotes\n      content\n      comment_date\n      replying_to\n      subreddit\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_reddit.html#cleaning-and-restructuring",
    "href": "data_reddit.html#cleaning-and-restructuring",
    "title": "Data Acquisition - Reddit",
    "section": "",
    "text": "The main cleaning that was required was for the content; posts, comments, and replies. The following processing occurred:\n\nIf the first post on a thread returned blank, this likely indicated a link to an outside source. In this case, the title was used as the content.\nValues within the content strings such as line breaks and reply indicators were removed.\nEmail addresses and links were removed.\nEmojis and other non-ASCII characters were removed.\nDuplicate posts were common and were removed.\n\n\nThis initial cleaning step still left the individual posts, comments, or replies as rows themselves.\n\nGiven that quite a few rows of content could be a few words, or even a single word reply, the idea for the next processing step was to have content from each author for each thread aggregated together. Additionally, author maps were created to track who communications were between. For each thread, the following restructuring was performed for a given author:\n\nLists:\n\nUpvotes (can be positive or negative)\nPosting Dates\nContent (posts, comments, and replies)\nReplies To (who the author has replied to)\nReplies From (who responded to the author)\n\nValues for Specific Threads:\n\nURL\nTitle\nSubreddit\nAuthor (Reddit username)\nOriginal Author (boolean for if the author started the thread - made the original post)\n\n\nA sample of the restructured Reddit data:\n\n\n\n\n    \n      \n      url\n      title\n      subreddit\n      author\n      original_author\n      author_upvotes\n      author_dates\n      author_content\n      author_content_aggregated\n      replies_to\n      replies_from\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)"
  },
  {
    "objectID": "data_reddit.html#top-ten-authors-across-scenarios",
    "href": "data_reddit.html#top-ten-authors-across-scenarios",
    "title": "Data Acquisition - Reddit",
    "section": "Top Ten Authors Across Scenarios",
    "text": "Top Ten Authors Across Scenarios\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom left to right in the above images:\n\nMost Frequent Authors Across Reddit Queries: illustrates the top ten most frequent for the total number of posts, comments, and replies by author across all Reddit threads.\nMost Frequent Authors Across Multiple Threads: illustrates the the top ten authors who have at least a single piece of content in a Reddit thread.\nMost Frequent Authors Across Multiple Subreddits: illustrates the the top ten authors who have at least a single piece of content in a Subreddit.\n\nThere were a few takeaways from these illustrations. Although many authors only had a single appearance, there were a few who frequently post in these Reddit topic-specific communities. One user is actually a Reddit sanctioned bot, known as a Moderator, which helps to control inappropriate or misplaced discussion. This bot, AutoModerator, has its post shown in orange above and will be removed in the subsequent analyses."
  },
  {
    "objectID": "vectorization_newsapi.html",
    "href": "vectorization_newsapi.html",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Using the data which has been prepared and merged with potential labels, as seen in Data Acquisition - NewsAPI, a few more steps can be taken to turn the news articles into numerical representations which can then be used for further analyses and machine learning applications.\n\n\nRecall that the prepared data looks like this:\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nThe main label of interest for this data is Political Bias, however other potential labels include:\n\nNews Organization Source\nAuthor\nDate\nSearch Query Parameter\n\nThe data itself will be the News Article, however other potential data sources include:\n\nTitle\nDescription\n\nThis page will focus just on the entire News Article for data, but it could be worth comparing Title and Description in the future.\nFor this text data, the additional preprocessing will take place for each article:\n\nRemove line breaks.\nRemove punctuation.\nRemove words containing numbers.\nRemove standalone numbers.\nRemove leading and trailing spaces.\nLowercase the remaining words.\nRemove any single-length text remaining.\n\n\n\n\nNow that the articles have been properly prepared to create a vectorized dataframe, several versions will be created. Namely, word count dataframes will be created using CountVectorizer() and normalized word count dataframes will be created using TfidfVectorizer(), both from scikit-learn. Stopwords will removed using these functions as well. Dataframes will be further subsetted along the political bias labels. Lemmatizing and Stemming will also be used to create different versions available for further analyses. One additional option could be further versions of maximum words allowed in a dataframe."
  },
  {
    "objectID": "vectorization_newsapi.html#strategy---further-preprocessing",
    "href": "vectorization_newsapi.html#strategy---further-preprocessing",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Recall that the prepared data looks like this:\n\n\n\n\n    \n      \n      source\n      url\n      article\n      source_bias\n      Bias Numeric\n      Bias Specific\n      author\n      description\n      date\n      title\n      search\n    \n  \n\n\n    \n        \n        \n        \n        \n        \n        \n        \n        \n    \n    \n   \n    \n      \n  \n        \n    \n    \n  \n        \n    \n    \n  \n        \n    \n      \n  \n        \n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n\n            \n                \n                \n            \n        \n    \n\n\nLoading ITables v2.2.4 from the internet...\n(need help?)\n\n\n\n\n\n\nThe main label of interest for this data is Political Bias, however other potential labels include:\n\nNews Organization Source\nAuthor\nDate\nSearch Query Parameter\n\nThe data itself will be the News Article, however other potential data sources include:\n\nTitle\nDescription\n\nThis page will focus just on the entire News Article for data, but it could be worth comparing Title and Description in the future.\nFor this text data, the additional preprocessing will take place for each article:\n\nRemove line breaks.\nRemove punctuation.\nRemove words containing numbers.\nRemove standalone numbers.\nRemove leading and trailing spaces.\nLowercase the remaining words.\nRemove any single-length text remaining."
  },
  {
    "objectID": "vectorization_newsapi.html#strategy---vectorizing",
    "href": "vectorization_newsapi.html#strategy---vectorizing",
    "title": "Vectorization - NewsAPI",
    "section": "",
    "text": "Now that the articles have been properly prepared to create a vectorized dataframe, several versions will be created. Namely, word count dataframes will be created using CountVectorizer() and normalized word count dataframes will be created using TfidfVectorizer(), both from scikit-learn. Stopwords will removed using these functions as well. Dataframes will be further subsetted along the political bias labels. Lemmatizing and Stemming will also be used to create different versions available for further analyses. One additional option could be further versions of maximum words allowed in a dataframe."
  }
]